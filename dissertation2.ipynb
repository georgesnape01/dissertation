{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2EOgxyWSmpJ"
      },
      "source": [
        "# Predicting Success: Machine Learning Models for CBT Outcomes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import sys\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "sys.path.append(file_path)\n",
        "\n",
        "import cleaning_functions as cleanf\n",
        "import eda_functions as edaf\n",
        "import preparation_functions as prepf\n",
        "import modelling_functions as modelf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGN3Rko0HCsf",
        "outputId": "c1404fed-d0c3-4358-fdfa-e94007d5b86c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat '/content/drive/MyDrive/Colab Notebooks/DissFunctions.py'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i91HgyMdbkfz",
        "outputId": "51fca0c6-bb6b-442d-83e8-60595816094d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"provenance\":[],\"authorship_tag\":\"ABX9TyPErySvhR6MH3vMn0fXcqQw\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"},\"language_info\":{\"name\":\"python\"}},\"cells\":[{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"W1ODmQunSVNP\"},\"source\":[\"# Functions\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"QV1f10TTIv53\"},\"source\":[\"## 1.2 Preprocessing Functions\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"8vw4-ITlmfz1\"},\"outputs\":[],\"source\":[\"def load_data():\\n\",\"\\n\",\"    drive.mount(\\\"/content/drive\\\") # access google drive folder\\n\",\"\\n\",\"    file_path = '/content/drive/MyDrive/Data/Dissertation_Data/'\\n\",\"    df = pd.read_csv(file_path + 'mental_health.csv', delimiter = ',') # csv\\n\",\"    raw_df = df.copy()\\n\",\"\\n\",\"    return df, raw_df\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"2nLysn57q38W\"},\"outputs\":[],\"source\":[\"def manage_df(df, name, option):\\n\",\"\\n\",\"    diss_path = '/content/drive/MyDrive/Data/Dissertation_Data/'\\n\",\"\\n\",\"    if option == 'save':\\n\",\"        df.to_csv(file_path + name + '.csv')\\n\",\"        print('Saved!')\\n\",\"\\n\",\"    elif option == 'load':\\n\",\"        df = pd.read_csv(file_path + name + '.csv', index_col=0)\\n\",\"        return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"QtShKAeACyh2\"},\"source\":[\"2.2 Non-Questionnaire Data Preprocessing\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"5I8pmofOI0n8\"},\"outputs\":[],\"source\":[\"def rename_features(df): # non-questionnaire features\\n\",\"\\n\",\"    df.rename(columns = {\\n\",\"        'Unnamed: 0': 'CaseID',\\n\",\"        'IAPTus_Num': 'ClientID',\\n\",\"        'Referral Date': 'ReferralDate',\\n\",\"        'Age_ReferralRequest_ReceivedDate': 'AgeAtReferralRequest',\\n\",\"        'EthnicDescGroupCode': 'EthnicCode',\\n\",\"        'EthnicCategoryGroupShortCode': 'EthnicCodeShort',\\n\",\"        'GenderIdentity': 'Gender',\\n\",\"        'SexualOrientationDesc': 'SexualOrientation',\\n\",\"        'EndDescGroupShort': 'Treated',\\n\",\"        'AllocatedCareProfNameCode': 'TherapistID',\\n\",\"        'JobTitleCode': 'ExperienceLevel',\\n\",\"        'Days to first assessment': 'DaystoAssessment',\\n\",\"        'Days to first treatment': 'DaystoTreatment',\\n\",\"        'CountOfAttendedCareContacts': 'CareContacts',\\n\",\"        'RecoveryDesc': 'Recovery',\\n\",\"        'ReliableRecoveryDesc': 'ReliableRecovery',\\n\",\"        'Date': 'DateOfQuestionnaire'},\\n\",\"        inplace = True)\\n\",\"\\n\",\"    return df\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"5XZE6Oq-LHCV\"},\"outputs\":[],\"source\":[\"def create_referral_count(df):\\n\",\"    def count_referrals(col):\\n\",\"        if '_1' in col:\\n\",\"            return 1\\n\",\"        elif '_2' in col:\\n\",\"            return 2\\n\",\"        elif '_3' in col:\\n\",\"            return 3\\n\",\"        elif '_4' in col:\\n\",\"            return 4\\n\",\"        elif '_5' in col:\\n\",\"            return 5\\n\",\"        else:\\n\",\"            return 1\\n\",\"    df.insert(2, \\\"ReferralCount\\\", df['ClientID'].apply(count_referrals)) # introduce next to ClientID\\n\",\"    return df\\n\",\"\\n\",\"def clean_client_id(df):\\n\",\"    for text in ['_1', '_2', '_3', '_4']:\\n\",\"        df['ClientID'] = df['ClientID'].str.replace(text, '') # remove ending\\n\",\"    df['ClientID'] = pd.to_numeric(df['ClientID'])\\n\",\"    return df\\n\",\"\\n\",\"def convert_features_to_datetime(df):\\n\",\"    df['ReferralDate'] = pd.to_datetime(df['ReferralDate'], format = '%d/%m/%Y')\\n\",\"    df['DateOfQuestionnaire'] = pd.to_datetime(df['DateOfQuestionnaire'], format = '%d/%m/%Y')\\n\",\"    return df\\n\",\"\\n\",\"def convert_float_features_to_int(df):\\n\",\"    df['EthnicCode'] = df['EthnicCode'].astype('Int64') # Int deals with NaNs, int does not\\n\",\"    df['EthnicCodeShort'] = df['EthnicCodeShort'].astype('Int64')\\n\",\"    df['TherapistID'] = df['TherapistID'].astype('Int64')\\n\",\"    df['ExperienceLevel'] = df['ExperienceLevel'].astype('Int64')\\n\",\"    return df\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"uAOanAnaN9hJ\"},\"outputs\":[],\"source\":[\"def map_features(df):\\n\",\"\\n\",\"    Gender_map = {\\n\",\"        'CHANGE ME': np.nan,\\n\",\"        'X': np.nan}\\n\",\"    df['Gender'] = df['Gender'].replace(Gender_map).astype('Int64')\\n\",\"\\n\",\"    Treated_map = {\\n\",\"        'Seen and treated': 1,\\n\",\"        'Seen but not treated': 0}\\n\",\"    df['Treated'] = df['Treated'].replace(Treated_map).astype('Int64')\\n\",\"\\n\",\"    ReliableChangeDesc_map = {\\n\",\"        'Reliable improvement': 2, # what about(-1, 0, 1)?\\n\",\"        'No reliable change': 1,\\n\",\"        'Reliable deterioration': 0,\\n\",\"        'Not applicable': np.nan}\\n\",\"    df['ReliableChangeDesc'] = df['ReliableChangeDesc'].replace(ReliableChangeDesc_map).astype('Int64')\\n\",\"\\n\",\"    Recovery_map = {\\n\",\"        'At recovery': 1,\\n\",\"        'Not at recovery': 0,\\n\",\"        'Not applicable': np.nan}\\n\",\"    df['Recovery'] = df['Recovery'].replace(Recovery_map).astype('Int64')\\n\",\"\\n\",\"    ReliableRecovery_map = {\\n\",\"        'Reliable recovery': 1,\\n\",\"        'No reliable recovery': 0,\\n\",\"        'Not applicable': np.nan}\\n\",\"    df['ReliableRecovery'] = df['ReliableRecovery'].replace(ReliableRecovery_map).astype('Int64')\\n\",\"\\n\",\"    return df\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"quT-_D2KcbCD\"},\"outputs\":[],\"source\":[\"def one_hot_encode_features(df):\\n\",\"\\n\",\"    EndDesc_cols = pd.get_dummies(df['EndDesc'], prefix = 'EndDesc')\\n\",\"    EndDesc_index = df.columns.get_loc('EndDesc')\\n\",\"    df = pd.concat([df.iloc[:, :EndDesc_index + 1], EndDesc_cols, df.iloc[:, EndDesc_index + 1:]], axis = 1)\\n\",\"    df = df.drop(columns = ['EndDesc'])\\n\",\"\\n\",\"    EndDescShort_cols = pd.get_dummies(df['EndDescShort'], prefix = 'EndDescShort')\\n\",\"    EndDescShort_index = df.columns.get_loc('EndDescShort')\\n\",\"    df = pd.concat([df.iloc[:, :EndDescShort_index + 1], EndDescShort_cols, df.iloc[:, EndDescShort_index + 1:]], axis = 1)\\n\",\"    df = df.drop(columns = ['EndDescShort'])\\n\",\"\\n\",\"    return df\\n\",\"\\n\",\"def convert_to_int_features(df):\\n\",\"    int_cols = ['SexualOrientation', 'DaystoAssessment', 'DaystoTreatment', 'CareContacts']\\n\",\"    for col in int_cols:\\n\",\"        df[col] = df[col].astype('Int64')\\n\",\"    return df\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"Hu8simKq4woU\"},\"outputs\":[],\"source\":[\"def plot_features(df):\\n\",\"\\n\",\"    plot_cols = 4\\n\",\"    plot_rows = len(df.columns)//4 + 1\\n\",\"\\n\",\"    plt.figure(figsize = (plot_cols*3, plot_rows*3))\\n\",\"\\n\",\"    for i, col in enumerate(df.columns, start = 1):\\n\",\"        if df[col].dtype in ['int64', 'float64', 'Int64', 'datetime64[ns]', 'bool']:\\n\",\"            data = df[col].dropna()\\n\",\"            if df[col].dtype == 'bool':  # convert boolean to integers\\n\",\"                data = data.astype(int)\\n\",\"            plt.subplot(plot_rows, 4, i)\\n\",\"            plt.hist(data, bins = 20, color='darkgrey', edgecolor='white')\\n\",\"            plt.title(col)\\n\",\"            plt.xlabel(col)\\n\",\"            plt.ylabel('Frequency')\\n\",\"\\n\",\"    plt.suptitle('Data Plots', y = 1, fontsize = 24)\\n\",\"    plt.tight_layout()\\n\",\"    plt.show()\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"2nfsedT9_wYy\"},\"outputs\":[],\"source\":[\"def preprocess_nonques_features(df):\\n\",\"\\n\",\"    df = rename_features(df)\\n\",\"    df = create_referral_count(df)\\n\",\"    df = clean_client_id(df)\\n\",\"    df = convert_features_to_datetime(df)\\n\",\"    df = convert_float_features_to_int(df)\\n\",\"    df = map_features(df)\\n\",\"    df = one_hot_encode_features(df)\\n\",\"    df = convert_to_int_features(df)\\n\",\"\\n\",\"    return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"xW2b1hV0CUIz\"},\"source\":[\"2.3 Questionnaire Data Preprocessing\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"GsoRt1DtCcWD\"},\"outputs\":[],\"source\":[\"def preprocess_ques_features(df):\\n\",\"\\n\",\"    # convert all variables to float\\n\",\"    for col in df.columns[27:]:\\n\",\"        df[col] = pd.to_numeric(df[col], errors = 'coerce') # (I think this also removes '.')\\n\",\"\\n\",\"    item_cols = df.columns.str.contains('Item').tolist()\\n\",\"    item_cols = df.columns[item_cols]\\n\",\"    for col in item_cols:\\n\",\"        df[col] = df[col].apply(lambda x: x if pd.isna(x) or x.is_integer() else np.nan) # NaN non-integer values\\n\",\"        df[col] = df[col].astype('Int64') # convert to int\\n\",\"\\n\",\"    thresh_cols = df.columns.str.contains('Threshold').tolist()\\n\",\"    thresh_cols = df.columns[thresh_cols]\\n\",\"    for col in thresh_cols:\\n\",\"        df[col] = df[col].apply(lambda x: x if pd.isna(x) or x.is_integer() else np.nan)\\n\",\"        df[col] = df[col].astype('Int64')\\n\",\"\\n\",\"    total_cols = df.columns.str.contains('Total').tolist()\\n\",\"    total_cols = df.columns[total_cols]\\n\",\"    for col in total_cols:\\n\",\"        df[col] = df[col].apply(lambda x: x if pd.isna(x) or x.is_integer() else np.nan)\\n\",\"        df[col] = df[col].astype('Int64')\\n\",\"\\n\",\"    # bool cols (all)\\n\",\"    bool_cols = df.select_dtypes(include = bool)\\n\",\"    for col in bool_cols:\\n\",\"        df[col] = df[col].astype('Int64')\\n\",\"\\n\",\"    return df\\n\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"2gQIQg-Y-TmD\"},\"source\":[\"2 Cleaning\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"S3XWbNeNlGLl\"},\"outputs\":[],\"source\":[\"def Clean_Data(df):\\n\",\"\\n\",\"    # clean non-questionnaire data\\n\",\"    df = preprocess_nonques_features(df)\\n\",\"\\n\",\"    # clean questionnaire data\\n\",\"    df = preprocess_ques_features(df)\\n\",\"\\n\",\"    return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"THGbfknuDUj7\"},\"source\":[\"## 1.3 EDA Functions\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"MjkFhRmV-aVr\"},\"source\":[\"3 EDA Plots\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"yYGl8zk5Dx48\"},\"outputs\":[],\"source\":[\"def single_summary_plot(df, tic_freq):\\n\",\"\\n\",\"    statistics = df.describe()\\n\",\"\\n\",\"    plt.figure(figsize=(6, 4))\\n\",\"    for index, row in statistics.iterrows():\\n\",\"        plt.plot(statistics.columns, row, label=index)\\n\",\"\\n\",\"    plt.xlabel('Column')\\n\",\"    plt.ylabel('Value')\\n\",\"    plt.title('Summary Statistics')\\n\",\"    plt.legend(loc = 'lower right')\\n\",\"    plt.xticks(statistics.columns[::tic_freq])\\n\",\"    plt.show()\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"6fkAzO2bD3mI\"},\"outputs\":[],\"source\":[\"def summary_plot(dfs, dfs_names):\\n\",\"\\n\",\"    # subplot dimensions\\n\",\"    num_dfs = len(dfs)\\n\",\"    num_rows = num_dfs // 2 + num_dfs % 2\\n\",\"\\n\",\"    # plot\\n\",\"    plt.figure(figsize=(15, 5 * num_rows))\\n\",\"    for i, df in enumerate(dfs):\\n\",\"\\n\",\"        statistics = df.describe()\\n\",\"\\n\",\"        plt.subplot(num_rows, 2, i+1)\\n\",\"        for index, row in statistics.iterrows():\\n\",\"            plt.plot(statistics.columns, row, label=index)\\n\",\"\\n\",\"        plt.xlabel('Column')\\n\",\"        plt.ylabel('Value')\\n\",\"        plt.title(f'{dfs_names[i]} Summary')\\n\",\"        plt.xticks(statistics.columns[::2])\\n\",\"        plt.legend(loc = 'center right')\\n\",\"\\n\",\"    plt.tight_layout()\\n\",\"    plt.show()\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"UGdQ8RZ5f26v\"},\"source\":[\"## 1.4 Processing Functions\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"74NDOIP3MXhd\"},\"source\":[\"### 4.1 Duplicates\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"kK5D0BZEMVkn\"},\"outputs\":[],\"source\":[\"def find_duplicates_features(df):\\n\",\"    dupe_cols = np.transpose(df).duplicated()\\n\",\"    dupe_cols = df.columns[dupe_cols].tolist()\\n\",\"    dupes_of = {}\\n\",\"    for col_name in dupe_cols:\\n\",\"        col_values = df[col_name]\\n\",\"        dupes = [other_col for other_col in df.columns if (other_col != col_name) and df[other_col].equals(col_values)]\\n\",\"        dupes_of[col_name] = dupes\\n\",\"        print(f'{col_name} is a duplicate of: {dupes}')\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"e9_mblKrB0dM\"},\"source\":[\"Final\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"gjiI4WFX_i2r\"},\"outputs\":[],\"source\":[\"def drop_duplicate_features(df):\\n\",\"    dupe_cols = np.transpose(df).duplicated()\\n\",\"    dupe_cols = df.columns[dupe_cols].tolist()\\n\",\"    df = df.drop(columns = dupe_cols)\\n\",\"    return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"FMCGtYfVQbR3\"},\"source\":[\"### 4.2 Outliers\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"S7h4646W_D_y\"},\"outputs\":[],\"source\":[\"def column_contents(df):\\n\",\"    pot_cols = []\\n\",\"    for col in df.columns:\\n\",\"        if not df[col].isin([0, 1, pd.NA]).all():\\n\",\"            pot_cols.append(col)\\n\",\"    if len(pot_cols) == 0:\\n\",\"        print(f'Columns only contain 0, 1 and <NA>')\\n\",\"    else:\\n\",\"        print(f'Columns contining more than 0, 1 and <NA>: {pot_cols}')\\n\",\"    return pot_cols\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"RX_6goE0QeVF\"},\"outputs\":[],\"source\":[\"def mad(df):\\n\",\"    median = df.median()\\n\",\"    deviations = np.abs(df - median)\\n\",\"    mad_val = deviations.median() # MAD\\n\",\"    return mad_val\\n\",\"\\n\",\"def modified_zscore(df, threshold = 3.5):\\n\",\"    median = df.median()\\n\",\"    mad_val = mad(df) # MAD\\n\",\"    modified_zscores = 0.6745 * (df - median) / mad_val # modified Z-score\\n\",\"    return np.abs(modified_zscores) > threshold\\n\",\"\\n\",\"def find_outlier_cols(df, threshold = 3.5): # counts also\\n\",\"\\n\",\"    outlier_df = modified_zscore(df)\\n\",\"    outlier_cols = set()\\n\",\"    for col in outlier_df.columns:\\n\",\"        outlier_list = []\\n\",\"        for i, val in outlier_df[col].items():\\n\",\"            if pd.notna(val) and val:\\n\",\"                outlier_list.append(df.iloc[i][col])\\n\",\"        if outlier_list:\\n\",\"            outlier_cols.add(col)\\n\",\"            print(f'{col} outlier count: {len(outlier_list)}')\\n\",\"\\n\",\"    # return outlier_df\\n\",\"    outlier_cols = list(outlier_cols)\\n\",\"    outlier_df = df[outlier_cols]\\n\",\"    return outlier_df\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"NytZCSxER-sR\"},\"outputs\":[],\"source\":[\"def plot_outlier_cols(outlier_df, title, legend=True):\\n\",\"    outlier_df.plot(figsize = (5, 3))\\n\",\"    plt.title(title)\\n\",\"    plt.xlabel('Index')\\n\",\"    plt.ylabel('Total Score')\\n\",\"    if legend:\\n\",\"        plt.legend(loc='upper right')\\n\",\"    else:\\n\",\"        plt.gca().legend().set_visible(False)\\n\",\"    plt.show()\\n\",\"\\n\",\"def plot_datetime_features(df):\\n\",\"    plt.figure(figsize=(10, 4))\\n\",\"    for i, col in enumerate(datetime_cols):\\n\",\"        plt.subplot(1, 2, i + 1)\\n\",\"        df[col].hist(bins = 100, color = 'grey', edgecolor = 'white')\\n\",\"        plt.title(col)\\n\",\"        plt.xlabel('Date')\\n\",\"        plt.ylabel('Frequency')\\n\",\"        plt.grid(False)\\n\",\"    plt.tight_layout()\\n\",\"    plt.show()\\n\",\"\\n\",\"def plot_outlier_cols2(outlier_df, title, color):\\n\",\"\\n\",\"    plot_cols = 4\\n\",\"    plot_rows = len(outlier_df.columns)//4 + 1\\n\",\"    plt.figure(figsize = (plot_cols*3, plot_rows*3))\\n\",\"\\n\",\"    for i, col in enumerate(outlier_df.columns, start = 1):\\n\",\"        plt.subplot(plot_rows, 4, i)\\n\",\"        outlier_df[col].plot(color=color)\\n\",\"        plt.title(col)\\n\",\"        plt.xlabel('Index')\\n\",\"        plt.ylabel('Total Score')\\n\",\"\\n\",\"    plt.suptitle(title, y = 1, fontsize = 24)\\n\",\"    plt.tight_layout()\\n\",\"    plt.show()\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"1BmANarfB5P7\"},\"source\":[\"Final\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"HcfP6XRJ3Jfn\"},\"outputs\":[],\"source\":[\"def replace_ques_outliers(df): # shortens search using quartiles\\n\",\"\\n\",\"    item_cols = [col for col in df.columns if 'Item' in col]\\n\",\"    item_df = df[item_cols]\\n\",\"\\n\",\"    total_cols = [col for col in df.columns if 'Total' in col]\\n\",\"    total_df = df[total_cols]\\n\",\"\\n\",\"    # replace negative totals with nan\\n\",\"    for col in total_df:\\n\",\"        df.loc[df[col] < 0, col] = np.nan # replace in df\\n\",\"        total_df.loc[total_df[col] < 0, col] = np.nan # replace in total_df\\n\",\"\\n\",\"    # find col contents\\n\",\"    def create_pot_cols(df):\\n\",\"        pot_cols = []\\n\",\"        for col in df.columns:\\n\",\"            if not df[col].isin([0, 1, pd.NA]).all():\\n\",\"                pot_cols.append(col)\\n\",\"        return pot_cols\\n\",\"\\n\",\"    # detect whether cols are just 0,1,na\\n\",\"    item_pot_cols = create_pot_cols(item_df)\\n\",\"    total_pot_cols = create_pot_cols(total_df)\\n\",\"\\n\",\"    # potential columns containing outliers (shortening the search, only these can contain outliers)\\n\",\"    total_pot_df = total_df[total_pot_cols]\\n\",\"    item_pot_df = item_df[item_pot_cols]\\n\",\"\\n\",\"    # outlier counter and df function\\n\",\"    def create_outlier_df(df, threshold = 3.5):\\n\",\"\\n\",\"        outlier_df = modified_zscore(df)\\n\",\"        outlier_cols = set()\\n\",\"        for col in outlier_df.columns:\\n\",\"            outlier_list = []\\n\",\"            for i, val in outlier_df[col].items():\\n\",\"                if pd.notna(val) and val:\\n\",\"                    outlier_list.append(df.iloc[i][col])\\n\",\"            if outlier_list:\\n\",\"                outlier_cols.add(col)\\n\",\"\\n\",\"        # return outlier_df\\n\",\"        outlier_cols = list(outlier_cols)\\n\",\"        outlier_df = df[outlier_cols]\\n\",\"        return outlier_df\\n\",\"\\n\",\"    # find outliers in variables\\n\",\"    total_outlier_df = create_outlier_df(total_pot_df)\\n\",\"    item_outlier_df = create_outlier_df(item_pot_df)\\n\",\"\\n\",\"    # replace outliers with nan\\n\",\"    for col in total_outlier_df:\\n\",\"        df.loc[df[col] > 200, col] = np.nan\\n\",\"        total_outlier_df.loc[total_outlier_df[col] > 200, col] = np.nan\\n\",\"\\n\",\"    for col in item_outlier_df:\\n\",\"        df.loc[df[col] > 50, col] = np.nan\\n\",\"        item_outlier_df.loc[item_outlier_df[col] > 50, col] = np.nan\\n\",\"\\n\",\"    df.loc[df['Total14'] > 37, 'Total14'] = np.nan\\n\",\"    df.loc[df['Item70'] > 3, 'Item70'] = np.nan\\n\",\"    df.loc[df['Item132'] > 5, 'Item132'] = np.nan\\n\",\"\\n\",\"    return df\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"FOvHn6v9CMeP\"},\"outputs\":[],\"source\":[\"def replace_outliers(df):\\n\",\"    df.loc[df['AgeAtReferralRequest'] == 0, 'AgeAtReferralRequest'] = np.nan\\n\",\"    df = replace_ques_outliers(df)\\n\",\"    return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"piAcrqMCUf_b\"},\"source\":[\"### 4.3 Constant and Quasi Constant Features\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"odjJkEB63184\"},\"outputs\":[],\"source\":[\"def drop_const_features(df):\\n\",\"    const_cols = [col for col in df.columns if df[col].nunique() == 1]\\n\",\"    df = df.drop(columns = const_cols)\\n\",\"    return df\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"bvejzgiDUj3L\"},\"outputs\":[],\"source\":[\"def plot_datetime_features(df):\\n\",\"    plt.figure(figsize = (10,4))\\n\",\"    for i, col in enumerate(datetime_cols, 1):\\n\",\"        plt.subplot(1, len(datetime_cols), i)\\n\",\"        plt.hist(df[col], bins = 20, color = 'grey', edgecolor = 'white')\\n\",\"        plt.xlabel(col)\\n\",\"        plt.ylabel('Frequency')\\n\",\"        plt.title(col)\\n\",\"    plt.tight_layout()\\n\",\"    plt.show()\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"JibfK4xBB-pj\"},\"source\":[\"Final\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"RijZyYsG_a4k\"},\"outputs\":[],\"source\":[\"def quasi_percentage(df):\\n\",\"\\n\",\"    # ordinal variables\\n\",\"    ordinal_df = df.select_dtypes(include = ['int64', 'Int64'])\\n\",\"\\n\",\"    # select col modes\\n\",\"    modes = ordinal_df.mode() # mode of each col\\n\",\"    modes = modes.iloc[0] # select modes only\\n\",\"\\n\",\"    # calc quasi percentage\\n\",\"    quasi_percentages = (ordinal_df == modes).sum() / len(ordinal_df) # how quasi a col is\\n\",\"\\n\",\"    return quasi_percentages\\n\",\"\\n\",\"# drop quasi features\\n\",\"def drop_quasi_features(df, threshold):\\n\",\"\\n\",\"    # calculate quasi percentage\\n\",\"    quasi_percentages = quasi_percentage(df)\\n\",\"\\n\",\"    # 0.995 is about 3 observations\\n\",\"    exceeding_threshold_columns = quasi_percentages[quasi_percentages > threshold].index\\n\",\"    df = df.drop(columns = exceeding_threshold_columns)\\n\",\"    return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"lTOHK-xOBdpr\"},\"source\":[\"### 4.4 Correlation\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"-uxid4K6dlg8\"},\"outputs\":[],\"source\":[\"def select_future_features(df):\\n\",\"    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\\n\",\"    #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\\n\",\"    future_vars = df.drop(columns = ['ReliableChangeDesc', 'Recovery', 'ReliableRecovery'] + EndDesc_cols)\\n\",\"    return future_vars\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"C_o2RbFkMvaX\"},\"outputs\":[],\"source\":[\"def generate_corr_matricies(df, load_matrices):\\n\",\"\\n\",\"    file_path = '/content/drive/MyDrive/Data/Dissertation_Data/'\\n\",\"\\n\",\"    if not load_matrices:\\n\",\"        # select explanatory variables, removing info on future\\n\",\"        future_df = select_future_features(df)\\n\",\"\\n\",\"        # create correlation matrices\\n\",\"        kendall_corr = future_df.corr(method='kendall').abs()\\n\",\"        spearman_corr = future_df.corr(method='spearman').abs()\\n\",\"        kendall_corr.to_csv(file_path + 'kendall_corr.csv') # save\\n\",\"        spearman_corr.to_csv(file_path + 'spearman_corr.csv') # save\\n\",\"\\n\",\"    else:\\n\",\"        kendall_corr = pd.read_csv(file_path + 'kendall_corr.csv', index_col=0)\\n\",\"        spearman_corr = pd.read_csv(file_path + 'spearman_corr.csv', index_col=0)\\n\",\"\\n\",\"    return kendall_corr, spearman_corr\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"KHv11Vma1dWy\"},\"outputs\":[],\"source\":[\"def plot_correlation_matrices(corr_matricies):\\n\",\"\\n\",\"    plt.figure(figsize=(8, 3))\\n\",\"\\n\",\"    plt.subplot(121)\\n\",\"    sns.heatmap(corr_matricies[0], annot = False, cmap = 'flare', fmt = \\\".2f\\\", xticklabels = False, yticklabels = False)\\n\",\"    plt.title('Kendall Correlation Matrix')\\n\",\"\\n\",\"    plt.subplot(122)\\n\",\"    sns.heatmap(corr_matricies[1], annot = False, cmap = 'flare', fmt = \\\".2f\\\", xticklabels = False, yticklabels = False)\\n\",\"    plt.title('Spearman Correlation Matrix')\\n\",\"\\n\",\"    plt.tight_layout()\\n\",\"    plt.show()\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"vaIUUFSLdsBA\"},\"outputs\":[],\"source\":[\"# count correlated variable pairs above each threshold\\n\",\"def count_correlated_pairs(correlation_matrix, thresholds):\\n\",\"\\n\",\"    counts = {}\\n\",\"    for threshold in thresholds:\\n\",\"        correlated_vars = (correlation_matrix.abs() > threshold)\\n\",\"        # get upper triangle\\n\",\"        np.fill_diagonal(correlated_vars.values, False)\\n\",\"        upper_triangle = correlated_vars.values[np.triu_indices_from(correlated_vars, k=1)]\\n\",\"        count = np.sum(upper_triangle)\\n\",\"        counts[threshold] = count\\n\",\"    return counts\\n\",\"\\n\",\"def correlated_features_above_treshold(matricies, thresholds):\\n\",\"\\n\",\"    print('Kendall Correlation:')\\n\",\"    kendall_counts = count_correlated_pairs(matricies[0], thresholds)\\n\",\"    for threshold, count in kendall_counts.items():\\n\",\"        print(f'{threshold * 100}%: {count}')\\n\",\"\\n\",\"    print('Spearman Correlation:')\\n\",\"    spearman_counts = count_correlated_pairs(matricies[1], thresholds)\\n\",\"    for threshold, count in spearman_counts.items():\\n\",\"        print(f'{threshold * 100}%: {count}')\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"Vs7au_sOADwf\"},\"outputs\":[],\"source\":[\"def drop_corr_cols(df, corr_matrix, threshold, ignore=True):\\n\",\"\\n\",\"    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\\n\",\"    corr_cols = [col for col in upper_tri.columns if any(upper_tri[col] > threshold)]\\n\",\"\\n\",\"    try:\\n\",\"        df = df.drop(corr_cols, axis=1)\\n\",\"    except KeyError as e:\\n\",\"        if ignore:\\n\",\"            #print(f\\\"Ignoring KeyError: {e}\\\")\\n\",\"            pass\\n\",\"        else:\\n\",\"            raise\\n\",\"\\n\",\"    return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"1fj1PpVXCl-K\"},\"source\":[\"Final\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"5VBGPv-Udz12\"},\"outputs\":[],\"source\":[\"def remove_corr_features(df, load_matrices, threshold):\\n\",\"\\n\",\"    kendall_corr, spearman_corr = generate_corr_matricies(df, load_matrices)\\n\",\"\\n\",\"    # remove highly correlated variables\\n\",\"    df = drop_corr_cols(df, kendall_corr, threshold)\\n\",\"    df = drop_corr_cols(df, spearman_corr, threshold)\\n\",\"\\n\",\"    return df\\n\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"_pSpe5vf52AJ\"},\"source\":[\"### 4.5 Missing Values within Columns\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"6ooEjn1a56BK\"},\"outputs\":[],\"source\":[\"def drop_missing_values_col(df, threshold):\\n\",\"\\n\",\"    missing_value_percentage = df.isna().mean(axis=0) # cols\\n\",\"\\n\",\"    drop_cols = missing_value_percentage[missing_value_percentage > threshold].index\\n\",\"    df = df.drop(drop_cols, axis=1)\\n\",\"\\n\",\"    return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"nFVrpck2Ls5P\"},\"source\":[\"### 4.6 Missing Values within Rows\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"HRvOzmlg5SMq\"},\"outputs\":[],\"source\":[\"def drop_missing_values_row(df, threshold):\\n\",\"\\n\",\"    missing_value_percentage = df.isna().mean(axis=1) # rows\\n\",\"\\n\",\"    drop_cols = missing_value_percentage[missing_value_percentage > threshold].index\\n\",\"    df = df.drop(drop_cols, axis=0)\\n\",\"\\n\",\"    return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"G3Gp1AdsW3ya\"},\"source\":[\"### 4.7 Scaling\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"drv67OT8W3QC\"},\"outputs\":[],\"source\":[\"def standardise(df):\\n\",\"    scaler = StandardScaler()\\n\",\"    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\\n\",\"    return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"Wv0ov3LGKkTv\"},\"source\":[\"### 4.8 Imputation\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"_7Ji0tiXTs0X\"},\"outputs\":[],\"source\":[\"def impute_data(df, method, n_neighbours=None, max_iter=None):\\n\",\"\\n\",\"    original_dtypes = df.dtypes.to_dict()\\n\",\"\\n\",\"    #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\\n\",\"    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\\n\",\"    nontarget_df = df.drop(columns = ['ReliableChangeDesc', 'Recovery', 'ReliableRecovery'] + EndDesc_cols)\\n\",\"\\n\",\"    datetime_cols = nontarget_df.select_dtypes(include=['datetime64[ns]']).columns\\n\",\"    for col in datetime_cols:\\n\",\"        nontarget_df[col] = pd.to_numeric(nontarget_df[col])\\n\",\"\\n\",\"    if method == 'none':\\n\",\"        nontarget_mat = nontarget_df.values\\n\",\"\\n\",\"    elif method == 'mean':\\n\",\"        imputer = SimpleImputer(strategy='mean')\\n\",\"        nontarget_mat = imputer.fit_transform(nontarget_df) # *numpy array*\\n\",\"\\n\",\"    elif method == 'median':\\n\",\"        imputer = SimpleImputer(strategy='median')\\n\",\"        nontarget_mat = imputer.fit_transform(nontarget_df)\\n\",\"\\n\",\"    elif method == 'knn':\\n\",\"        imputer = KNNImputer(n_neighbors=n_neighbours)\\n\",\"        nontarget_mat = imputer.fit_transform(nontarget_df)\\n\",\"\\n\",\"    elif method == 'iterative':\\n\",\"        imputer = IterativeImputer(max_iter=max_iter, random_state=2001)\\n\",\"        nontarget_mat = imputer.fit_transform(nontarget_df)\\n\",\"\\n\",\"    nontarget_df.iloc[:, :] = nontarget_mat.tolist()\\n\",\"\\n\",\"    # add imputed data to df\\n\",\"    for col in nontarget_df.columns:\\n\",\"        df[col] = nontarget_df[col]\\n\",\"\\n\",\"    if method == 'none':\\n\",\"        pass\\n\",\"\\n\",\"    else:\\n\",\"        for col, dtype in original_dtypes.items():\\n\",\"            if dtype == 'datetime64[ns]':\\n\",\"                pass # keep as float\\n\",\"            else:\\n\",\"                try:\\n\",\"                    df[col] = df[col].astype(dtype)\\n\",\"                except (ValueError, TypeError):\\n\",\"                    # *convert to original dtype*\\n\",\"                    df[col] = df[col].round().astype(int)\\n\",\"                    df[col] = df[col].astype(dtype)\\n\",\"\\n\",\"    return df\\n\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"RW5BGatARrMi\"},\"outputs\":[],\"source\":[\"# def multi_impute_data(df, method='knn', n_neighbours=3, iter=1):\\n\",\"\\n\",\"#     EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\\n\",\"#     #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\\n\",\"#     nontarget_df = df.drop(columns = ['ReliableChangeDesc', 'Recovery', 'ReliableRecovery'] + EndDesc_cols)\\n\",\"\\n\",\"#     datetime_cols = nontarget_df.select_dtypes(include=['datetime64[ns]']).columns\\n\",\"#     for col in datetime_cols:\\n\",\"#         nontarget_df[col] = nontarget_df[col].astype(int) / 10**9\\n\",\"\\n\",\"#     ordinal_cols = list(nontarget_df.select_dtypes(include=['Int64', 'int64']).columns)\\n\",\"#     imputed_dfs = []\\n\",\"\\n\",\"#     for i in range(iter):\\n\",\"\\n\",\"#         copy_df = nontarget_df.copy()\\n\",\"\\n\",\"#         if method == 'mean':\\n\",\"#             mean_imputer = SimpleImputer(strategy='mean')\\n\",\"#             copy_df[nontarget_df.columns] = mean_imputer.fit_transform(copy_df[nontarget_df.columns])\\n\",\"\\n\",\"#         elif method == 'median':\\n\",\"#             median_imputer = SimpleImputer(strategy='median')\\n\",\"#             copy_df[nontarget_df.columns] = median_imputer.fit_transform(copy_df[nontarget_df.columns])\\n\",\"\\n\",\"#         elif method == 'knn':\\n\",\"#             KNN_imputer = KNNImputer(n_neighbors=n_neighbours)\\n\",\"#             copy_df[nontarget_df.columns] = KNN_imputer.fit_transform(copy_df)\\n\",\"\\n\",\"#         imputed_dfs.append(copy_df)\\n\",\"\\n\",\"#     df = np.mean(imputed_dfs, axis=0)\\n\",\"#     print(df.dtypes)\\n\",\"#     for col in datetime_cols:\\n\",\"#         df[col] = pd.to_datetime(df[col], unit='ns')\\n\",\"\\n\",\"#     # convert imputed values for ordinal variables to discrete values\\n\",\"#     for col in ordinal_cols:\\n\",\"#         df[col] = df[col].round().astype(int)\\n\",\"\\n\",\"#     return df\\n\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"ILBLDx4T1ycH\"},\"source\":[\"### 4.9 Feature Engineering\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"F_pDAKYE1xx_\"},\"outputs\":[],\"source\":[\"def create_date_features(df):\\n\",\"\\n\",\"    # df['ReferralYear'] = df['ReferralDate'].dt.year\\n\",\"    # df['ReferralMonth'] = df['ReferralDate'].dt.month\\n\",\"    # df['ReferralWeek'] = df['ReferralDate'].dt.isocalendar().week\\n\",\"    # df['ReferralDay'] = df['ReferralDate'].dt.day\\n\",\"    # df['ReferralHour'] = df['ReferralDate'].dt.hour\\n\",\"    # df['ReferralWeekDay'] = df['ReferralDate'].dt.dayofweek\\n\",\"    # df['ReferralYearDay'] = df['ReferralDate'].dt.dayofyear\\n\",\"\\n\",\"    df['YearofQuestionnaire'] = df['DateOfQuestionnaire'].dt.year\\n\",\"    df['MonthofQuestionnaire'] = df['DateOfQuestionnaire'].dt.month\\n\",\"    df['WeekofQuestionnaire'] = df['DateOfQuestionnaire'].dt.isocalendar().week\\n\",\"    df['DayofQuestionnaire'] = df['DateOfQuestionnaire'].dt.day\\n\",\"    df['HourofQuestionnaire'] = df['DateOfQuestionnaire'].dt.hour\\n\",\"    df['WeekDayofQuestionnaire'] = df['DateOfQuestionnaire'].dt.dayofweek\\n\",\"    df['YearDayofQuestionnaire'] = df['DateOfQuestionnaire'].dt.dayofyear\\n\",\"\\n\",\"    return df\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"LgJIuZFR25pq\"},\"outputs\":[],\"source\":[\"def feature_engineering(df):\\n\",\"\\n\",\"    df = create_date_features(df)\\n\",\"\\n\",\"    return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"y8mML9d7ahEe\"},\"source\":[\"### 4.10 Feature Importance\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"U6uBX7QQanjd\"},\"outputs\":[],\"source\":[\"def find_important_features(df, k_features, target='Recovery'):\\n\",\"\\n\",\"    \\\"\\\"\\\"\\n\",\"    k is the number of features each feature selection method should select.\\n\",\"    NOT the number of features returned\\n\",\"\\n\",\"    \\\"\\\"\\\"\\n\",\"\\n\",\"    if k_features == None:\\n\",\"        important_features = df.columns\\n\",\"\\n\",\"    else:\\n\",\"        features = df.dropna()\\n\",\"        EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\\n\",\"        explanatory = features.drop(['Recovery', 'ReliableRecovery', 'ReliableChangeDesc'] + EndDesc_cols, axis = 1)\\n\",\"        target = features[target]\\n\",\"\\n\",\"        # fishers score\\n\",\"        kbest_fisher = SelectKBest(score_func=f_classif, k=k_features)\\n\",\"        selected_fisher = kbest_fisher.fit_transform(explanatory, target)\\n\",\"        index_fisher = kbest_fisher.get_support(indices=True)\\n\",\"        names_fisher = explanatory.columns[index_fisher]\\n\",\"\\n\",\"        # mutual information gain\\n\",\"        def mutual_info_classif_wseed(X, y):\\n\",\"            return mutual_info_classif(X, y, random_state=11)\\n\",\"        kbest_gain = SelectKBest(score_func=mutual_info_classif_wseed, k=k_features)\\n\",\"        selected_gain = kbest_gain.fit_transform(explanatory, target)\\n\",\"        index_gain = kbest_gain.get_support(indices=True)\\n\",\"        names_gain = explanatory.columns[index_gain]\\n\",\"\\n\",\"        important_combined = set(names_fisher).union(set(names_gain))\\n\",\"        important_features = list(important_combined)\\n\",\"\\n\",\"    return important_features\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"NTQjIpLqg-1a\"},\"outputs\":[],\"source\":[\"def select_important_features(df, k_features, target='Recovery'):\\n\",\"\\n\",\"    important_features = find_important_features(df, k_features, target='Recovery')\\n\",\"\\n\",\"    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\\n\",\"    df = df[['Recovery', 'ReliableRecovery', 'ReliableChangeDesc'] + EndDesc_cols + important_features]\\n\",\"\\n\",\"    return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"p3-sW_8Qa2Pt\"},\"source\":[\"### 4 Prepare Data\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"ggq9DTNw4ycz\"},\"outputs\":[],\"source\":[\"def Prepare_Data(df,\\n\",\"                 quasi_thresh=1.0,\\n\",\"                 corr_thresh=1.0,\\n\",\"                 load_matrices=True,\\n\",\"                 col_thresh=1.0,\\n\",\"                 row_thresh=1.0,\\n\",\"                 imputation_method='knn',\\n\",\"                 n_neighbours=10,\\n\",\"                 max_iter=10,\\n\",\"                 k_features=200):\\n\",\"\\n\",\"    df = drop_duplicate_features(df)\\n\",\"    df = replace_outliers(df)\\n\",\"    df = drop_const_features(df)\\n\",\"    df = drop_quasi_features(df, quasi_thresh)\\n\",\"    df = remove_corr_features(df, corr_thresh, load_matrices)\\n\",\"    df = drop_missing_values_col(df, col_thresh)\\n\",\"    df = drop_missing_values_col(df, row_thresh)\\n\",\"    #df = standardise(df)\\n\",\"    df = impute_data(df, imputation_method, n_neighbours, max_iter)\\n\",\"    #df = feature_engineering(df)\\n\",\"    df = select_important_features(df, k_features)\\n\",\"\\n\",\"    return df\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"BkNvB98vLreZ\"},\"source\":[\"## 1.5 Modelling\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"xV4vMwKNS5vp\"},\"source\":[\"### Summary Plots\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"FzadOsBMS8NH\"},\"outputs\":[],\"source\":[\"def ModelSelection_Summary(model):\\n\",\"\\n\",\"# pr curve\\n\",\"\\n\",\"    scores = model[0]\\n\",\"    preds = model[1]\\n\",\"    actuals = model[2]\\n\",\"\\n\",\"    print('Average accuracy score: {0}'.format(np.average(scores)))\\n\",\"\\n\",\"    prec, recall, _ = metrics.precision_recall_curve(actuals, preds)\\n\",\"    print('AUPRC score: {0}\\\\n'.format(metrics.auc(recall, prec)))\\n\",\"\\n\",\"    # plot pr curve\\n\",\"    plt.figure(figsize=(12,4))\\n\",\"    plt.subplot(121)\\n\",\"    plt.plot(recall, prec, marker='.')\\n\",\"    plt.xlabel('Recall')\\n\",\"    plt.ylabel('Precision')\\n\",\"    plt.show()\\n\",\"\\n\",\"# confusion matrix\\n\",\"\\n\",\"    # probabilities to binary\\n\",\"    threshold = 0.5\\n\",\"    binary_preds = [1 if pred >= threshold else 0 for pred in preds]\\n\",\"\\n\",\"    # plot confusion matrix\\n\",\"    conf_matrix = confusion_matrix(actuals, binary_preds)\\n\",\"\\n\",\"    plt.subplot(121)\\n\",\"    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = 'Purples')\\n\",\"    plt.xlabel('Predicted labels')\\n\",\"    plt.ylabel('True labels')\\n\",\"    plt.title('Confusion Matrix')\\n\",\"    plt.tight_layout()\\n\",\"    plt.show()\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"IVmXoPNQL1IT\"},\"source\":[\"### XGBoost\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"8zN_w7g0LwvE\"},\"outputs\":[],\"source\":[\"def XGBoost_ModelSelection(df, target, selector, param_grid, k=5):\\n\",\"\\n\",\"    # dataset\\n\",\"    sample = df.dropna(subset = [target])\\n\",\"    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col] #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\\n\",\"    X = sample.drop(['ReliableChangeDesc', 'ReliableRecovery', 'Recovery'] + EndDesc_cols, axis = 1)\\n\",\"    y = sample[target]\\n\",\"    cols = X.columns\\n\",\"\\n\",\"    # machine learning algorithm\\n\",\"    classifier = XGBClassifier()\\n\",\"\\n\",\"    # feature selection method\\n\",\"    if selector == 'SelectFromModel':\\n\",\"        selector = SelectFromModel(classifier)\\n\",\"    elif selector == 'RFE':\\n\",\"        selector = RFE(classifier)\\n\",\"    else:\\n\",\"        # fill this #\\n\",\"        raise ValueError('Unsupported selector type')\\n\",\"\\n\",\"    # pipeline\\n\",\"    pipeline = Pipeline([(\\\"FS\\\", selector), (\\\"classifier\\\", classifier)])\\n\",\"\\n\",\"    # initialise lists\\n\",\"    scores, preds, actuals = [], [], []\\n\",\"\\n\",\"    # cross validation and hyperparameter tuning\\n\",\"    outer_cv = StratifiedKFold(n_splits = k, shuffle = True)\\n\",\"    inner_cv = StratifiedKFold(n_splits = k, shuffle = True) # both set to k for now\\n\",\"    for train_index, test_index in outer_cv.split(X, y):\\n\",\"\\n\",\"        # outer CV train and test sets\\n\",\"        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\\n\",\"        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\\n\",\"\\n\",\"        # hyper-parameter tuning\\n\",\"        grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=inner_cv, scoring='accuracy', verbose=0, n_jobs=-1)\\n\",\"        grid_search.fit(X_train, y_train)\\n\",\"        print(\\\"Inner CV accuracy: {}\\\".format(grid_search.best_score_)) # validation sets\\n\",\"\\n\",\"        # optimal model\\n\",\"        estimator = grid_search.best_estimator_\\n\",\"\\n\",\"        # count features selected\\n\",\"        support = estimator.named_steps['FS'].get_support()\\n\",\"        num_feat = np.sum(support)\\n\",\"        print(\\\"Number of selected features {0}\\\".format(num_feat))\\n\",\"\\n\",\"        # features selected\\n\",\"        col_index = np.where(support)[0]\\n\",\"        col_names = [cols[col] for col in col_index]\\n\",\"        print(\\\"Selected features {0}\\\".format(col_names))\\n\",\"\\n\",\"        # hyperparameters selected\\n\",\"        print(\\\"Max depth {0}\\\".format(estimator.named_steps[\\\"classifier\\\"].max_depth))\\n\",\"        print(\\\"Number of trees {0}\\\".format(estimator.named_steps[\\\"classifier\\\"].n_estimators))\\n\",\"        print(\\\"Learning rate {0}\\\".format(estimator.named_steps[\\\"classifier\\\"].learning_rate))\\n\",\"        print(\\\"Minimum child weight {0}\\\".format(estimator.named_steps[\\\"classifier\\\"].min_child_weight))\\n\",\"        print(\\\"Subsample {0}\\\".format(estimator.named_steps[\\\"classifier\\\"].subsample))\\n\",\"        print(\\\"Colsample bytree {0}\\\".format(estimator.named_steps[\\\"classifier\\\"].colsample_bytree))\\n\",\"        print(\\\"Gamma {0}\\\".format(estimator.named_steps[\\\"classifier\\\"].gamma))\\n\",\"        print(\\\"Lambda {0}\\\".format(estimator.named_steps[\\\"classifier\\\"].reg_lambda))\\n\",\"        print(\\\"Alpha {0}\\\".format(estimator.named_steps[\\\"classifier\\\"].reg_alpha))\\n\",\"\\n\",\"        # evaluating optimised model on test\\n\",\"        predictions = estimator.predict(X_test)\\n\",\"        score = metrics.accuracy_score(y_test, predictions)\\n\",\"        scores.append(score)\\n\",\"        print('Outer CV accuracy: {}'.format(score)) # test sets\\n\",\"\\n\",\"        print(\\\"--------------------------------------------------\\\")\\n\",\"\\n\",\"        probs = estimator.predict_proba(X_test)[:, 1]\\n\",\"        preds.extend(probs)\\n\",\"        actuals.extend(y_test)\\n\",\"\\n\",\"    return scores, preds, actuals\\n\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"uLNplPcaaHL3\"},\"source\":[\"## BERT\"]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"Y31WhPuUaIp6\"},\"outputs\":[],\"source\":[\"from sklearn.model_selection import StratifiedKFold, GridSearchCV\\n\",\"from sklearn.metrics import accuracy_score\\n\",\"from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\\n\",\"from transformers import pipeline\\n\",\"from sklearn.base import BaseEstimator, TransformerMixin\\n\",\"from sklearn.pipeline import Pipeline\\n\",\"import torch\\n\",\"\\n\",\"class BERTClassifier(BaseEstimator, TransformerMixin):\\n\",\"    def __init__(self, model_name='bert-base-uncased', max_length=128):\\n\",\"        self.model_name = model_name\\n\",\"        self.max_length = max_length\\n\",\"        self.tokenizer = BertTokenizer.from_pretrained(model_name)\\n\",\"        self.model = BertForSequenceClassification.from_pretrained(model_name)\\n\",\"\\n\",\"    def fit(self, X, y):\\n\",\"        self.model.train()\\n\",\"        return self\\n\",\"\\n\",\"    def predict(self, X):\\n\",\"        inputs = self.tokenizer(X.tolist(), return_tensors='pt', truncation=True, padding=True, max_length=self.max_length)\\n\",\"        with torch.no_grad():\\n\",\"            outputs = self.model(**inputs)\\n\",\"        logits = outputs.logits\\n\",\"        preds = torch.argmax(logits, axis=1)\\n\",\"        return preds.numpy()\\n\",\"\\n\",\"    def predict_proba(self, X):\\n\",\"        inputs = self.tokenizer(X.tolist(), return_tensors='pt', truncation=True, padding=True, max_length=self.max_length)\\n\",\"        with torch.no_grad():\\n\",\"            outputs = self.model(**inputs)\\n\",\"        logits = outputs.logits\\n\",\"        probs = torch.nn.functional.softmax(logits, dim=1)\\n\",\"        return probs.numpy()\\n\",\"\\n\",\"def BERT_ModelSelection(df, text_column, target, param_grid, k=5):\\n\",\"    # dataset to model\\n\",\"    sample = df.dropna(subset=[target])  # remove missing target values\\n\",\"    X = sample[text_column]\\n\",\"    y = sample[target]\\n\",\"\\n\",\"    # machine learning model\\n\",\"    classifier = BERTClassifier()\\n\",\"\\n\",\"    # pipeline\\n\",\"    pipeline = Pipeline([('classifier', classifier)])\\n\",\"\\n\",\"    # initialize lists\\n\",\"    scores = []\\n\",\"    preds = []\\n\",\"    actuals = []\\n\",\"\\n\",\"    # k-fold CV\\n\",\"    kf = StratifiedKFold(n_splits=k, shuffle=True)\\n\",\"    for train_index, test_index in kf.split(X, y):\\n\",\"        # train and test data for CV\\n\",\"        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\\n\",\"        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\\n\",\"\\n\",\"        # finding optimal models, hyperparameter tuning\\n\",\"        grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=kf, scoring='accuracy', verbose=0, n_jobs=-1)\\n\",\"        grid_search.fit(X_train, y_train)\\n\",\"        print(\\\"Internal CV Accuracy of estimator: {}\\\".format(grid_search.best_score_))\\n\",\"\\n\",\"        # best estimator\\n\",\"        estimator = grid_search.best_estimator_\\n\",\"\\n\",\"        # print hyperparameters selected\\n\",\"        # For BERT, hyperparameters can include learning rate, batch size, etc.\\n\",\"        # Example:\\n\",\"        print(\\\"BERT Model used: {}\\\".format(estimator.named_steps[\\\"classifier\\\"].model_name))\\n\",\"        print(\\\"Max length used: {}\\\".format(estimator.named_steps[\\\"classifier\\\"].max_length))\\n\",\"\\n\",\"        # predicting the test data with the optimized models\\n\",\"        predictions = estimator.predict(X_test)\\n\",\"        score = accuracy_score(y_test, predictions)\\n\",\"        scores.append(score)\\n\",\"        print('Accuracy performance on this test set: {}'.format(score))\\n\",\"\\n\",\"        print(\\\"--------------------------------------------------\\\")\\n\",\"\\n\",\"        probs = estimator.predict_proba(X_test)[:, 1]\\n\",\"        preds.extend(probs)\\n\",\"        actuals.extend(y_test)\\n\",\"\\n\",\"    return scores, preds, actuals\"]}]}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1ODmQunSVNP"
      },
      "source": [
        "# A1) Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwWPHLvlIwtj"
      },
      "source": [
        "This study aims to utilise machine learning techniques to predict the likelihood of success in Cognitive Behavioral Therapy (CBT) using data. The dataset encompasses various demographic, treatment-related, and psychological assessment variables, offering insights into patient profiles and treatment outcomes.  By leveraging various analytical skills, predictive models will aid in revealing the success probability for CBT in individual patients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV1f10TTIv53"
      },
      "source": [
        "## 1.2 Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vw4-ITlmfz1"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "\n",
        "    drive.mount(\"/content/drive\") # access google drive folder\n",
        "\n",
        "    file_path = '/content/drive/MyDrive/Data/Dissertation_Data/'\n",
        "    df = pd.read_csv(file_path + 'mental_health.csv', delimiter = ',') # csv\n",
        "    raw_df = df.copy()\n",
        "\n",
        "    return df, raw_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nLysn57q38W"
      },
      "outputs": [],
      "source": [
        "def manage_df(df, name, option):\n",
        "\n",
        "    diss_path = '/content/drive/MyDrive/Data/Dissertation_Data/'\n",
        "\n",
        "    if option == 'save':\n",
        "        df.to_csv(file_path + name + '.csv')\n",
        "        print('Saved!')\n",
        "\n",
        "    elif option == 'load':\n",
        "        df = pd.read_csv(file_path + name + '.csv', index_col=0)\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtShKAeACyh2"
      },
      "source": [
        "2.2 Non-Questionnaire Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I8pmofOI0n8"
      },
      "outputs": [],
      "source": [
        "def rename_features(df): # non-questionnaire features\n",
        "\n",
        "    df.rename(columns = {\n",
        "        'Unnamed: 0': 'CaseID',\n",
        "        'IAPTus_Num': 'ClientID',\n",
        "        'Referral Date': 'ReferralDate',\n",
        "        'Age_ReferralRequest_ReceivedDate': 'AgeAtReferralRequest',\n",
        "        'EthnicDescGroupCode': 'EthnicCode',\n",
        "        'EthnicCategoryGroupShortCode': 'EthnicCodeShort',\n",
        "        'GenderIdentity': 'Gender',\n",
        "        'SexualOrientationDesc': 'SexualOrientation',\n",
        "        'EndDescGroupShort': 'Treated',\n",
        "        'AllocatedCareProfNameCode': 'TherapistID',\n",
        "        'JobTitleCode': 'ExperienceLevel',\n",
        "        'Days to first assessment': 'DaystoAssessment',\n",
        "        'Days to first treatment': 'DaystoTreatment',\n",
        "        'CountOfAttendedCareContacts': 'CareContacts',\n",
        "        'RecoveryDesc': 'Recovery',\n",
        "        'ReliableRecoveryDesc': 'ReliableRecovery',\n",
        "        'Date': 'DateOfQuestionnaire'},\n",
        "        inplace = True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XZE6Oq-LHCV"
      },
      "outputs": [],
      "source": [
        "def create_referral_count(df):\n",
        "    def count_referrals(col):\n",
        "        if '_1' in col:\n",
        "            return 1\n",
        "        elif '_2' in col:\n",
        "            return 2\n",
        "        elif '_3' in col:\n",
        "            return 3\n",
        "        elif '_4' in col:\n",
        "            return 4\n",
        "        elif '_5' in col:\n",
        "            return 5\n",
        "        else:\n",
        "            return 1\n",
        "    df.insert(2, \"ReferralCount\", df['ClientID'].apply(count_referrals)) # introduce next to ClientID\n",
        "    return df\n",
        "\n",
        "def clean_client_id(df):\n",
        "    for text in ['_1', '_2', '_3', '_4']:\n",
        "        df['ClientID'] = df['ClientID'].str.replace(text, '') # remove ending\n",
        "    df['ClientID'] = pd.to_numeric(df['ClientID'])\n",
        "    return df\n",
        "\n",
        "def convert_features_to_datetime(df):\n",
        "    df['ReferralDate'] = pd.to_datetime(df['ReferralDate'], format = '%d/%m/%Y')\n",
        "    df['DateOfQuestionnaire'] = pd.to_datetime(df['DateOfQuestionnaire'], format = '%d/%m/%Y')\n",
        "    return df\n",
        "\n",
        "def convert_float_features_to_int(df):\n",
        "    df['EthnicCode'] = df['EthnicCode'].astype('Int64') # Int deals with NaNs, int does not\n",
        "    df['EthnicCodeShort'] = df['EthnicCodeShort'].astype('Int64')\n",
        "    df['TherapistID'] = df['TherapistID'].astype('Int64')\n",
        "    df['ExperienceLevel'] = df['ExperienceLevel'].astype('Int64')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAOanAnaN9hJ"
      },
      "outputs": [],
      "source": [
        "def map_features(df):\n",
        "\n",
        "    Gender_map = {\n",
        "        'CHANGE ME': np.nan,\n",
        "        'X': np.nan}\n",
        "    df['Gender'] = df['Gender'].replace(Gender_map).astype('Int64')\n",
        "\n",
        "    Treated_map = {\n",
        "        'Seen and treated': 1,\n",
        "        'Seen but not treated': 0}\n",
        "    df['Treated'] = df['Treated'].replace(Treated_map).astype('Int64')\n",
        "\n",
        "    ReliableChangeDesc_map = {\n",
        "        'Reliable improvement': 2, # what about(-1, 0, 1)?\n",
        "        'No reliable change': 1,\n",
        "        'Reliable deterioration': 0,\n",
        "        'Not applicable': np.nan}\n",
        "    df['ReliableChangeDesc'] = df['ReliableChangeDesc'].replace(ReliableChangeDesc_map).astype('Int64')\n",
        "\n",
        "    Recovery_map = {\n",
        "        'At recovery': 1,\n",
        "        'Not at recovery': 0,\n",
        "        'Not applicable': np.nan}\n",
        "    df['Recovery'] = df['Recovery'].replace(Recovery_map).astype('Int64')\n",
        "\n",
        "    ReliableRecovery_map = {\n",
        "        'Reliable recovery': 1,\n",
        "        'No reliable recovery': 0,\n",
        "        'Not applicable': np.nan}\n",
        "    df['ReliableRecovery'] = df['ReliableRecovery'].replace(ReliableRecovery_map).astype('Int64')\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quT-_D2KcbCD"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode_features(df):\n",
        "\n",
        "    EndDesc_cols = pd.get_dummies(df['EndDesc'], prefix = 'EndDesc')\n",
        "    EndDesc_index = df.columns.get_loc('EndDesc')\n",
        "    df = pd.concat([df.iloc[:, :EndDesc_index + 1], EndDesc_cols, df.iloc[:, EndDesc_index + 1:]], axis = 1)\n",
        "    df = df.drop(columns = ['EndDesc'])\n",
        "\n",
        "    EndDescShort_cols = pd.get_dummies(df['EndDescShort'], prefix = 'EndDescShort')\n",
        "    EndDescShort_index = df.columns.get_loc('EndDescShort')\n",
        "    df = pd.concat([df.iloc[:, :EndDescShort_index + 1], EndDescShort_cols, df.iloc[:, EndDescShort_index + 1:]], axis = 1)\n",
        "    df = df.drop(columns = ['EndDescShort'])\n",
        "\n",
        "    return df\n",
        "\n",
        "def convert_to_int_features(df):\n",
        "    int_cols = ['SexualOrientation', 'DaystoAssessment', 'DaystoTreatment', 'CareContacts']\n",
        "    for col in int_cols:\n",
        "        df[col] = df[col].astype('Int64')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu8simKq4woU"
      },
      "outputs": [],
      "source": [
        "def plot_features(df):\n",
        "\n",
        "    plot_cols = 4\n",
        "    plot_rows = len(df.columns)//4 + 1\n",
        "\n",
        "    plt.figure(figsize = (plot_cols*3, plot_rows*3))\n",
        "\n",
        "    for i, col in enumerate(df.columns, start = 1):\n",
        "        if df[col].dtype in ['int64', 'float64', 'Int64', 'datetime64[ns]', 'bool']:\n",
        "            data = df[col].dropna()\n",
        "            if df[col].dtype == 'bool':  # convert boolean to integers\n",
        "                data = data.astype(int)\n",
        "            plt.subplot(plot_rows, 4, i)\n",
        "            plt.hist(data, bins = 20, color='darkgrey', edgecolor='white')\n",
        "            plt.title(col)\n",
        "            plt.xlabel(col)\n",
        "            plt.ylabel('Frequency')\n",
        "\n",
        "    plt.suptitle('Data Plots', y = 1, fontsize = 24)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nfsedT9_wYy"
      },
      "outputs": [],
      "source": [
        "def preprocess_nonques_features(df):\n",
        "\n",
        "    df = rename_features(df)\n",
        "    df = create_referral_count(df)\n",
        "    df = clean_client_id(df)\n",
        "    df = convert_features_to_datetime(df)\n",
        "    df = convert_float_features_to_int(df)\n",
        "    df = map_features(df)\n",
        "    df = one_hot_encode_features(df)\n",
        "    df = convert_to_int_features(df)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW2b1hV0CUIz"
      },
      "source": [
        "2.3 Questionnaire Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsoRt1DtCcWD"
      },
      "outputs": [],
      "source": [
        "def preprocess_ques_features(df):\n",
        "\n",
        "    # convert all variables to float\n",
        "    for col in df.columns[27:]:\n",
        "        df[col] = pd.to_numeric(df[col], errors = 'coerce') # (I think this also removes '.')\n",
        "\n",
        "    item_cols = df.columns.str.contains('Item').tolist()\n",
        "    item_cols = df.columns[item_cols]\n",
        "    for col in item_cols:\n",
        "        df[col] = df[col].apply(lambda x: x if pd.isna(x) or x.is_integer() else np.nan) # NaN non-integer values\n",
        "        df[col] = df[col].astype('Int64') # convert to int\n",
        "\n",
        "    thresh_cols = df.columns.str.contains('Threshold').tolist()\n",
        "    thresh_cols = df.columns[thresh_cols]\n",
        "    for col in thresh_cols:\n",
        "        df[col] = df[col].apply(lambda x: x if pd.isna(x) or x.is_integer() else np.nan)\n",
        "        df[col] = df[col].astype('Int64')\n",
        "\n",
        "    total_cols = df.columns.str.contains('Total').tolist()\n",
        "    total_cols = df.columns[total_cols]\n",
        "    for col in total_cols:\n",
        "        df[col] = df[col].apply(lambda x: x if pd.isna(x) or x.is_integer() else np.nan)\n",
        "        df[col] = df[col].astype('Int64')\n",
        "\n",
        "    # bool cols (all)\n",
        "    bool_cols = df.select_dtypes(include = bool)\n",
        "    for col in bool_cols:\n",
        "        df[col] = df[col].astype('Int64')\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gQIQg-Y-TmD"
      },
      "source": [
        "2 Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3XWbNeNlGLl"
      },
      "outputs": [],
      "source": [
        "def Clean_Data(df):\n",
        "\n",
        "    # clean non-questionnaire data\n",
        "    df = preprocess_nonques_features(df)\n",
        "\n",
        "    # clean questionnaire data\n",
        "    df = preprocess_ques_features(df)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THGbfknuDUj7"
      },
      "source": [
        "## 1.3 EDA Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjkFhRmV-aVr"
      },
      "source": [
        "3 EDA Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYGl8zk5Dx48"
      },
      "outputs": [],
      "source": [
        "def single_summary_plot(df, tic_freq):\n",
        "\n",
        "    statistics = df.describe()\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    for index, row in statistics.iterrows():\n",
        "        plt.plot(statistics.columns, row, label=index)\n",
        "\n",
        "    plt.xlabel('Column')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Summary Statistics')\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.xticks(statistics.columns[::tic_freq])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fkAzO2bD3mI"
      },
      "outputs": [],
      "source": [
        "def summary_plot(dfs, dfs_names):\n",
        "\n",
        "    # subplot dimensions\n",
        "    num_dfs = len(dfs)\n",
        "    num_rows = num_dfs // 2 + num_dfs % 2\n",
        "\n",
        "    # plot\n",
        "    plt.figure(figsize=(15, 5 * num_rows))\n",
        "    for i, df in enumerate(dfs):\n",
        "\n",
        "        statistics = df.describe()\n",
        "\n",
        "        plt.subplot(num_rows, 2, i+1)\n",
        "        for index, row in statistics.iterrows():\n",
        "            plt.plot(statistics.columns, row, label=index)\n",
        "\n",
        "        plt.xlabel('Column')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title(f'{dfs_names[i]} Summary')\n",
        "        plt.xticks(statistics.columns[::2])\n",
        "        plt.legend(loc = 'center right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGdQ8RZ5f26v"
      },
      "source": [
        "## 1.4 Processing Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74NDOIP3MXhd"
      },
      "source": [
        "### 4.1 Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK5D0BZEMVkn"
      },
      "outputs": [],
      "source": [
        "def find_duplicates_features(df):\n",
        "    dupe_cols = np.transpose(df).duplicated()\n",
        "    dupe_cols = df.columns[dupe_cols].tolist()\n",
        "    dupes_of = {}\n",
        "    for col_name in dupe_cols:\n",
        "        col_values = df[col_name]\n",
        "        dupes = [other_col for other_col in df.columns if (other_col != col_name) and df[other_col].equals(col_values)]\n",
        "        dupes_of[col_name] = dupes\n",
        "        print(f'{col_name} is a duplicate of: {dupes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9_mblKrB0dM"
      },
      "source": [
        "Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjiI4WFX_i2r"
      },
      "outputs": [],
      "source": [
        "def drop_duplicate_features(df):\n",
        "    dupe_cols = np.transpose(df).duplicated()\n",
        "    dupe_cols = df.columns[dupe_cols].tolist()\n",
        "    df = df.drop(columns = dupe_cols)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMCGtYfVQbR3"
      },
      "source": [
        "### 4.2 Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7h4646W_D_y"
      },
      "outputs": [],
      "source": [
        "def column_contents(df):\n",
        "    pot_cols = []\n",
        "    for col in df.columns:\n",
        "        if not df[col].isin([0, 1, pd.NA]).all():\n",
        "            pot_cols.append(col)\n",
        "    if len(pot_cols) == 0:\n",
        "        print(f'Columns only contain 0, 1 and <NA>')\n",
        "    else:\n",
        "        print(f'Columns contining more than 0, 1 and <NA>: {pot_cols}')\n",
        "    return pot_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX_6goE0QeVF"
      },
      "outputs": [],
      "source": [
        "def mad(df):\n",
        "    median = df.median()\n",
        "    deviations = np.abs(df - median)\n",
        "    mad_val = deviations.median() # MAD\n",
        "    return mad_val\n",
        "\n",
        "def modified_zscore(df, threshold = 3.5):\n",
        "    median = df.median()\n",
        "    mad_val = mad(df) # MAD\n",
        "    modified_zscores = 0.6745 * (df - median) / mad_val # modified Z-score\n",
        "    return np.abs(modified_zscores) > threshold\n",
        "\n",
        "def find_outlier_cols(df, threshold = 3.5): # counts also\n",
        "\n",
        "    outlier_df = modified_zscore(df)\n",
        "    outlier_cols = set()\n",
        "    for col in outlier_df.columns:\n",
        "        outlier_list = []\n",
        "        for i, val in outlier_df[col].items():\n",
        "            if pd.notna(val) and val:\n",
        "                outlier_list.append(df.iloc[i][col])\n",
        "        if outlier_list:\n",
        "            outlier_cols.add(col)\n",
        "            print(f'{col} outlier count: {len(outlier_list)}')\n",
        "\n",
        "    # return outlier_df\n",
        "    outlier_cols = list(outlier_cols)\n",
        "    outlier_df = df[outlier_cols]\n",
        "    return outlier_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NytZCSxER-sR"
      },
      "outputs": [],
      "source": [
        "def plot_outlier_cols(outlier_df, title, legend=True):\n",
        "    outlier_df.plot(figsize = (5, 3))\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Total Score')\n",
        "    if legend:\n",
        "        plt.legend(loc='upper right')\n",
        "    else:\n",
        "        plt.gca().legend().set_visible(False)\n",
        "    plt.show()\n",
        "\n",
        "def plot_datetime_features(df):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i, col in enumerate(datetime_cols):\n",
        "        plt.subplot(1, 2, i + 1)\n",
        "        df[col].hist(bins = 100, color = 'grey', edgecolor = 'white')\n",
        "        plt.title(col)\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.grid(False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_outlier_cols2(outlier_df, title, color):\n",
        "\n",
        "    plot_cols = 4\n",
        "    plot_rows = len(outlier_df.columns)//4 + 1\n",
        "    plt.figure(figsize = (plot_cols*3, plot_rows*3))\n",
        "\n",
        "    for i, col in enumerate(outlier_df.columns, start = 1):\n",
        "        plt.subplot(plot_rows, 4, i)\n",
        "        outlier_df[col].plot(color=color)\n",
        "        plt.title(col)\n",
        "        plt.xlabel('Index')\n",
        "        plt.ylabel('Total Score')\n",
        "\n",
        "    plt.suptitle(title, y = 1, fontsize = 24)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BmANarfB5P7"
      },
      "source": [
        "Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcfP6XRJ3Jfn"
      },
      "outputs": [],
      "source": [
        "def replace_ques_outliers(df): # shortens search using quartiles\n",
        "\n",
        "    item_cols = [col for col in df.columns if 'Item' in col]\n",
        "    item_df = df[item_cols]\n",
        "\n",
        "    total_cols = [col for col in df.columns if 'Total' in col]\n",
        "    total_df = df[total_cols]\n",
        "\n",
        "    # replace negative totals with nan\n",
        "    for col in total_df:\n",
        "        df.loc[df[col] < 0, col] = np.nan # replace in df\n",
        "        total_df.loc[total_df[col] < 0, col] = np.nan # replace in total_df\n",
        "\n",
        "    # find col contents\n",
        "    def create_pot_cols(df):\n",
        "        pot_cols = []\n",
        "        for col in df.columns:\n",
        "            if not df[col].isin([0, 1, pd.NA]).all():\n",
        "                pot_cols.append(col)\n",
        "        return pot_cols\n",
        "\n",
        "    # detect whether cols are just 0,1,na\n",
        "    item_pot_cols = create_pot_cols(item_df)\n",
        "    total_pot_cols = create_pot_cols(total_df)\n",
        "\n",
        "    # potential columns containing outliers (shortening the search, only these can contain outliers)\n",
        "    total_pot_df = total_df[total_pot_cols]\n",
        "    item_pot_df = item_df[item_pot_cols]\n",
        "\n",
        "    # outlier counter and df function\n",
        "    def create_outlier_df(df, threshold = 3.5):\n",
        "\n",
        "        outlier_df = modified_zscore(df)\n",
        "        outlier_cols = set()\n",
        "        for col in outlier_df.columns:\n",
        "            outlier_list = []\n",
        "            for i, val in outlier_df[col].items():\n",
        "                if pd.notna(val) and val:\n",
        "                    outlier_list.append(df.iloc[i][col])\n",
        "            if outlier_list:\n",
        "                outlier_cols.add(col)\n",
        "\n",
        "        # return outlier_df\n",
        "        outlier_cols = list(outlier_cols)\n",
        "        outlier_df = df[outlier_cols]\n",
        "        return outlier_df\n",
        "\n",
        "    # find outliers in variables\n",
        "    total_outlier_df = create_outlier_df(total_pot_df)\n",
        "    item_outlier_df = create_outlier_df(item_pot_df)\n",
        "\n",
        "    # replace outliers with nan\n",
        "    for col in total_outlier_df:\n",
        "        df.loc[df[col] > 200, col] = np.nan\n",
        "        total_outlier_df.loc[total_outlier_df[col] > 200, col] = np.nan\n",
        "\n",
        "    for col in item_outlier_df:\n",
        "        df.loc[df[col] > 50, col] = np.nan\n",
        "        item_outlier_df.loc[item_outlier_df[col] > 50, col] = np.nan\n",
        "\n",
        "    df.loc[df['Total14'] > 37, 'Total14'] = np.nan\n",
        "    df.loc[df['Item70'] > 3, 'Item70'] = np.nan\n",
        "    df.loc[df['Item132'] > 5, 'Item132'] = np.nan\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOvHn6v9CMeP"
      },
      "outputs": [],
      "source": [
        "def replace_outliers(df):\n",
        "    df.loc[df['AgeAtReferralRequest'] == 0, 'AgeAtReferralRequest'] = np.nan\n",
        "    df = replace_ques_outliers(df)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piAcrqMCUf_b"
      },
      "source": [
        "### 4.3 Constant and Quasi Constant Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odjJkEB63184"
      },
      "outputs": [],
      "source": [
        "def drop_const_features(df):\n",
        "    const_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
        "    df = df.drop(columns = const_cols)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvejzgiDUj3L"
      },
      "outputs": [],
      "source": [
        "def plot_datetime_features(df):\n",
        "    plt.figure(figsize = (10,4))\n",
        "    for i, col in enumerate(datetime_cols, 1):\n",
        "        plt.subplot(1, len(datetime_cols), i)\n",
        "        plt.hist(df[col], bins = 20, color = 'grey', edgecolor = 'white')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title(col)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JibfK4xBB-pj"
      },
      "source": [
        "Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RijZyYsG_a4k"
      },
      "outputs": [],
      "source": [
        "def quasi_percentage(df):\n",
        "\n",
        "    # ordinal variables\n",
        "    ordinal_df = df.select_dtypes(include = ['int64', 'Int64'])\n",
        "\n",
        "    # select col modes\n",
        "    modes = ordinal_df.mode() # mode of each col\n",
        "    modes = modes.iloc[0] # select modes only\n",
        "\n",
        "    # calc quasi percentage\n",
        "    quasi_percentages = (ordinal_df == modes).sum() / len(ordinal_df) # how quasi a col is\n",
        "\n",
        "    return quasi_percentages\n",
        "\n",
        "# drop quasi features\n",
        "def drop_quasi_features(df, threshold):\n",
        "\n",
        "    # calculate quasi percentage\n",
        "    quasi_percentages = quasi_percentage(df)\n",
        "\n",
        "    # 0.995 is about 3 observations\n",
        "    exceeding_threshold_columns = quasi_percentages[quasi_percentages > threshold].index\n",
        "    df = df.drop(columns = exceeding_threshold_columns)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTOHK-xOBdpr"
      },
      "source": [
        "### 4.4 Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uxid4K6dlg8"
      },
      "outputs": [],
      "source": [
        "def select_future_features(df):\n",
        "    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "    #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\n",
        "    future_vars = df.drop(columns = ['ReliableChangeDesc', 'Recovery', 'ReliableRecovery'] + EndDesc_cols)\n",
        "    return future_vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_o2RbFkMvaX"
      },
      "outputs": [],
      "source": [
        "def generate_corr_matricies(df, load_matrices):\n",
        "\n",
        "    file_path = '/content/drive/MyDrive/Data/Dissertation_Data/'\n",
        "\n",
        "    if not load_matrices:\n",
        "        # select explanatory variables, removing info on future\n",
        "        future_df = select_future_features(df)\n",
        "\n",
        "        # create correlation matrices\n",
        "        kendall_corr = future_df.corr(method='kendall').abs()\n",
        "        spearman_corr = future_df.corr(method='spearman').abs()\n",
        "        kendall_corr.to_csv(file_path + 'kendall_corr.csv') # save\n",
        "        spearman_corr.to_csv(file_path + 'spearman_corr.csv') # save\n",
        "\n",
        "    else:\n",
        "        kendall_corr = pd.read_csv(file_path + 'kendall_corr.csv', index_col=0)\n",
        "        spearman_corr = pd.read_csv(file_path + 'spearman_corr.csv', index_col=0)\n",
        "\n",
        "    return kendall_corr, spearman_corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHv11Vma1dWy"
      },
      "outputs": [],
      "source": [
        "def plot_correlation_matrices(corr_matricies):\n",
        "\n",
        "    plt.figure(figsize=(8, 3))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    sns.heatmap(corr_matricies[0], annot = False, cmap = 'flare', fmt = \".2f\", xticklabels = False, yticklabels = False)\n",
        "    plt.title('Kendall Correlation Matrix')\n",
        "\n",
        "    plt.subplot(122)\n",
        "    sns.heatmap(corr_matricies[1], annot = False, cmap = 'flare', fmt = \".2f\", xticklabels = False, yticklabels = False)\n",
        "    plt.title('Spearman Correlation Matrix')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaIUUFSLdsBA"
      },
      "outputs": [],
      "source": [
        "# count correlated variable pairs above each threshold\n",
        "def count_correlated_pairs(correlation_matrix, thresholds):\n",
        "\n",
        "    counts = {}\n",
        "    for threshold in thresholds:\n",
        "        correlated_vars = (correlation_matrix.abs() > threshold)\n",
        "        # get upper triangle\n",
        "        np.fill_diagonal(correlated_vars.values, False)\n",
        "        upper_triangle = correlated_vars.values[np.triu_indices_from(correlated_vars, k=1)]\n",
        "        count = np.sum(upper_triangle)\n",
        "        counts[threshold] = count\n",
        "    return counts\n",
        "\n",
        "def correlated_features_above_treshold(matricies, thresholds):\n",
        "\n",
        "    print('Kendall Correlation:')\n",
        "    kendall_counts = count_correlated_pairs(matricies[0], thresholds)\n",
        "    for threshold, count in kendall_counts.items():\n",
        "        print(f'{threshold * 100}%: {count}')\n",
        "\n",
        "    print('Spearman Correlation:')\n",
        "    spearman_counts = count_correlated_pairs(matricies[1], thresholds)\n",
        "    for threshold, count in spearman_counts.items():\n",
        "        print(f'{threshold * 100}%: {count}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs7au_sOADwf"
      },
      "outputs": [],
      "source": [
        "def drop_corr_cols(df, corr_matrix, threshold, ignore=True):\n",
        "\n",
        "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "    corr_cols = [col for col in upper_tri.columns if any(upper_tri[col] > threshold)]\n",
        "\n",
        "    try:\n",
        "        df = df.drop(corr_cols, axis=1)\n",
        "    except KeyError as e:\n",
        "        if ignore:\n",
        "            #print(f\"Ignoring KeyError: {e}\")\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fj1PpVXCl-K"
      },
      "source": [
        "Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VBGPv-Udz12"
      },
      "outputs": [],
      "source": [
        "def remove_corr_features(df, load_matrices, threshold):\n",
        "\n",
        "    kendall_corr, spearman_corr = generate_corr_matricies(df, load_matrices)\n",
        "\n",
        "    # remove highly correlated variables\n",
        "    df = drop_corr_cols(df, kendall_corr, threshold)\n",
        "    df = drop_corr_cols(df, spearman_corr, threshold)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pSpe5vf52AJ"
      },
      "source": [
        "### 4.5 Missing Values within Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ooEjn1a56BK"
      },
      "outputs": [],
      "source": [
        "def drop_missing_values_col(df, threshold):\n",
        "\n",
        "    missing_value_percentage = df.isna().mean(axis=0) # cols\n",
        "\n",
        "    drop_cols = missing_value_percentage[missing_value_percentage > threshold].index\n",
        "    df = df.drop(drop_cols, axis=1)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFVrpck2Ls5P"
      },
      "source": [
        "### 4.6 Missing Values within Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRvOzmlg5SMq"
      },
      "outputs": [],
      "source": [
        "def drop_missing_values_row(df, threshold):\n",
        "\n",
        "    missing_value_percentage = df.isna().mean(axis=1) # rows\n",
        "\n",
        "    drop_cols = missing_value_percentage[missing_value_percentage > threshold].index\n",
        "    df = df.drop(drop_cols, axis=0)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Gp1AdsW3ya"
      },
      "source": [
        "### 4.7 Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drv67OT8W3QC"
      },
      "outputs": [],
      "source": [
        "def standardise(df):\n",
        "    scaler = StandardScaler()\n",
        "    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv0ov3LGKkTv"
      },
      "source": [
        "### 4.8 Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7Ji0tiXTs0X"
      },
      "outputs": [],
      "source": [
        "def impute_data(df, method, n_neighbours=None, max_iter=None):\n",
        "\n",
        "    original_dtypes = df.dtypes.to_dict()\n",
        "\n",
        "    #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\n",
        "    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "    nontarget_df = df.drop(columns = ['ReliableChangeDesc', 'Recovery', 'ReliableRecovery'] + EndDesc_cols)\n",
        "\n",
        "    datetime_cols = nontarget_df.select_dtypes(include=['datetime64[ns]']).columns\n",
        "    for col in datetime_cols:\n",
        "        nontarget_df[col] = pd.to_numeric(nontarget_df[col])\n",
        "\n",
        "    if method == 'none':\n",
        "        nontarget_mat = nontarget_df.values\n",
        "\n",
        "    elif method == 'mean':\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        nontarget_mat = imputer.fit_transform(nontarget_df) # *numpy array*\n",
        "\n",
        "    elif method == 'median':\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        nontarget_mat = imputer.fit_transform(nontarget_df)\n",
        "\n",
        "    elif method == 'knn':\n",
        "        imputer = KNNImputer(n_neighbors=n_neighbours)\n",
        "        nontarget_mat = imputer.fit_transform(nontarget_df)\n",
        "\n",
        "    elif method == 'iterative':\n",
        "        imputer = IterativeImputer(max_iter=max_iter, random_state=2001)\n",
        "        nontarget_mat = imputer.fit_transform(nontarget_df)\n",
        "\n",
        "    nontarget_df.iloc[:, :] = nontarget_mat.tolist()\n",
        "\n",
        "    # add imputed data to df\n",
        "    for col in nontarget_df.columns:\n",
        "        df[col] = nontarget_df[col]\n",
        "\n",
        "    if method == 'none':\n",
        "        pass\n",
        "\n",
        "    else:\n",
        "        for col, dtype in original_dtypes.items():\n",
        "            if dtype == 'datetime64[ns]':\n",
        "                pass # keep as float\n",
        "            else:\n",
        "                try:\n",
        "                    df[col] = df[col].astype(dtype)\n",
        "                except (ValueError, TypeError):\n",
        "                    # *convert to original dtype*\n",
        "                    df[col] = df[col].round().astype(int)\n",
        "                    df[col] = df[col].astype(dtype)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW5BGatARrMi"
      },
      "outputs": [],
      "source": [
        "# def multi_impute_data(df, method='knn', n_neighbours=3, iter=1):\n",
        "\n",
        "#     EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "#     #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\n",
        "#     nontarget_df = df.drop(columns = ['ReliableChangeDesc', 'Recovery', 'ReliableRecovery'] + EndDesc_cols)\n",
        "\n",
        "#     datetime_cols = nontarget_df.select_dtypes(include=['datetime64[ns]']).columns\n",
        "#     for col in datetime_cols:\n",
        "#         nontarget_df[col] = nontarget_df[col].astype(int) / 10**9\n",
        "\n",
        "#     ordinal_cols = list(nontarget_df.select_dtypes(include=['Int64', 'int64']).columns)\n",
        "#     imputed_dfs = []\n",
        "\n",
        "#     for i in range(iter):\n",
        "\n",
        "#         copy_df = nontarget_df.copy()\n",
        "\n",
        "#         if method == 'mean':\n",
        "#             mean_imputer = SimpleImputer(strategy='mean')\n",
        "#             copy_df[nontarget_df.columns] = mean_imputer.fit_transform(copy_df[nontarget_df.columns])\n",
        "\n",
        "#         elif method == 'median':\n",
        "#             median_imputer = SimpleImputer(strategy='median')\n",
        "#             copy_df[nontarget_df.columns] = median_imputer.fit_transform(copy_df[nontarget_df.columns])\n",
        "\n",
        "#         elif method == 'knn':\n",
        "#             KNN_imputer = KNNImputer(n_neighbors=n_neighbours)\n",
        "#             copy_df[nontarget_df.columns] = KNN_imputer.fit_transform(copy_df)\n",
        "\n",
        "#         imputed_dfs.append(copy_df)\n",
        "\n",
        "#     df = np.mean(imputed_dfs, axis=0)\n",
        "#     print(df.dtypes)\n",
        "#     for col in datetime_cols:\n",
        "#         df[col] = pd.to_datetime(df[col], unit='ns')\n",
        "\n",
        "#     # convert imputed values for ordinal variables to discrete values\n",
        "#     for col in ordinal_cols:\n",
        "#         df[col] = df[col].round().astype(int)\n",
        "\n",
        "#     return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILBLDx4T1ycH"
      },
      "source": [
        "### 4.9 Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_pDAKYE1xx_"
      },
      "outputs": [],
      "source": [
        "def create_date_features(df):\n",
        "\n",
        "    # df['ReferralYear'] = df['ReferralDate'].dt.year\n",
        "    # df['ReferralMonth'] = df['ReferralDate'].dt.month\n",
        "    # df['ReferralWeek'] = df['ReferralDate'].dt.isocalendar().week\n",
        "    # df['ReferralDay'] = df['ReferralDate'].dt.day\n",
        "    # df['ReferralHour'] = df['ReferralDate'].dt.hour\n",
        "    # df['ReferralWeekDay'] = df['ReferralDate'].dt.dayofweek\n",
        "    # df['ReferralYearDay'] = df['ReferralDate'].dt.dayofyear\n",
        "\n",
        "    df['YearofQuestionnaire'] = df['DateOfQuestionnaire'].dt.year\n",
        "    df['MonthofQuestionnaire'] = df['DateOfQuestionnaire'].dt.month\n",
        "    df['WeekofQuestionnaire'] = df['DateOfQuestionnaire'].dt.isocalendar().week\n",
        "    df['DayofQuestionnaire'] = df['DateOfQuestionnaire'].dt.day\n",
        "    df['HourofQuestionnaire'] = df['DateOfQuestionnaire'].dt.hour\n",
        "    df['WeekDayofQuestionnaire'] = df['DateOfQuestionnaire'].dt.dayofweek\n",
        "    df['YearDayofQuestionnaire'] = df['DateOfQuestionnaire'].dt.dayofyear\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgJIuZFR25pq"
      },
      "outputs": [],
      "source": [
        "def feature_engineering(df):\n",
        "\n",
        "    df = create_date_features(df)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8mML9d7ahEe"
      },
      "source": [
        "### 4.10 Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6uBX7QQanjd"
      },
      "outputs": [],
      "source": [
        "def find_important_features(df, k_features, target='Recovery'):\n",
        "\n",
        "    \"\"\"\n",
        "    k is the number of features each feature selection method should select.\n",
        "    NOT the number of features returned\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if k_features == None:\n",
        "        important_features = df.columns\n",
        "\n",
        "    else:\n",
        "        features = df.dropna()\n",
        "        EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "        explanatory = features.drop(['Recovery', 'ReliableRecovery', 'ReliableChangeDesc'] + EndDesc_cols, axis = 1)\n",
        "        target = features[target]\n",
        "\n",
        "        # fishers score\n",
        "        kbest_fisher = SelectKBest(score_func=f_classif, k=k_features)\n",
        "        selected_fisher = kbest_fisher.fit_transform(explanatory, target)\n",
        "        index_fisher = kbest_fisher.get_support(indices=True)\n",
        "        names_fisher = explanatory.columns[index_fisher]\n",
        "\n",
        "        # mutual information gain\n",
        "        def mutual_info_classif_wseed(X, y):\n",
        "            return mutual_info_classif(X, y, random_state=11)\n",
        "        kbest_gain = SelectKBest(score_func=mutual_info_classif_wseed, k=k_features)\n",
        "        selected_gain = kbest_gain.fit_transform(explanatory, target)\n",
        "        index_gain = kbest_gain.get_support(indices=True)\n",
        "        names_gain = explanatory.columns[index_gain]\n",
        "\n",
        "        important_combined = set(names_fisher).union(set(names_gain))\n",
        "        important_features = list(important_combined)\n",
        "\n",
        "    return important_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTQjIpLqg-1a"
      },
      "outputs": [],
      "source": [
        "def select_important_features(df, k_features, target='Recovery'):\n",
        "\n",
        "    important_features = find_important_features(df, k_features, target='Recovery')\n",
        "\n",
        "    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "    df = df[['Recovery', 'ReliableRecovery', 'ReliableChangeDesc'] + EndDesc_cols + important_features]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3-sW_8Qa2Pt"
      },
      "source": [
        "### 4 Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggq9DTNw4ycz"
      },
      "outputs": [],
      "source": [
        "def Prepare_Data(df,\n",
        "                 quasi_thresh=1.0,\n",
        "                 corr_thresh=1.0,\n",
        "                 load_matrices=True,\n",
        "                 col_thresh=1.0,\n",
        "                 row_thresh=1.0,\n",
        "                 imputation_method='knn',\n",
        "                 n_neighbours=10,\n",
        "                 max_iter=10,\n",
        "                 k_features=200):\n",
        "\n",
        "    df = drop_duplicate_features(df)\n",
        "    df = replace_outliers(df)\n",
        "    df = drop_const_features(df)\n",
        "    df = drop_quasi_features(df, quasi_thresh)\n",
        "    df = remove_corr_features(df, corr_thresh, load_matrices)\n",
        "    df = drop_missing_values_col(df, col_thresh)\n",
        "    df = drop_missing_values_col(df, row_thresh)\n",
        "    #df = standardise(df)\n",
        "    df = impute_data(df, imputation_method, n_neighbours, max_iter)\n",
        "    #df = feature_engineering(df)\n",
        "    df = select_important_features(df, k_features)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkNvB98vLreZ"
      },
      "source": [
        "## 1.5 Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV4vMwKNS5vp"
      },
      "source": [
        "### Summary Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzadOsBMS8NH"
      },
      "outputs": [],
      "source": [
        "def ModelSelection_Summary(model):\n",
        "\n",
        "# pr curve\n",
        "\n",
        "    scores = model[0]\n",
        "    preds = model[1]\n",
        "    actuals = model[2]\n",
        "\n",
        "    print('Average accuracy score: {0}'.format(np.average(scores)))\n",
        "\n",
        "    prec, recall, _ = metrics.precision_recall_curve(actuals, preds)\n",
        "    print('AUPRC score: {0}\\n'.format(metrics.auc(recall, prec)))\n",
        "\n",
        "    # plot pr curve\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(recall, prec, marker='.')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.show()\n",
        "\n",
        "# confusion matrix\n",
        "\n",
        "    # probabilities to binary\n",
        "    threshold = 0.5\n",
        "    binary_preds = [1 if pred >= threshold else 0 for pred in preds]\n",
        "\n",
        "    # plot confusion matrix\n",
        "    conf_matrix = confusion_matrix(actuals, binary_preds)\n",
        "\n",
        "    plt.subplot(121)\n",
        "    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = 'Purples')\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVmXoPNQL1IT"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zN_w7g0LwvE"
      },
      "outputs": [],
      "source": [
        "def XGBoost_ModelSelection(df, target, selector, param_grid, k=5):\n",
        "\n",
        "    # dataset\n",
        "    sample = df.dropna(subset = [target])\n",
        "    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col] #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\n",
        "    X = sample.drop(['ReliableChangeDesc', 'ReliableRecovery', 'Recovery'] + EndDesc_cols, axis = 1)\n",
        "    y = sample[target]\n",
        "    cols = X.columns\n",
        "\n",
        "    # machine learning algorithm\n",
        "    classifier = XGBClassifier()\n",
        "\n",
        "    # feature selection method\n",
        "    if selector == 'SelectFromModel':\n",
        "        selector = SelectFromModel(classifier)\n",
        "    elif selector == 'RFE':\n",
        "        selector = RFE(classifier)\n",
        "    else:\n",
        "        # fill this #\n",
        "        raise ValueError('Unsupported selector type')\n",
        "\n",
        "    # pipeline\n",
        "    pipeline = Pipeline([(\"FS\", selector), (\"classifier\", classifier)])\n",
        "\n",
        "    # initialise lists\n",
        "    scores, preds, actuals = [], [], []\n",
        "\n",
        "    # cross validation and hyperparameter tuning\n",
        "    outer_cv = StratifiedKFold(n_splits = k, shuffle = True)\n",
        "    inner_cv = StratifiedKFold(n_splits = k, shuffle = True) # both set to k for now\n",
        "    for train_index, test_index in outer_cv.split(X, y):\n",
        "\n",
        "        # outer CV train and test sets\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        # hyper-parameter tuning\n",
        "        grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=inner_cv, scoring='accuracy', verbose=0, n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        print(\"Inner CV accuracy: {}\".format(grid_search.best_score_)) # validation sets\n",
        "\n",
        "        # optimal model\n",
        "        estimator = grid_search.best_estimator_\n",
        "\n",
        "        # count features selected\n",
        "        support = estimator.named_steps['FS'].get_support()\n",
        "        num_feat = np.sum(support)\n",
        "        print(\"Number of selected features {0}\".format(num_feat))\n",
        "\n",
        "        # features selected\n",
        "        col_index = np.where(support)[0]\n",
        "        col_names = [cols[col] for col in col_index]\n",
        "        print(\"Selected features {0}\".format(col_names))\n",
        "\n",
        "        # hyperparameters selected\n",
        "        print(\"Max depth {0}\".format(estimator.named_steps[\"classifier\"].max_depth))\n",
        "        print(\"Number of trees {0}\".format(estimator.named_steps[\"classifier\"].n_estimators))\n",
        "        print(\"Learning rate {0}\".format(estimator.named_steps[\"classifier\"].learning_rate))\n",
        "        print(\"Minimum child weight {0}\".format(estimator.named_steps[\"classifier\"].min_child_weight))\n",
        "        print(\"Subsample {0}\".format(estimator.named_steps[\"classifier\"].subsample))\n",
        "        print(\"Colsample bytree {0}\".format(estimator.named_steps[\"classifier\"].colsample_bytree))\n",
        "        print(\"Gamma {0}\".format(estimator.named_steps[\"classifier\"].gamma))\n",
        "        print(\"Lambda {0}\".format(estimator.named_steps[\"classifier\"].reg_lambda))\n",
        "        print(\"Alpha {0}\".format(estimator.named_steps[\"classifier\"].reg_alpha))\n",
        "\n",
        "        # evaluating optimised model on test\n",
        "        predictions = estimator.predict(X_test)\n",
        "        score = metrics.accuracy_score(y_test, predictions)\n",
        "        scores.append(score)\n",
        "        print('Outer CV accuracy: {}'.format(score)) # test sets\n",
        "\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "        probs = estimator.predict_proba(X_test)[:, 1]\n",
        "        preds.extend(probs)\n",
        "        actuals.extend(y_test)\n",
        "\n",
        "    return scores, preds, actuals\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLNplPcaaHL3"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y31WhPuUaIp6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import torch\n",
        "\n",
        "class BERTClassifier(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, model_name='bert-base-uncased', max_length=128):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model.train()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        inputs = self.tokenizer(X.tolist(), return_tensors='pt', truncation=True, padding=True, max_length=self.max_length)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, axis=1)\n",
        "        return preds.numpy()\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        inputs = self.tokenizer(X.tolist(), return_tensors='pt', truncation=True, padding=True, max_length=self.max_length)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "        return probs.numpy()\n",
        "\n",
        "def BERT_ModelSelection(df, text_column, target, param_grid, k=5):\n",
        "    # dataset to model\n",
        "    sample = df.dropna(subset=[target])  # remove missing target values\n",
        "    X = sample[text_column]\n",
        "    y = sample[target]\n",
        "\n",
        "    # machine learning model\n",
        "    classifier = BERTClassifier()\n",
        "\n",
        "    # pipeline\n",
        "    pipeline = Pipeline([('classifier', classifier)])\n",
        "\n",
        "    # initialize lists\n",
        "    scores = []\n",
        "    preds = []\n",
        "    actuals = []\n",
        "\n",
        "    # k-fold CV\n",
        "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
        "    for train_index, test_index in kf.split(X, y):\n",
        "        # train and test data for CV\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        # finding optimal models, hyperparameter tuning\n",
        "        grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=kf, scoring='accuracy', verbose=0, n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        print(\"Internal CV Accuracy of estimator: {}\".format(grid_search.best_score_))\n",
        "\n",
        "        # best estimator\n",
        "        estimator = grid_search.best_estimator_\n",
        "\n",
        "        # print hyperparameters selected\n",
        "        # For BERT, hyperparameters can include learning rate, batch size, etc.\n",
        "        # Example:\n",
        "        print(\"BERT Model used: {}\".format(estimator.named_steps[\"classifier\"].model_name))\n",
        "        print(\"Max length used: {}\".format(estimator.named_steps[\"classifier\"].max_length))\n",
        "\n",
        "        # predicting the test data with the optimized models\n",
        "        predictions = estimator.predict(X_test)\n",
        "        score = accuracy_score(y_test, predictions)\n",
        "        scores.append(score)\n",
        "        print('Accuracy performance on this test set: {}'.format(score))\n",
        "\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "        probs = estimator.predict_proba(X_test)[:, 1]\n",
        "        preds.extend(probs)\n",
        "        actuals.extend(y_test)\n",
        "\n",
        "    return scores, preds, actuals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUvgZXCYEhOq"
      },
      "source": [
        "# A2) Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_8xMCfJ2fAH"
      },
      "source": [
        "## 2.1 Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s98iF4_MF92D"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# statistics\n",
        "from scipy import stats\n",
        "\n",
        "# scaling\n",
        "from sklearn.preprocessing import MinMaxScaler # normalisation\n",
        "from sklearn.preprocessing import Normalizer # works on rows not features\n",
        "from sklearn.preprocessing import StandardScaler # standardisation\n",
        "\n",
        "# missing values\n",
        "import missingno as msno\n",
        "#from scipy.stats import chi2_contingency\n",
        "\n",
        "# imputation\n",
        "from statsmodels.imputation.mice import MICEData\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# machine learning models\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# feature selection\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# fine-tuning\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Wgk3MEkVnCYE",
        "outputId": "a3d4d03e-1911-46d9-a8da-8eff9c94a8ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive\") # access google drive folder\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Data/Dissertation_Data/'\n",
        "df = pd.read_csv(file_path + 'mental_health.csv', delimiter = ',') # csv\n",
        "raw_df = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "aw7BFIiI223z",
        "outputId": "3e433b06-ce48-4b75-846e-656c1e204ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of data:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0 IAPTus_Num Referral Date  Age_ReferralRequest_ReceivedDate  \\\n",
              "0           1      24475    08/09/2018                          5.099020   \n",
              "1           2    24476_1    10/04/2019                          4.358899   \n",
              "2           3    24476_2    28/03/2021                          4.582576   \n",
              "\n",
              "   EthnicDescGroupCode  EthnicCategoryGroupShortCode GenderIdentity  \\\n",
              "0                  1.0                           1.0              2   \n",
              "1                  1.0                           1.0              2   \n",
              "2                  1.0                           1.0              2   \n",
              "\n",
              "   SexualOrientationDesc                                            EndDesc  \\\n",
              "0                    NaN            Mutually agreed completion of treatment   \n",
              "1                    NaN            Mutually agreed completion of treatment   \n",
              "2                    NaN  Termition of treatment earlier than Care Profe...   \n",
              "\n",
              "  EndDescGroupShort  ...  Item216  Item217  Item218  Item219  Item220 Item221  \\\n",
              "0  Seen and treated  ...      NaN      NaN      NaN      NaN      NaN     NaN   \n",
              "1  Seen and treated  ...      NaN      NaN      NaN      NaN      NaN     NaN   \n",
              "2  Seen and treated  ...        0        0        1        1        1       0   \n",
              "\n",
              "  Item222 Item223 Item224 Item225  \n",
              "0     NaN     NaN     NaN     NaN  \n",
              "1     NaN     NaN     NaN     NaN  \n",
              "2       0       0       0       0  \n",
              "\n",
              "[3 rows x 279 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-472acef3-f6c1-408f-ad34-eb79ba43245f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>IAPTus_Num</th>\n",
              "      <th>Referral Date</th>\n",
              "      <th>Age_ReferralRequest_ReceivedDate</th>\n",
              "      <th>EthnicDescGroupCode</th>\n",
              "      <th>EthnicCategoryGroupShortCode</th>\n",
              "      <th>GenderIdentity</th>\n",
              "      <th>SexualOrientationDesc</th>\n",
              "      <th>EndDesc</th>\n",
              "      <th>EndDescGroupShort</th>\n",
              "      <th>...</th>\n",
              "      <th>Item216</th>\n",
              "      <th>Item217</th>\n",
              "      <th>Item218</th>\n",
              "      <th>Item219</th>\n",
              "      <th>Item220</th>\n",
              "      <th>Item221</th>\n",
              "      <th>Item222</th>\n",
              "      <th>Item223</th>\n",
              "      <th>Item224</th>\n",
              "      <th>Item225</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>24475</td>\n",
              "      <td>08/09/2018</td>\n",
              "      <td>5.099020</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Mutually agreed completion of treatment</td>\n",
              "      <td>Seen and treated</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>24476_1</td>\n",
              "      <td>10/04/2019</td>\n",
              "      <td>4.358899</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Mutually agreed completion of treatment</td>\n",
              "      <td>Seen and treated</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>24476_2</td>\n",
              "      <td>28/03/2021</td>\n",
              "      <td>4.582576</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Termition of treatment earlier than Care Profe...</td>\n",
              "      <td>Seen and treated</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 279 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-472acef3-f6c1-408f-ad34-eb79ba43245f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-472acef3-f6c1-408f-ad34-eb79ba43245f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-472acef3-f6c1-408f-ad34-eb79ba43245f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ac263463-263d-4295-9b85-58b7b7cca070\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ac263463-263d-4295-9b85-58b7b7cca070')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ac263463-263d-4295-9b85-58b7b7cca070 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "raw_df"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "print('Sample of data:\\n')\n",
        "raw_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiHAyIIfoz2k",
        "outputId": "5aee9ea2-0be9-408e-9382-e790f74b6463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data structure:\n",
            "\n",
            "Number of rows: 728\n",
            "Number of columns: 279\n",
            "\n",
            "Type: <class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "print('Data structure:')\n",
        "print('\\nNumber of rows:', raw_df.shape[0])\n",
        "print('Number of columns:', raw_df.shape[1])\n",
        "print('\\nType:', type(raw_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2CKBGU8XFPF",
        "outputId": "2438ce50-60f0-45e1-ccd7-e3833d1d3999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data types:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0                            int64\n",
              "IAPTus_Num                           object\n",
              "Referral Date                        object\n",
              "Age_ReferralRequest_ReceivedDate    float64\n",
              "EthnicDescGroupCode                 float64\n",
              "                                     ...   \n",
              "Item221                              object\n",
              "Item222                              object\n",
              "Item223                              object\n",
              "Item224                              object\n",
              "Item225                              object\n",
              "Length: 279, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# data types\n",
        "print('Data types:\\n')\n",
        "raw_df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QdplL0yE1eg"
      },
      "source": [
        "## 2.2 Non-Questionnaire Data Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pgft0gNJKhMo"
      },
      "outputs": [],
      "source": [
        "df = cleanf.rename_features(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt0LEu5KU4JD"
      },
      "outputs": [],
      "source": [
        "print('Column names and types:\\n')\n",
        "df.iloc[:, 0:20].dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpasYdTPqwrQ"
      },
      "source": [
        "Including the ID variables, make changes to the numerical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1eODVVcC8S2"
      },
      "outputs": [],
      "source": [
        "df = create_referral_count(df)\n",
        "df = clean_client_id(df)\n",
        "df = convert_features_to_datetime(df)\n",
        "df = convert_float_features_to_int(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbb4ewYzgt5B"
      },
      "source": [
        "Explanation:\n",
        "- Create a new column called ReferralCount counting the referrals (from clientID)\n",
        "- Converted ClientID from object to numeric\n",
        "- Converted date variables to datetime types\n",
        "- Converted relevant types from floats to integers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M4D26WMC2s1"
      },
      "source": [
        "Viewing the unique values the object variables contain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm8MJRaIK1Ld"
      },
      "outputs": [],
      "source": [
        "object_vars = df.iloc[:, 0:20].select_dtypes(include = ['object'])\n",
        "\n",
        "print('Unique values:\\n')\n",
        "for var in object_vars:\n",
        "    unique_vals = df[var].unique()\n",
        "    print(var)\n",
        "    print(unique_vals)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp3PGczkKYf7"
      },
      "source": [
        "Creating appropriate maps to convert from object data to numerical whilst also one hot encoding too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2ugUZ-gZsOi"
      },
      "outputs": [],
      "source": [
        "df = map_features(df) # gender, treated, reliablechangedesc, recovery, reliablerecovery\n",
        "df = one_hot_encode_features(df) # enddesc, enddescshort\n",
        "df = convert_to_int_features(df) # sexualorientation, daystoassessment, daystotreatment, carecontacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjGYeYdw7p4Y"
      },
      "outputs": [],
      "source": [
        "print('New data types:\\n')\n",
        "print('Column names and types:')\n",
        "df.iloc[:, 0:26].dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgETgftyIhMH"
      },
      "outputs": [],
      "source": [
        "# data sample\n",
        "df.iloc[:, 0:26].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blf8aoZCE7Jy"
      },
      "source": [
        "## 2.3 Questionnaire Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxmOX_cuvEQP"
      },
      "source": [
        "Inspect the questionnaire data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfjcyoFJi1Lr"
      },
      "outputs": [],
      "source": [
        "# questionnaire data\n",
        "df.iloc[:, 27:].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz6yqxnZkBjH"
      },
      "outputs": [],
      "source": [
        "object_vars2 = df.iloc[:, 27:].select_dtypes(include = ['object'])\n",
        "\n",
        "print('Unique values:\\n')\n",
        "lines_code = 5\n",
        "for var in object_vars2.iloc[:, :lines_code]:\n",
        "    unique_vals = df[var].unique()\n",
        "    print(var)\n",
        "    print(unique_vals)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgmmKje5ImXs"
      },
      "source": [
        "Should change '.' to NaN and then convert these to integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p57-cON6Hc9a"
      },
      "outputs": [],
      "source": [
        "# preprocess questionnaire data\n",
        "df = preprocess_ques_features(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaF4M1XMUBbq"
      },
      "source": [
        "# A3) Explanatory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7KzDKYWzbiE"
      },
      "source": [
        "## 3.1 Non-questionnaire Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXISzc4cq_sG"
      },
      "source": [
        "General plots of non-questionnaire data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IOmk6sPFAbN"
      },
      "outputs": [],
      "source": [
        "item1_loc = df.columns.get_loc('Item1')\n",
        "nonques_df = df.iloc[:, :item1_loc]\n",
        "#desc_df = nonques_df[nonques_df.columns[nonques_df.nunique() >= 10]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPCSptPSGkgy"
      },
      "outputs": [],
      "source": [
        "# plot non-questionnaire data\n",
        "plot_features(nonques_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB9zWLVNWOg7"
      },
      "outputs": [],
      "source": [
        "for col in nonques_df.columns: # convert datetime to numeric\n",
        "    if nonques_df[col].dtype == 'datetime64[ns]':\n",
        "        nonques_df[col] = pd.to_numeric(nonques_df[col], errors='coerce')\n",
        "\n",
        "dfs = [nonques_df]\n",
        "dfs_names = ['Non-questionnaire Data']\n",
        "summary_plot(dfs, dfs_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4uJzTCazeMz"
      },
      "source": [
        "## 3.2 Questionnaire Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-5YB5B3qoxn"
      },
      "source": [
        "General plots of questionnaire data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q1tAf9f05RX"
      },
      "outputs": [],
      "source": [
        "item_cols = [col for col in df.columns if 'Item' in col]\n",
        "item_df = df[item_cols]\n",
        "\n",
        "thresh_cols = [col for col in df.columns if 'Threshold' in col]\n",
        "thresh_df = df[thresh_cols]\n",
        "\n",
        "total_cols = [col for col in df.columns if 'Total' in col]\n",
        "total_df = df[total_cols]\n",
        "\n",
        "# check accounts for all questionnaire data\n",
        "if (nonques_df.shape[1] + item_df.shape[1] + thresh_df.shape[1] + total_df.shape[1]) == df.shape[1]:\n",
        "    print('All features are being considered')\n",
        "else:\n",
        "    print('All features are not being considered')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh2uam29Ol8y"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "plt.subplot(221)\n",
        "sns.lineplot(data=item_df, legend=False)\n",
        "plt.xlabel('Observations')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Item Variables')\n",
        "plt.xticks([])\n",
        "\n",
        "plt.subplot(222)\n",
        "sns.lineplot(data=thresh_df, legend=False)\n",
        "plt.xlabel('Observations')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Threshold Variables')\n",
        "plt.xticks([])\n",
        "\n",
        "plt.subplot(223)\n",
        "sns.lineplot(data=total_df, legend=False)\n",
        "plt.xlabel('Observations')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Total Variables')\n",
        "plt.xticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2WKjezW-Z-i"
      },
      "source": [
        "** **Add random features maybe?** **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oTQ68HSVbuo"
      },
      "outputs": [],
      "source": [
        "# questionnaire data summary\n",
        "dfs = [item_df, total_df, thresh_df]\n",
        "dfs_names = ['Item Variables', 'Total Variables', 'Threshold Variables']\n",
        "summary_plot(dfs, dfs_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97kMZR9sdiMm"
      },
      "source": [
        "Items:\n",
        "- Quite a lot\n",
        "\n",
        "Totals:\n",
        "- At least one negative value in Total14\n",
        "- Some high values in Total21 too\n",
        "\n",
        "Thresholds:\n",
        "- Everything potentially ok in these variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO83MmW4-RyT"
      },
      "source": [
        "## 3.3 Stratified by Target (or End of Treatment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m03txd40Dt_e"
      },
      "outputs": [],
      "source": [
        "EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "target_cols = ['Recovery', 'ReliableRecovery', 'ReliableChangeDesc'] + EndDesc_cols\n",
        "targets_df = df[target_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B35A0W2rCv6Z"
      },
      "outputs": [],
      "source": [
        "plot_features(targets_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW31o9VfDkg6"
      },
      "outputs": [],
      "source": [
        "cont_tb1 = pd.crosstab(df['Recovery'], df['ReliableRecovery'], dropna=False)\n",
        "cont_tb2 = pd.crosstab(df['Recovery'], df['ReliableChangeDesc'], dropna=False)\n",
        "cont_tb3 = pd.crosstab(df['ReliableRecovery'], df['ReliableChangeDesc'], dropna=False)\n",
        "\n",
        "cont_tbs = [cont_tb1, cont_tb2, cont_tb3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lptBGRr_LI9v"
      },
      "outputs": [],
      "source": [
        "def plot_cont_tables(cont_tbs):\n",
        "\n",
        "    def plot_cont_table(cont_tb, cmap):\n",
        "        sns.heatmap(cont_tb, annot=True, fmt='d', cmap=cmap)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    plt.subplot(221)\n",
        "    plot_cont_table(cont_tbs[0], 'Blues')\n",
        "    plt.subplot(222)\n",
        "    plot_cont_table(cont_tbs[1], 'Reds')\n",
        "    plt.subplot(223)\n",
        "    plot_cont_table(cont_tbs[2], 'Greens')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYHnF3wIKfJ5"
      },
      "outputs": [],
      "source": [
        "plot_cont_tables(cont_tbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZCwma3fNFX_"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Perform Chi-Square test\n",
        "chi2, p, dof, expected = chi2_contingency(cont_tb1)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Chi-Square Statistic: {chi2}\")\n",
        "print(f\"P-Value: {p}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(\"Expected Frequencies:\")\n",
        "print(expected)\n",
        "\n",
        "# Interpret the p-value\n",
        "alpha = 0.05\n",
        "if p < alpha:\n",
        "    print(\"There is a significant association between Recovery and ReliableRecovery.\")\n",
        "else:\n",
        "    print(\"There is no significant association between Recovery and ReliableRecovery.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0KGP3EH-hjn"
      },
      "outputs": [],
      "source": [
        "target = 'Recovery'\n",
        "\n",
        "success_df = df[df[target] == 1]\n",
        "notsuccess_df = df[df[target] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07_9z8wCQ0on"
      },
      "outputs": [],
      "source": [
        "target_corr1 = df.corr(method='kendall')[target]\n",
        "target_corr2 = df.corr(method='spearman')[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTsk5m-iPtjg"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(121)\n",
        "sns.barplot(data=target_corr1, color='orange')\n",
        "plt.axhline(y=0.3, color='r', linestyle='--')\n",
        "plt.axhline(y=-0.3, color='r', linestyle='--')\n",
        "plt.title(f'Correlation with {target}')\n",
        "plt.xticks([])\n",
        "\n",
        "plt.subplot(122)\n",
        "sns.barplot(data=target_corr2, color='skyblue')\n",
        "plt.axhline(y=0.3, color='r', linestyle='--')\n",
        "plt.axhline(y=-0.3, color='r', linestyle='--')\n",
        "plt.title(f'Correlation with {target}')\n",
        "plt.xticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDukjr2ERBsc"
      },
      "outputs": [],
      "source": [
        "df.columns[abs(target_corr1) >= 0.3], df.columns[abs(target_corr2) >= 0.3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7VGDeL_5VMn"
      },
      "source": [
        "# A4) Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFx52VqtDlOV"
      },
      "source": [
        "## 4.1 Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_5f8_IgDtpm"
      },
      "outputs": [],
      "source": [
        "dupe_rows = df.duplicated().sum()\n",
        "dupe_cols = np.transpose(df).duplicated().sum()\n",
        "print(f'Duplicate rows: {dupe_rows}')\n",
        "print(f'Duplicate columns: {dupe_cols}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m95ZdFhzLJRO"
      },
      "outputs": [],
      "source": [
        "print('Duplicate details:\\n')\n",
        "find_duplicates_features(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohoS-dpesQlJ"
      },
      "source": [
        "Removing these duplicates from the dataset to reduce noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KngQomt3QdRb"
      },
      "outputs": [],
      "source": [
        "df = drop_duplicate_features(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7BmhUef8gQK"
      },
      "source": [
        "## 4.2 Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0_ivynTVvMR"
      },
      "source": [
        "**Non-questionnaire Outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY_VGDkZIA4B"
      },
      "source": [
        "From EDA in section 3, these features seem to contain potential outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULKY26JD0lp1"
      },
      "outputs": [],
      "source": [
        "item1_start = df.columns.get_loc('Item1')\n",
        "nonques_df = df.iloc[:, :item1_start]\n",
        "observed_outliers = ['AgeAtReferralRequest', 'DaystoAssessment', 'DateOfQuestionnaire']\n",
        "plot_features(nonques_df[observed_outliers])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2JcxYUy4Fly"
      },
      "outputs": [],
      "source": [
        "nonques_df[observed_outliers].min(), nonques_df[observed_outliers].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B7s77YW7_0G"
      },
      "outputs": [],
      "source": [
        "age_outliers = df[df['AgeAtReferralRequest'] == 0]\n",
        "print(f'Number of age outliers: {age_outliers.shape[0]}')\n",
        "age_outliers.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzo_3I0M_0Oo"
      },
      "outputs": [],
      "source": [
        "wait_outliers = df['DaystoAssessment'].sort_values(ascending=False)\n",
        "wait_outliers.head(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYqVaF1F-tyU"
      },
      "outputs": [],
      "source": [
        "ques_date_outliers = df['DateOfQuestionnaire'].sort_values()\n",
        "ques_date_outliers.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fygt33H_AWfh"
      },
      "source": [
        "- Age of 0 seems to be outlier\n",
        "- Waiting times to be assessed seem appropriate\n",
        "- Questionnaire dates seems to be fair too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN7x9qQnA-zS"
      },
      "outputs": [],
      "source": [
        "# replace outliers with nan\n",
        "df.loc[df['AgeAtReferralRequest'] == 0, 'AgeAtReferralRequest'] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy3_zocxILg-"
      },
      "source": [
        "** **Use quartiles to find outliers** **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZzkxlCDfsdP"
      },
      "source": [
        "**Questionnaire Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvOXzRlVC0yl"
      },
      "outputs": [],
      "source": [
        "item_cols = [col for col in df.columns if 'Item' in col]\n",
        "item_df = df[item_cols]\n",
        "\n",
        "thresh_cols = [col for col in df.columns if 'Threshold' in col]\n",
        "thresh_df = df[thresh_cols]\n",
        "\n",
        "total_cols = [col for col in df.columns if 'Total' in col]\n",
        "total_df = df[total_cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv_I5RZ1fAT-"
      },
      "source": [
        "From the EDA, there exists negative total values. Removing these now will speed up the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLoe1D0nfLG1"
      },
      "outputs": [],
      "source": [
        "# replace negative totals with nan\n",
        "for col in total_df:\n",
        "    df.loc[df[col] < 0, col] = np.nan # replace in df\n",
        "    total_df.loc[total_df[col] < 0, col] = np.nan # replace in total_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLspzhrXyRzg"
      },
      "source": [
        "Viewing potential features containing outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eazn0hRyMw2"
      },
      "outputs": [],
      "source": [
        "# detect whether cols are just 0,1,na\n",
        "thresh_pot_cols = column_contents(thresh_df)\n",
        "item_pot_cols = column_contents(item_df)\n",
        "total_pot_cols = column_contents(total_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtDcSYSn5TL4"
      },
      "source": [
        "Initial Beliefs\n",
        "- Treshold varibles contain no outliers as there are only 0 and 1s\n",
        "- Constant columns and quasi constant columns will be explored and removed later\n",
        "- Potential outliers in Items and Totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fw-Gl4oD35h"
      },
      "outputs": [],
      "source": [
        "# potential columns containing outliers (shortening the search, only these can contain outliers)\n",
        "total_pot_df = total_df[total_pot_cols]\n",
        "item_pot_df = item_df[item_pot_cols]\n",
        "\n",
        "# find outliers in Total variables\n",
        "print('Outlier counts:\\n')\n",
        "total_outlier_df = find_outlier_cols(total_pot_df)\n",
        "print('\\n')\n",
        "item_outlier_df = find_outlier_cols(item_pot_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFL5-4Vp_JWW"
      },
      "outputs": [],
      "source": [
        "plot_outlier_cols(total_outlier_df, 'Total Variables')\n",
        "plot_outlier_cols(item_outlier_df, 'Item Variables', legend=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ricssM6IQ19b"
      },
      "source": [
        "Clearly the two massive distinct spikes are mistakes, removing these initally and takin another look before removing the other spike in the second plot seems resonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JskWc4OWD0RF"
      },
      "outputs": [],
      "source": [
        "# replace outliers with nan\n",
        "for col in total_outlier_df:\n",
        "    df.loc[df[col] > 200, col] = np.nan\n",
        "    total_outlier_df.loc[total_outlier_df[col] > 200, col] = np.nan\n",
        "\n",
        "for col in item_outlier_df:\n",
        "    df.loc[df[col] > 50, col] = np.nan\n",
        "    item_outlier_df.loc[item_outlier_df[col] > 50, col] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koo7SaJ3EkKf"
      },
      "outputs": [],
      "source": [
        "plot_outlier_cols(total_outlier_df, 'Total Variables', legend=False)\n",
        "plot_outlier_cols(item_outlier_df, 'Item Variables', legend=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3dR0WA-at5K"
      },
      "outputs": [],
      "source": [
        "total_outlier_df[total_outlier_df > 36].dropna(axis=1, how='all').dropna(axis=0, how='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxwimFhhJat8"
      },
      "source": [
        "Totals:\n",
        "- Since there are so many instances of 36 being the highest total in Total21 it seems unlikely one achived higher, so removed.\n",
        "- No large unusual points left.\n",
        "- Everything else seems to be ok, since there are not many observations it would be hard to determine if one is an outlier if the difference from the rest is so small.\n",
        "\n",
        "Items:\n",
        "- This spike must be a mistake for Item70, it seems unlikely there was an option of 10 when the rest are 0 and 1.\n",
        "- Similarly with the other reasoning, there are too little observations to accurately determine if any others are mistsakes since the difference in scoring is so small now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD6bx6Z8gADD"
      },
      "outputs": [],
      "source": [
        "def plot_outlier_cols2(outlier_df, title, color):\n",
        "\n",
        "    plot_cols = 4\n",
        "    plot_rows = len(outlier_df.columns)//4 + 1\n",
        "    plt.figure(figsize = (plot_cols*3, plot_rows*3))\n",
        "\n",
        "    for i, col in enumerate(outlier_df.columns, start = 1):\n",
        "        plt.subplot(plot_rows, 4, i)\n",
        "        outlier_df[col].plot(color=color)\n",
        "        plt.title(col)\n",
        "        plt.xlabel('Index')\n",
        "        plt.ylabel('Total Score')\n",
        "\n",
        "    plt.suptitle(title, y = 1, fontsize = 24)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLxvWM6ihNyj"
      },
      "outputs": [],
      "source": [
        "plot_outlier_cols2(total_outlier_df, 'Total Variables', 'limegreen')\n",
        "plot_outlier_cols2(item_outlier_df, 'Item Variables', 'gold')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3Zk2WIhHeKq"
      },
      "outputs": [],
      "source": [
        "df.loc[df['Total14'] > 37, 'Total14'] = np.nan\n",
        "df.loc[df['Item70'] > 3, 'Item70'] = np.nan\n",
        "df.loc[df['Item132'] > 5, 'Item132'] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpRmffavsbTg"
      },
      "source": [
        "** **Alter item_df and total_df if analysis goes further** **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcoDd1cI5Tuy"
      },
      "source": [
        "## 4.3 Constant and Quasi Constant Features (+99.5%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP7Fgjkge2EB"
      },
      "source": [
        "Removing useless constant columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwu8HVMw5dSY"
      },
      "outputs": [],
      "source": [
        "df = drop_const_features(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk33O-izkWIO"
      },
      "source": [
        "Exploring the data for any quasi constant features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa9lDTHCooiR"
      },
      "outputs": [],
      "source": [
        "ordinal_cols = df.select_dtypes(include = ['Int64', 'int64']).columns\n",
        "ordinal_df = df[ordinal_cols]\n",
        "\n",
        "cont_cols = df.drop(columns = ordinal_cols).columns.tolist()\n",
        "cont_df = df[cont_cols]\n",
        "\n",
        "non_datetime_cols = df.select_dtypes(exclude = ['datetime64']).columns\n",
        "non_datetime_df = df[non_datetime_cols]\n",
        "\n",
        "datetime_cols = df.select_dtypes(include = ['datetime64']).columns\n",
        "datetime_df = df[datetime_cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsxRkBtSygQ7"
      },
      "source": [
        "Viewing the variance across the continuous variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrNdQTNlNNP9"
      },
      "outputs": [],
      "source": [
        "# coefficient of variance\n",
        "cov = df[non_datetime_cols].var() / df[non_datetime_cols].mean()\n",
        "\n",
        "plt.figure(figsize = (10, 4))\n",
        "plt.bar(cov.index, cov.values, color = 'dimgrey', edgecolor = 'white')\n",
        "plt.xlabel('Variables')\n",
        "plt.ylabel('Coefficient of Variation')\n",
        "plt.title('Coefficient of Variation for each Variable')\n",
        "plt.xticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxdrv5476B7H"
      },
      "outputs": [],
      "source": [
        "print('Features with low CoV:\\n')\n",
        "cov.sort_values().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lylkKwPtQmL"
      },
      "outputs": [],
      "source": [
        "quasi_percentages = quasi_percentage(df)\n",
        "print('Quasi percentages:\\n')\n",
        "print(quasi_percentages.sort_values(ascending=False).head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9BWiM_57Y-9"
      },
      "outputs": [],
      "source": [
        "#df[cov.sort_values().head().index].plot()\n",
        "#df[quasi_percentages.sort_values(ascending=False).head().index].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs3e6JjzUS2o"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(quasi_percentages.values, bins = 100, color = 'grey', edgecolor = 'white')\n",
        "plt.xlabel('Percentage of Mode Occurrences')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Percentage of Mode Occurrences for Ordinal Variables')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-ZBTz_xa87T"
      },
      "outputs": [],
      "source": [
        "df = drop_quasi_features(df, threshold=0.995)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FsneM9OkOWL"
      },
      "source": [
        "** **This is deleting a column that is the product of a categorical variable. Delete the whole rows too** **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiKr7I10DDMQ"
      },
      "source": [
        "## 4.4 Correlation (+99%, set to load)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y_hq8nOB7tI"
      },
      "source": [
        "** **Revisit, all variables can be used in Kendall and Spearman** **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwQpIuFtIiQy"
      },
      "outputs": [],
      "source": [
        "corr_matricies = generate_corr_matricies(df, load_matrices=True)\n",
        "kendall_corr = corr_matricies[0]\n",
        "spearman_corr = corr_matricies[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHtXdjdZShq0"
      },
      "outputs": [],
      "source": [
        "corr_matricies = [kendall_corr, spearman_corr]\n",
        "plot_correlation_matrices(corr_matricies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjszkIGpyLs-"
      },
      "outputs": [],
      "source": [
        "thresholds = [0.7, 0.8, 0.9, 0.95, 1.0]\n",
        "correlated_features_above_treshold(corr_matricies, thresholds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvZqS514ZKwn"
      },
      "outputs": [],
      "source": [
        "df = remove_corr_features(df, load_matrices=True, threshold=0.99)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-o8JXYW8dbh"
      },
      "source": [
        "## 4.5 Missing Values within Features (+99%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qp1iHbZpKfM"
      },
      "outputs": [],
      "source": [
        "print('Percentage missing data:')\n",
        "print(df.isna().mean().mean() * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpILEPuWD8Dl"
      },
      "outputs": [],
      "source": [
        "msno.matrix(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GtUSEIAtN_8"
      },
      "outputs": [],
      "source": [
        "df.isna().mean(axis=0).hist(color='teal', edgecolor='white', bins=df.shape[1]//10)\n",
        "plt.axvline(x=0.5, color='firebrick', linestyle='--')  # Line at 50%\n",
        "plt.title('Feature Missingness')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Missingness Percentage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2SpLOR65lB1"
      },
      "outputs": [],
      "source": [
        "df.isna().mean(axis=1).hist(color='salmon', edgecolor='white', bins=df.shape[0]//10)\n",
        "plt.axvline(x=0.5, color='firebrick', linestyle='--')  # Line at 50%\n",
        "plt.title('Observation Missingness')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Missingness Percentage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lbLPKlfDOcEy"
      },
      "outputs": [],
      "source": [
        "# for col1 in df.columns:\n",
        "#     for col2 in df.columns:\n",
        "#         # Only perform the chi-square test for pairs of categorical variables\n",
        "#         if df[col1].dtype == 'int64'or'Int64' and df[col2].dtype == 'int64'or'Int64':\n",
        "#             # Exclude performing the test for the same variable\n",
        "#             if col1 != col2:\n",
        "#                 # Create the contingency table\n",
        "#                 ct = pd.crosstab(df[col1], df[col2])\n",
        "#                 # Perform the chi-square test\n",
        "#                 chi2, p, dof, exp = chi2_contingency(ct)\n",
        "#                 if p <= 0.001:\n",
        "#                 # Print the results\n",
        "#                     print(f\"Chi-square test for variables {col1} and {col2}:\")\n",
        "#                     #print(f\"Chi-square statistic: {chi2}\")\n",
        "#                     print(f\"P-value: {p}\")\n",
        "#                     #print(f\"Degrees of freedom: {dof}\")\n",
        "#                     #print(\"Expected frequencies:\")\n",
        "#                     #print(exp)\n",
        "#                     print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr-q3KJD49Hk"
      },
      "source": [
        "Remove features with +50% missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjOBWvGeSxpK"
      },
      "outputs": [],
      "source": [
        "df = drop_missing_values_col(df, threshold=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-2npawoQoAB"
      },
      "source": [
        "## 4.6 Missing Data within Observations (+99%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBbieF2m6QIN"
      },
      "outputs": [],
      "source": [
        "df.isna().sum(axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UUpthwXQ7Uo"
      },
      "outputs": [],
      "source": [
        "missing_value_percentage = df.isna().sum(axis = 1).sort_values() / df.shape[0]\n",
        "\n",
        "plt.figure(figsize = (12, 4))\n",
        "\n",
        "plt.subplot(121)\n",
        "missing_value_percentage.plot(kind = 'bar', color='white', edgecolor='dimgrey')\n",
        "plt.title('Proportion of Missing Values by Observation')\n",
        "plt.xlabel('Observations')\n",
        "plt.ylabel('Percentage of Missing Values')\n",
        "plt.xticks([])\n",
        "plt.axhline(y = 0.5, color='red', linestyle='--')  # line at 50%\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.hist(missing_value_percentage, bins = 100, color='darkgrey', edgecolor='white')\n",
        "plt.title('Proportion of Missing Values by Observation')\n",
        "plt.xlabel('Percentage of Missing Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axvline(x=0.5, color='red', linestyle='--')  # line at 50%\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeksPRnPa-3f"
      },
      "source": [
        "The aim is to retain as much data as possible, especially since there is not a lot as it is. Luckily non seem to exceed 70%, very few above 50%. It would be best not to remove anything at this point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzIRxGF8Xa-e"
      },
      "source": [
        "## 4.7 Scaling (none)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VlTZCqbCnHT"
      },
      "source": [
        "For classification tasks its recommended to standardise data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rol1qioTXeB0"
      },
      "outputs": [],
      "source": [
        "#df = standardise(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgpQl4PJ4s31"
      },
      "source": [
        "## 4.8 Imputation (knn, k = 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4BF-bhm44uch"
      },
      "outputs": [],
      "source": [
        "df = impute_data(df, method='knn', n_neighbours=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXn3OpVAI6CX"
      },
      "source": [
        "** **Make sure not rounding to non-existing number** **\n",
        "- could check that: before_df.unique() == after_df.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg_alK3K1nFc"
      },
      "source": [
        "## 4.9 Feature Engineering (none)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7xwM6Tffr2H"
      },
      "outputs": [],
      "source": [
        "temp_df = df.copy()\n",
        "#df = temp_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIvtxVG75KG3"
      },
      "outputs": [],
      "source": [
        "df = feature_engineering(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VoG0baFiOTh"
      },
      "outputs": [],
      "source": [
        "df.iloc[:,:-5].columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPpggAMYBVtO"
      },
      "source": [
        "## 4.10 Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgGte6oLRRJE"
      },
      "source": [
        "### Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdCkQ9uQWt_d"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.feature_selection import chi2 # neg values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDI-LCivXX5D"
      },
      "outputs": [],
      "source": [
        "features = df.dropna()\n",
        "target = 'Recovery'\n",
        "EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "explanatory = features.drop(['Recovery', 'ReliableRecovery', 'ReliableChangeDesc'] + EndDesc_cols, axis = 1)\n",
        "target = features[target]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llXy6iooas-T"
      },
      "source": [
        "### Univariate feature selection (used, k = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1mo28sbXkso"
      },
      "outputs": [],
      "source": [
        "# fishers score\n",
        "kbest_fisher = SelectKBest(score_func=f_classif, k=20)\n",
        "selected_fisher = kbest_fisher.fit_transform(explanatory, target)\n",
        "ind_fisher = kbest_fisher.get_support(indices=True)\n",
        "names_fisher = explanatory.columns[ind_fisher]\n",
        "print(\"Selected Features:\")\n",
        "print(names_fisher)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNqdvKJpK40b"
      },
      "outputs": [],
      "source": [
        "# fishers score\n",
        "f_values, p_values = f_classif(explanatory, target) # fisher score\n",
        "feat_importances = pd.Series(f_values, index=explanatory.columns)\n",
        "feat_importances = feat_importances[feat_importances > 10] # threshold\n",
        "feat_importances.plot(kind='barh', color='darkblue')\n",
        "plt.title(\"Univariate Feature Selection\\nFisher's Score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpscu28ObM6V"
      },
      "outputs": [],
      "source": [
        "# mutual information gain\n",
        "def mutual_info_classif_wseed(X, y):\n",
        "    return mutual_info_classif(X, y, random_state=11)\n",
        "kbest_gain = SelectKBest(score_func=mutual_info_classif_wseed, k=20)\n",
        "selected_gain = kbest_gain.fit_transform(explanatory, target)\n",
        "ind_gain = kbest_gain.get_support(indices=True)\n",
        "names_gain = explanatory.columns[ind_gain]\n",
        "print(\"Selected Features:\")\n",
        "print(names_gain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4IAtc1gF_v7"
      },
      "outputs": [],
      "source": [
        "# mutual information gain\n",
        "importances = mutual_info_classif(explanatory, target, random_state=11) # mutual information gain\n",
        "feat_importances = pd.Series(importances, index=explanatory.columns)\n",
        "feat_importances = feat_importances[feat_importances > 0.05] # threshold\n",
        "feat_importances.plot(kind='barh', color='darkgreen')\n",
        "plt.title(\"Univariate Feature Selection\\nMutual Information Gain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8n2TkivVAm0"
      },
      "outputs": [],
      "source": [
        "# Combine the two lists and get unique values using a set\n",
        "important_combined = set(names_fisher).union(set(names_gain))\n",
        "\n",
        "# Convert the set back to a list (optional) and print the unique feature names\n",
        "important_features = list(important_combined)\n",
        "print(\"Unique selected features:\")\n",
        "print(important_features)\n",
        "print(f'Number of unique features: {len(important_features)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTTp1nG0q8sJ"
      },
      "source": [
        "### Recursive feature elimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ynn-DdEjs-cm"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "min_features_to_select = 1  # Minimum number of features to consider\n",
        "clf = XGBClassifier(seed=2001)\n",
        "cv = StratifiedKFold(5)\n",
        "\n",
        "rfecv = RFECV(\n",
        "    estimator=clf,\n",
        "    step=1,\n",
        "    cv=cv,\n",
        "    scoring=\"accuracy\",\n",
        "    min_features_to_select=min_features_to_select,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "rfecv.fit(explanatory, target)\n",
        "\n",
        "print(f\"Optimal number of features: {rfecv.n_features_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o-c24SPtS7X"
      },
      "outputs": [],
      "source": [
        "n_scores = len(rfecv.cv_results_[\"mean_test_score\"])\n",
        "plt.figure()\n",
        "plt.xlabel(\"Number of features selected\")\n",
        "plt.ylabel(\"Mean test accuracy\")\n",
        "plt.errorbar(\n",
        "    range(min_features_to_select, n_scores + min_features_to_select),\n",
        "    rfecv.cv_results_[\"mean_test_score\"],\n",
        "    yerr=rfecv.cv_results_[\"std_test_score\"],\n",
        ")\n",
        "plt.title(\"Recursive Feature Elimination\\nwith Correlated Features\")\n",
        "plt.xlim(0, 50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzAhsM5CQS5b"
      },
      "source": [
        "### Feature selection using SelectFromModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrVV5iyuNmaq"
      },
      "source": [
        "Model-based and sequential feature selection with Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm6C0KNRNpj7"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "# Fit RidgeCV model\n",
        "ridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(explanatory, target)\n",
        "\n",
        "# Calculate the importance of each feature\n",
        "importance = np.abs(ridge.coef_)\n",
        "feature_names = np.array(explanatory.columns)\n",
        "\n",
        "# Filter features with importance greater than 0.0001\n",
        "threshold = 0.00005\n",
        "mask = importance > threshold\n",
        "filtered_importance = importance[mask]\n",
        "filtered_feature_names = feature_names[mask]\n",
        "\n",
        "# Plot the filtered feature importances\n",
        "plt.bar(height=filtered_importance, x=filtered_feature_names)\n",
        "plt.title(\"Feature importances via coefficients (filtered)\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLZ7a5IQnXU"
      },
      "source": [
        "Tree-based feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA8ZrsHpQrBD"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "clf = ExtraTreesClassifier(n_estimators=50)\n",
        "clf = clf.fit(explanatory, target)\n",
        "clf.feature_importances_\n",
        "model = SelectFromModel(clf, prefit=True)\n",
        "X_new = model.transform(explanatory)\n",
        "X_new.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE3ELwC-RO68"
      },
      "source": [
        "### Sequential Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CddhNrzsVriH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0hZzeB_rC_e"
      },
      "source": [
        "### Correlation between target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsOZQCH-BTHY"
      },
      "outputs": [],
      "source": [
        "kendall_corr = df.corr(method='kendall')\n",
        "spearman_corr = df.corr(method='spearman')\n",
        "\n",
        "target = 'Recovery'\n",
        "target_kendall_corr = (kendall_corr[target].abs() > 0.5).dropna()\n",
        "target_spearman_corr = (spearman_corr[target].abs() > 0.5).dropna()\n",
        "\n",
        "correlations = pd.DataFrame({'Kendall': target_kendall_corr, 'Spearman': target_spearman_corr})\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(correlations, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(f'Correlation of Features with {target}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYnsL7foEIwq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "correlations.plot(kind='bar', cmap='coolwarm')\n",
        "plt.title(f'Correlation of Features with {target} (Absolute Values > 0.5)')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Correlation Coefficient')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Correlation Method')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUaRK4DQ9ALv"
      },
      "source": [
        "# A5) Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0iQFWjR-gRE"
      },
      "source": [
        "## 5.1 XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKMvVPP_5ou2"
      },
      "source": [
        "### Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFAcYp7AOQa0"
      },
      "outputs": [],
      "source": [
        "param_grid1 = dict(\n",
        "    #FS__threshold = [\"mean\", \"min\"],\n",
        "    FS__n_features_to_select = [10, 20],\n",
        "    classifier__n_estimators = [200, 500],\n",
        "    classifier__max_depth = [3, 5, 10],\n",
        "    classifier__learning_rate = [0.001, 0.01, 0.1],\n",
        "    classifier__subsample = [0.5, 1.0],\n",
        "    classifier__colsample_bytree = [0.5, 1.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly7uV2sXeeN-"
      },
      "outputs": [],
      "source": [
        "# model 1\n",
        "xgb_spec_knn7_df = {\n",
        "    'df': knn7_df,\n",
        "    'target': 'ReliableRecovery',\n",
        "    #'selector': 'SelectFromModel',\n",
        "    'selector': 'RFE',\n",
        "    'param_grid': param_grid1,\n",
        "    'k': 5}\n",
        "\n",
        "xgb_knn7_df = XGBoost_ModelSelection(**xgb_spec_knn7_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fszZ7EjjOEa"
      },
      "outputs": [],
      "source": [
        "ModelSelection_Summary(xgb_knn15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS1d7Lj-TIqv"
      },
      "outputs": [],
      "source": [
        "xgb_spec_knn10= {\n",
        "    'df': knn10_df,\n",
        "    'target': 'ReliableRecovery',\n",
        "    'selector': 'SelectFromModel',\n",
        "    'param_grid': param_grid1,\n",
        "    'k': 5}\n",
        "\n",
        "xgb_knn10 = XGBoost_ModelSelection(**xgb_spec_knn10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pquTH0gRTKkY"
      },
      "outputs": [],
      "source": [
        "ModelSelection_Summary(xgb_knn10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9LVXloeTTBE"
      },
      "outputs": [],
      "source": [
        "xgb_spec_knn7= {\n",
        "    'df': knn7_df,\n",
        "    'target': 'ReliableRecovery',\n",
        "    'selector': 'SelectFromModel',\n",
        "    'param_grid': param_grid1,\n",
        "    'k': 5}\n",
        "\n",
        "xgb_knn7 = XGBoost_ModelSelection(**xgb_spec_knn7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVeOfX7jTRTz"
      },
      "outputs": [],
      "source": [
        "ModelSelection_Summary(xgb_knn7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ltCoR-45q6B"
      },
      "source": [
        "### Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn_XHemBBvqZ"
      },
      "outputs": [],
      "source": [
        "# hyperparameters for XGBoost and feature selector\n",
        "param_grid2 = dict(\n",
        "    FS__threshold = [\"mean\", \"median\"],\n",
        "    classifier__n_estimators = [400, 500, 600],\n",
        "    classifier__max_depth = [5, 7, 10],\n",
        "    classifier__learning_rate = [0.05, 0.1, 0.5],\n",
        "    classifier__min_child_weight = [0, 1],\n",
        "    classifier__subsample = [0.8, 1],\n",
        "    classifier__colsample_bytree = [0.8, 1],\n",
        "    classifier__gamma = [0, 0.01],\n",
        "    classifier__lambda = [0, 0.01],\n",
        "    classifier__alpha = [0, 0.01])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn6qz3X_AkRS"
      },
      "source": [
        "### Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSXs_kaUAoFg"
      },
      "outputs": [],
      "source": [
        "# Initialize XGBoost classifier with best hyperparameters\n",
        "best_params = {\n",
        "    'n_estimators': 200,\n",
        "    'max_depth': 5,\n",
        "    'learning_rate': 0.1,\n",
        "    'min_child_weight': None,\n",
        "    'subsample': 1.0,\n",
        "    'colsample_bytree': 0.5,\n",
        "    'gamma': None,\n",
        "    'reg_lambda': None,\n",
        "    'reg_alpha': None\n",
        "}\n",
        "\n",
        "xgb_classifier = XGBClassifier(**best_params)\n",
        "\n",
        "# Feature selector\n",
        "feature_selector = SelectFromModel(xgb_classifier)\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('FS', feature_selector),\n",
        "    ('classifier', xgb_classifier)])\n",
        "\n",
        "# Train the model on the entire dataset\n",
        "pipeline.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-UUReUl-PF6"
      },
      "source": [
        "# B) Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIt67DIqdUNI"
      },
      "source": [
        "## Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5nyvLAZsP4o"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJbA0iPb-Yqd"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# eda\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# processing\n",
        "from sklearn.preprocessing import MinMaxScaler # normalisation\n",
        "from sklearn.preprocessing import Normalizer # works on rows not features\n",
        "from sklearn.preprocessing import StandardScaler # standardisation\n",
        "\n",
        "from statsmodels.imputation.mice import MICEData\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer # MICE (returning single rather than multiple)\n",
        "\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.feature_selection import chi2 # neg values\n",
        "\n",
        "# modelling\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4f_FnudsSmA"
      },
      "source": [
        "### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFtPEz0v-wnZ"
      },
      "outputs": [],
      "source": [
        "# create different processed datasets\n",
        "data, raw_df = load_data()\n",
        "data = Clean_Data(data)\n",
        "\n",
        "prep1_df = Prepare_Data(\n",
        "    df=data,\n",
        "    quasi_thresh=0.995,\n",
        "    corr_thresh=0.95,\n",
        "    load_matrices=True,\n",
        "    col_thresh=0.5,\n",
        "    row_thresh=0.5,\n",
        "    imputation_method='iterative',\n",
        "    #n_neighbours=3,\n",
        "    max_iter=10,\n",
        "    k_features=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU2DB7h8dXeX"
      },
      "source": [
        "## Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI7mhzsU_ASs"
      },
      "outputs": [],
      "source": [
        "param_grid1 = dict(\n",
        "    FS__threshold = [\"mean\", \"median\"],\n",
        "    #FS__n_features_to_select = [10, 15],\n",
        "    classifier__n_estimators = [200, 400, 500],\n",
        "    classifier__max_depth = [3, 5],\n",
        "    classifier__learning_rate = [0.01, 0.1, 0.5],\n",
        "    classifier__subsample = [0.5, 0.8],\n",
        "    classifier__colsample_bytree = [0.5, 0.8])\n",
        "\n",
        "# model 1\n",
        "xgb_spec1 = {\n",
        "    'df': prep1_df,\n",
        "    'target': 'ReliableRecovery',\n",
        "    'selector': 'SelectFromModel',\n",
        "    #'selector': 'RFE',\n",
        "    'param_grid': param_grid1,\n",
        "    'k': 10}\n",
        "\n",
        "xgb_model1 = XGBoost_ModelSelection(**xgb_spec1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrn5Aa_lUhd-"
      },
      "outputs": [],
      "source": [
        "ModelSelection_Summary(xgb_model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8Tzf4pZiyJv"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Data/Dissertation_Data/xgb_model1.pkl'\n",
        "\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(xgb_model1, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSI5QUpac_nT"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW-0L2rMswoO"
      },
      "outputs": [],
      "source": [
        "def XGBoost_Train(df, target, selector, param_grid, k=5):\n",
        "\n",
        "    # dataset\n",
        "    sample = df.dropna(subset = [target])\n",
        "    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col] #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\n",
        "    X = sample.drop(['ReliableChangeDesc', 'ReliableRecovery', 'Recovery'] + EndDesc_cols, axis = 1)\n",
        "    y = sample[target]\n",
        "    cols = X.columns\n",
        "\n",
        "    # machine learning algorithm\n",
        "    classifier = XGBClassifier()\n",
        "\n",
        "    # feature selection method\n",
        "    if selector == 'SelectFromModel':\n",
        "        selector = SelectFromModel(classifier)\n",
        "    elif selector == 'RFE':\n",
        "        selector = RFE(classifier)\n",
        "    else:\n",
        "        # fill this #\n",
        "        raise ValueError('Unsupported selector type')\n",
        "\n",
        "    # pipeline\n",
        "    pipeline = Pipeline([(\"FS\", selector), (\"classifier\", classifier)])\n",
        "\n",
        "    # initialise lists\n",
        "    scores, preds, actuals = [], [], []\n",
        "\n",
        "    # cross validation and hyperparameter tuning\n",
        "    outer_cv = StratifiedKFold(n_splits = k, shuffle = True)\n",
        "    for train_index, test_index in outer_cv.split(X, y):\n",
        "\n",
        "        # outer CV train and test sets\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        # fit data\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # evaluating optimised model on test\n",
        "        predictions = pipeline.predict(X_test)\n",
        "        score = metrics.accuracy_score(y_test, predictions)\n",
        "        scores.append(score)\n",
        "        #print('Accuracy: {}'.format(score)) # test sets\n",
        "\n",
        "        probs = pipeline.predict_proba(X_test)[:, 1]\n",
        "        preds.extend(probs)\n",
        "        actuals.extend(y_test)\n",
        "\n",
        "    print('Overall accuracy: {}'.format(np.mean(scores))) # test sets\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    return scores, preds, actuals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3fpbORikbIm"
      },
      "outputs": [],
      "source": [
        "def xgb_grid(params_str):\n",
        "    params = params_str.split(', ')\n",
        "    param_grid = {\n",
        "        'FS__threshold': params[0],\n",
        "        'classifier__max_depth': int(params[1]) if params[1] != 'None' else None,\n",
        "        'classifier__n_estimators': int(params[2]) if params[2] != 'None' else None,\n",
        "        'classifier__learning_rate': float(params[3]) if params[3] != 'None' else None,\n",
        "        'classifier__min_child_weight': float(params[4]) if params[4] != 'None' else None,\n",
        "        'classifier__subsample': float(params[5]) if params[5] != 'None' else None,\n",
        "        'classifier__colsample_bytree': float(params[6]) if params[6] != 'None' else None,\n",
        "        'classifier__gamma': float(params[7]) if params[7] != 'None' else None,\n",
        "        'classifier__reg_lambda': float(params[8]) if params[8] != 'None' else None,\n",
        "        'classifier__reg_alpha': float(params[9]) if params[9] != 'None' else None\n",
        "    }\n",
        "    return param_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJLCnUCIkcmc"
      },
      "outputs": [],
      "source": [
        "param_grid1 = xgb_grid('\"mean\", 3, 200, 0.1, None, 0.5, 0.8, None, None, None')\n",
        "param_grid2 = xgb_grid('\"mean\", 5, 200, 0.5, None, 0.5, 0.5, None, None, None')\n",
        "param_grid3 = xgb_grid('\"median\", 5, 500, 0.01, None, 0.5, 0.5, None, None, None')\n",
        "param_grid4 = xgb_grid('\"median\", 5, 500, 0.01, None, 0.8, 0.5, None, None, None')\n",
        "param_grid5 = xgb_grid('\"median\", 5, 200, 0.5, None, 0.8, 0.5, None, None, None')\n",
        "param_grid6 = xgb_grid('\"mean\", 5, 400, 0.01, None, 0.8, 0.8, None, None, None')\n",
        "param_grid7 = xgb_grid('\"mean\", 5, 400, 0.1, None, 0.8, 0.8, None, None, None')\n",
        "param_grid8 = xgb_grid('\"median\", 5, 400, 0.01, None, 0.8, 0.8, None, None, None')\n",
        "param_grid9 = xgb_grid('\"median\", 5, 200, 0.5, None, 0.8, 0.8, None, None, None')\n",
        "param_grid10 = xgb_grid('\"mean\", 5, 200, 0.1, None, 0.8, 0.8, None, None, None')\n",
        "\n",
        "param_grids = [param_grid1, param_grid2, param_grid3, param_grid4, param_grid5,\n",
        "               param_grid6, param_grid7, param_grid8, param_grid9, param_grid10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEbfQq3kpKQq"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    xgb_spec = {\n",
        "        'df': prep1_df,\n",
        "        'target': 'ReliableRecovery',\n",
        "        'selector': 'SelectFromModel',\n",
        "        #'selector': 'RFE',\n",
        "        'param_grid': param_grids[i],\n",
        "        'k': 10}\n",
        "\n",
        "    XGBoost_Train(**xgb_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPbLHHf_svBB"
      },
      "source": [
        "## Modelling 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a1K8BqVrIbG"
      },
      "source": [
        "# C) Large Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfeCuI3odTpP"
      },
      "source": [
        "## Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffQj1QIRs8y6"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KskRGSZds45p"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# eda\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# processing\n",
        "from sklearn.preprocessing import MinMaxScaler # normalisation\n",
        "from sklearn.preprocessing import Normalizer # works on rows not features\n",
        "from sklearn.preprocessing import StandardScaler # standardisation\n",
        "\n",
        "from statsmodels.imputation.mice import MICEData\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer # MICE (returning single rather than multiple)\n",
        "\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.feature_selection import chi2 # neg values\n",
        "\n",
        "# modelling\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4qStBJVs6V5"
      },
      "source": [
        "### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S1DgvhyrWX3"
      },
      "outputs": [],
      "source": [
        "# create different processed datasets\n",
        "data, raw_df = load_data()\n",
        "data = Clean_Data(data)\n",
        "\n",
        "prep1_df = Prepare_Data(\n",
        "    df=data,\n",
        "    quasi_thresh=0.999,\n",
        "    corr_thresh=0.999,\n",
        "    load_matrices=True,\n",
        "    col_thresh=0.9,\n",
        "    row_thresh=0.9,\n",
        "    imputation_method='iterative',\n",
        "    #n_neighbours=3,\n",
        "    max_iter=10,\n",
        "    k_features=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13-N5AhaCIFc"
      },
      "source": [
        "## Modelling 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-2DgshLGCK19"
      },
      "outputs": [],
      "source": [
        "!pip install datasets;\n",
        "!pip install evaluate;\n",
        "!pip install -U accelerate;\n",
        "!pip install -U transformers;\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers import GPT2ForSequenceClassification\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxjArkTOCRKX"
      },
      "outputs": [],
      "source": [
        "def BERT_Train(df, target='Recovery', k=5):\n",
        "\n",
        "    # dataset\n",
        "    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "    explanatory_df = df.drop(['ReliableChangeDesc', 'ReliableRecovery', 'Recovery'] + EndDesc_cols, axis = 1)\n",
        "    text = explanatory_df.apply(lambda row: row.to_json(), axis=1)\n",
        "    text_df = pd.DataFrame({\n",
        "        'text': text,\n",
        "        'label': df[target]})\n",
        "    text_df = text_df.dropna()\n",
        "    dataset = Dataset.from_pandas(text_df)\n",
        "\n",
        "    # tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # model\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "    # evaluation\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"test_trainer\",\n",
        "        #evaluation_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=1,  # Reduce batch size here\n",
        "        per_device_eval_batch_size=1,   # Optionally, reduce for evaluation as well\n",
        "        gradient_accumulation_steps=4)\n",
        "\n",
        "    # cross validation\n",
        "    cv = StratifiedKFold(n_splits=k, shuffle=True)\n",
        "    splits = cv.split(text_df['text'], text_df['label'])\n",
        "    for fold, (train_index, test_index) in enumerate(splits):\n",
        "\n",
        "        # train and test sets\n",
        "        train_split = tokenized_datasets.select(train_index.tolist())\n",
        "        test_split = tokenized_datasets.select(test_index.tolist())\n",
        "\n",
        "        # trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_set,\n",
        "            eval_dataset=test_set,\n",
        "            compute_metrics=compute_metrics,)\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        trainer.evaluate()\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KkTuIyXClOu"
      },
      "outputs": [],
      "source": [
        "BERT_Train(prep1_df, target='Recovery', k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWoQbrzhw4Pi"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o5tMmuGwIdf4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets;\n",
        "!pip install evaluate;\n",
        "!pip install -U accelerate;\n",
        "!pip install -U transformers;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64T_LR1UEaqf"
      },
      "outputs": [],
      "source": [
        "EndDesc_cols = [col for col in prep1_df.columns if 'EndDesc' in col]\n",
        "explanatory_df = prep1_df.drop(['ReliableChangeDesc', 'ReliableRecovery', 'Recovery'] + EndDesc_cols, axis = 1)\n",
        "\n",
        "text = explanatory_df.apply(lambda row: row.to_json(), axis=1)\n",
        "text_df = pd.DataFrame({\n",
        "    'text': text,\n",
        "    'label': prep1_df['Recovery']})\n",
        "\n",
        "text_df = text_df.dropna()\n",
        "\n",
        "#!pip install datasets;\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "\n",
        "dataset = Dataset.from_pandas(text_df)\n",
        "\n",
        "train_size = int((text_df.shape[0]) * 0.8)\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "test_dataset = dataset.select(range(train_size, len(dataset)))\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'test': test_dataset})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc64G9Fzsz0x"
      },
      "outputs": [],
      "source": [
        "### Tokenizer ###\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# small train and evaluation sets\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(79Z))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(79))\n",
        "\n",
        "### Initialise base model ###\n",
        "\n",
        "from transformers import GPT2ForSequenceClassification\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "\n",
        "### Evaluate method ###\n",
        "\n",
        "#!pip install evaluate;\n",
        "import evaluate\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "### Fine-tune (Trainer method) ###\n",
        "\n",
        "#!pip install -U accelerate;\n",
        "#!pip install -U transformers;\n",
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "   output_dir=\"test_trainer\",\n",
        "   #evaluation_strategy=\"epoch\",\n",
        "   per_device_train_batch_size=1,  # Reduce batch size here\n",
        "   per_device_eval_batch_size=1,   # Optionally, reduce for evaluation as well\n",
        "   gradient_accumulation_steps=4)\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=small_train_dataset,\n",
        "   eval_dataset=small_eval_dataset,\n",
        "   compute_metrics=compute_metrics,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUqKRLpoGjzF"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqrj3FnKGjL9"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnGM3ELEGcsw"
      },
      "outputs": [],
      "source": [
        "# trainer.save_model(\"test_trainer\")\n",
        "# tokenizer.save_pretrained(\"test_trainer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chO9Z8iAEdzf"
      },
      "outputs": [],
      "source": [
        "# tokenize data\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# small train and evaluation sets\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(50))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(50))\n",
        "\n",
        "from transformers import BertForSequenceClassification\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# !pip install evaluate\n",
        "import evaluate\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# !pip install -U accelerate transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",\n",
        "    per_device_train_batch_size=1,  # Reduce batch size here\n",
        "    per_device_eval_batch_size=1,   # Optionally, reduce for evaluation as well\n",
        "    gradient_accumulation_steps=4)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vlTs5G2aFMfI"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU0zjUMoFNSp"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "titCJKGhlNB_"
      },
      "outputs": [],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scwTZCNpkegv"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import optuna\n",
        "import evaluate\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Create small training and evaluation datasets\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2001).select(range(50))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2001).select(range(50))\n",
        "\n",
        "# Load model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Metric for evaluation\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Function to initialize the model\n",
        "def model_init():\n",
        "    return BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Training arguments template\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Define hyperparameter search space\n",
        "def hyperparameter_search_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 5e-5, 5e-4, log=True),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [1, 2, 4])\n",
        "    }\n",
        "\n",
        "# Conduct hyperparameter search\n",
        "best_trial = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"optuna\",\n",
        "    n_trials=10,\n",
        "    compute_objective=lambda metrics: metrics[\"eval_accuracy\"],\n",
        "    hp_space=hyperparameter_search_space\n",
        ")\n",
        "\n",
        "print(\"Best Hyperparameters:\\n\", best_trial.hyperparameters)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6yv5PiBjVOH"
      },
      "source": [
        "## Old"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyoCO8qRSoZd"
      },
      "source": [
        "** **Consider whether order of features matters - its sequential** **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOnuq6qmaynN"
      },
      "outputs": [],
      "source": [
        "bert_param_grid = {\n",
        "    'classifier__max_length': [128, 256, 512]\n",
        "    # Add more hyperparameters if needed\n",
        "}\n",
        "\n",
        "bert1_model = BERT_ModelSelection(serialised_df, 'serialised_data', 'Recovery', bert_param_grid, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjoL2j2ce56S"
      },
      "outputs": [],
      "source": [
        "ModelSelection_Summary(bert1_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "W1ODmQunSVNP",
        "THGbfknuDUj7",
        "UGdQ8RZ5f26v",
        "74NDOIP3MXhd",
        "FMCGtYfVQbR3",
        "piAcrqMCUf_b",
        "lTOHK-xOBdpr",
        "_pSpe5vf52AJ",
        "nFVrpck2Ls5P",
        "G3Gp1AdsW3ya",
        "Wv0ov3LGKkTv",
        "ILBLDx4T1ycH",
        "y8mML9d7ahEe",
        "p3-sW_8Qa2Pt",
        "xV4vMwKNS5vp",
        "IVmXoPNQL1IT",
        "uLNplPcaaHL3",
        "3QdplL0yE1eg",
        "blf8aoZCE7Jy",
        "HaF4M1XMUBbq",
        "W4uJzTCazeMz",
        "VO83MmW4-RyT",
        "X7VGDeL_5VMn",
        "JFx52VqtDlOV",
        "N7BmhUef8gQK",
        "TcoDd1cI5Tuy",
        "QiKr7I10DDMQ",
        "t-o8JXYW8dbh",
        "R-2npawoQoAB",
        "YzIRxGF8Xa-e",
        "NUaRK4DQ9ALv",
        "_ltCoR-45q6B",
        "Mn6qz3X_AkRS",
        "w-UUReUl-PF6",
        "H5nyvLAZsP4o",
        "r4f_FnudsSmA",
        "zU2DB7h8dXeX",
        "TSI5QUpac_nT",
        "ffQj1QIRs8y6",
        "i4qStBJVs6V5",
        "WWoQbrzhw4Pi",
        "h6yv5PiBjVOH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}