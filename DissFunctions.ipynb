{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "PQWWr-pT5BfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A97xMnpA4pbc",
        "outputId": "48364584-c320-440c-ea24-2aa38a52638d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     Unnamed: 0 IAPTus_Num Referral Date  Age_ReferralRequest_ReceivedDate  \\\n",
              " 0             1      24475    08/09/2018                          5.099020   \n",
              " 1             2    24476_1    10/04/2019                          4.358899   \n",
              " 2             3    24476_2    28/03/2021                          4.582576   \n",
              " 3             4      24479    16/04/2019                          4.898979   \n",
              " 4             5      24482    07/09/2018                          0.000000   \n",
              " ..          ...        ...           ...                               ...   \n",
              " 723         724      24706           NaN                          0.000000   \n",
              " 724         725      24721           NaN                          0.000000   \n",
              " 725         727      24744           NaN                          0.000000   \n",
              " 726         730      24658           NaN                          0.000000   \n",
              " 727         737      24786           NaN                          0.000000   \n",
              " \n",
              "      EthnicDescGroupCode  EthnicCategoryGroupShortCode GenderIdentity  \\\n",
              " 0                    1.0                           1.0              2   \n",
              " 1                    1.0                           1.0              2   \n",
              " 2                    1.0                           1.0              2   \n",
              " 3                    1.0                           1.0              2   \n",
              " 4                    2.0                           2.0              2   \n",
              " ..                   ...                           ...            ...   \n",
              " 723                  NaN                           NaN            NaN   \n",
              " 724                  NaN                           NaN            NaN   \n",
              " 725                  NaN                           NaN            NaN   \n",
              " 726                  NaN                           NaN            NaN   \n",
              " 727                  NaN                           NaN            NaN   \n",
              " \n",
              "      SexualOrientationDesc                                            EndDesc  \\\n",
              " 0                      NaN            Mutually agreed completion of treatment   \n",
              " 1                      NaN            Mutually agreed completion of treatment   \n",
              " 2                      NaN  Termition of treatment earlier than Care Profe...   \n",
              " 3                      NaN            Mutually agreed completion of treatment   \n",
              " 4                      NaN  Termition of treatment earlier than Care Profe...   \n",
              " ..                     ...                                                ...   \n",
              " 723                    NaN                                                NaN   \n",
              " 724                    NaN                                                NaN   \n",
              " 725                    NaN                                                NaN   \n",
              " 726                    NaN                                                NaN   \n",
              " 727                    NaN                                                NaN   \n",
              " \n",
              "     EndDescGroupShort  ...  Item216  Item217  Item218  Item219  Item220  \\\n",
              " 0    Seen and treated  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 1    Seen and treated  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 2    Seen and treated  ...        0        0        1        1        1   \n",
              " 3    Seen and treated  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 4    Seen and treated  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " ..                ...  ...      ...      ...      ...      ...      ...   \n",
              " 723               NaN  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 724               NaN  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 725               NaN  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 726               NaN  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 727               NaN  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " \n",
              "     Item221 Item222 Item223 Item224 Item225  \n",
              " 0       NaN     NaN     NaN     NaN     NaN  \n",
              " 1       NaN     NaN     NaN     NaN     NaN  \n",
              " 2         0       0       0       0       0  \n",
              " 3       NaN     NaN     NaN     NaN     NaN  \n",
              " 4       NaN     NaN     NaN     NaN     NaN  \n",
              " ..      ...     ...     ...     ...     ...  \n",
              " 723     NaN     NaN     NaN     NaN     NaN  \n",
              " 724     NaN     NaN     NaN     NaN     NaN  \n",
              " 725     NaN     NaN     NaN     NaN     NaN  \n",
              " 726     NaN     NaN     NaN     NaN     NaN  \n",
              " 727     NaN     NaN     NaN     NaN     NaN  \n",
              " \n",
              " [728 rows x 279 columns],\n",
              "      Unnamed: 0 IAPTus_Num Referral Date  Age_ReferralRequest_ReceivedDate  \\\n",
              " 0             1      24475    08/09/2018                          5.099020   \n",
              " 1             2    24476_1    10/04/2019                          4.358899   \n",
              " 2             3    24476_2    28/03/2021                          4.582576   \n",
              " 3             4      24479    16/04/2019                          4.898979   \n",
              " 4             5      24482    07/09/2018                          0.000000   \n",
              " ..          ...        ...           ...                               ...   \n",
              " 723         724      24706           NaN                          0.000000   \n",
              " 724         725      24721           NaN                          0.000000   \n",
              " 725         727      24744           NaN                          0.000000   \n",
              " 726         730      24658           NaN                          0.000000   \n",
              " 727         737      24786           NaN                          0.000000   \n",
              " \n",
              "      EthnicDescGroupCode  EthnicCategoryGroupShortCode GenderIdentity  \\\n",
              " 0                    1.0                           1.0              2   \n",
              " 1                    1.0                           1.0              2   \n",
              " 2                    1.0                           1.0              2   \n",
              " 3                    1.0                           1.0              2   \n",
              " 4                    2.0                           2.0              2   \n",
              " ..                   ...                           ...            ...   \n",
              " 723                  NaN                           NaN            NaN   \n",
              " 724                  NaN                           NaN            NaN   \n",
              " 725                  NaN                           NaN            NaN   \n",
              " 726                  NaN                           NaN            NaN   \n",
              " 727                  NaN                           NaN            NaN   \n",
              " \n",
              "      SexualOrientationDesc                                            EndDesc  \\\n",
              " 0                      NaN            Mutually agreed completion of treatment   \n",
              " 1                      NaN            Mutually agreed completion of treatment   \n",
              " 2                      NaN  Termition of treatment earlier than Care Profe...   \n",
              " 3                      NaN            Mutually agreed completion of treatment   \n",
              " 4                      NaN  Termition of treatment earlier than Care Profe...   \n",
              " ..                     ...                                                ...   \n",
              " 723                    NaN                                                NaN   \n",
              " 724                    NaN                                                NaN   \n",
              " 725                    NaN                                                NaN   \n",
              " 726                    NaN                                                NaN   \n",
              " 727                    NaN                                                NaN   \n",
              " \n",
              "     EndDescGroupShort  ...  Item216  Item217  Item218  Item219  Item220  \\\n",
              " 0    Seen and treated  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 1    Seen and treated  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 2    Seen and treated  ...        0        0        1        1        1   \n",
              " 3    Seen and treated  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 4    Seen and treated  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " ..                ...  ...      ...      ...      ...      ...      ...   \n",
              " 723               NaN  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 724               NaN  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 725               NaN  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 726               NaN  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " 727               NaN  ...      NaN      NaN      NaN      NaN      NaN   \n",
              " \n",
              "     Item221 Item222 Item223 Item224 Item225  \n",
              " 0       NaN     NaN     NaN     NaN     NaN  \n",
              " 1       NaN     NaN     NaN     NaN     NaN  \n",
              " 2         0       0       0       0       0  \n",
              " 3       NaN     NaN     NaN     NaN     NaN  \n",
              " 4       NaN     NaN     NaN     NaN     NaN  \n",
              " ..      ...     ...     ...     ...     ...  \n",
              " 723     NaN     NaN     NaN     NaN     NaN  \n",
              " 724     NaN     NaN     NaN     NaN     NaN  \n",
              " 725     NaN     NaN     NaN     NaN     NaN  \n",
              " 726     NaN     NaN     NaN     NaN     NaN  \n",
              " 727     NaN     NaN     NaN     NaN     NaN  \n",
              " \n",
              " [728 rows x 279 columns])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_features(df): # non-questionnaire features\n",
        "\n",
        "    df.rename(columns = {\n",
        "        'Unnamed: 0': 'CaseID',\n",
        "        'IAPTus_Num': 'ClientID',\n",
        "        'Referral Date': 'ReferralDate',\n",
        "        'Age_ReferralRequest_ReceivedDate': 'AgeAtReferralRequest',\n",
        "        'EthnicDescGroupCode': 'EthnicCode',\n",
        "        'EthnicCategoryGroupShortCode': 'EthnicCodeShort',\n",
        "        'GenderIdentity': 'Gender',\n",
        "        'SexualOrientationDesc': 'SexualOrientation',\n",
        "        'EndDescGroupShort': 'Treated',\n",
        "        'AllocatedCareProfNameCode': 'TherapistID',\n",
        "        'JobTitleCode': 'ExperienceLevel',\n",
        "        'Days to first assessment': 'DaystoAssessment',\n",
        "        'Days to first treatment': 'DaystoTreatment',\n",
        "        'CountOfAttendedCareContacts': 'CareContacts',\n",
        "        'RecoveryDesc': 'Recovery',\n",
        "        'ReliableRecoveryDesc': 'ReliableRecovery',\n",
        "        'Date': 'DateOfQuestionnaire'},\n",
        "        inplace = True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "FIsMdbPKJzzZ"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1ODmQunSVNP"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV1f10TTIv53"
      },
      "source": [
        "## 1.2 Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "8vw4-ITlmfz1"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "\n",
        "    drive.mount(\"/content/drive\") # access google drive folder\n",
        "\n",
        "    file_path = '/content/drive/MyDrive/Data/Dissertation_Data/'\n",
        "    df = pd.read_csv(file_path + 'mental_health.csv', delimiter = ',') # csv\n",
        "    raw_df = df.copy()\n",
        "\n",
        "    return df, raw_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "2nLysn57q38W"
      },
      "outputs": [],
      "source": [
        "def manage_df(df, name, option):\n",
        "\n",
        "    diss_path = '/content/drive/MyDrive/Data/Dissertation_Data/'\n",
        "\n",
        "    if option == 'save':\n",
        "        df.to_csv(file_path + name + '.csv')\n",
        "        print('Saved!')\n",
        "\n",
        "    elif option == 'load':\n",
        "        df = pd.read_csv(file_path + name + '.csv', index_col=0)\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtShKAeACyh2"
      },
      "source": [
        "2.2 Non-Questionnaire Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "5I8pmofOI0n8"
      },
      "outputs": [],
      "source": [
        "def rename_features(df): # non-questionnaire features\n",
        "\n",
        "    df.rename(columns = {\n",
        "        'Unnamed: 0': 'CaseID',\n",
        "        'IAPTus_Num': 'ClientID',\n",
        "        'Referral Date': 'ReferralDate',\n",
        "        'Age_ReferralRequest_ReceivedDate': 'AgeAtReferralRequest',\n",
        "        'EthnicDescGroupCode': 'EthnicCode',\n",
        "        'EthnicCategoryGroupShortCode': 'EthnicCodeShort',\n",
        "        'GenderIdentity': 'Gender',\n",
        "        'SexualOrientationDesc': 'SexualOrientation',\n",
        "        'EndDescGroupShort': 'Treated',\n",
        "        'AllocatedCareProfNameCode': 'TherapistID',\n",
        "        'JobTitleCode': 'ExperienceLevel',\n",
        "        'Days to first assessment': 'DaystoAssessment',\n",
        "        'Days to first treatment': 'DaystoTreatment',\n",
        "        'CountOfAttendedCareContacts': 'CareContacts',\n",
        "        'RecoveryDesc': 'Recovery',\n",
        "        'ReliableRecoveryDesc': 'ReliableRecovery',\n",
        "        'Date': 'DateOfQuestionnaire'},\n",
        "        inplace = True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "5XZE6Oq-LHCV"
      },
      "outputs": [],
      "source": [
        "def create_referral_count(df):\n",
        "    def count_referrals(col):\n",
        "        if '_1' in col:\n",
        "            return 1\n",
        "        elif '_2' in col:\n",
        "            return 2\n",
        "        elif '_3' in col:\n",
        "            return 3\n",
        "        elif '_4' in col:\n",
        "            return 4\n",
        "        elif '_5' in col:\n",
        "            return 5\n",
        "        else:\n",
        "            return 1\n",
        "    df.insert(2, \"ReferralCount\", df['ClientID'].apply(count_referrals)) # introduce next to ClientID\n",
        "    return df\n",
        "\n",
        "def clean_client_id(df):\n",
        "    for text in ['_1', '_2', '_3', '_4']:\n",
        "        df['ClientID'] = df['ClientID'].str.replace(text, '') # remove ending\n",
        "    df['ClientID'] = pd.to_numeric(df['ClientID'])\n",
        "    return df\n",
        "\n",
        "def convert_features_to_datetime(df):\n",
        "    df['ReferralDate'] = pd.to_datetime(df['ReferralDate'], format = '%d/%m/%Y')\n",
        "    df['DateOfQuestionnaire'] = pd.to_datetime(df['DateOfQuestionnaire'], format = '%d/%m/%Y')\n",
        "    return df\n",
        "\n",
        "def convert_float_features_to_int(df):\n",
        "    df['EthnicCode'] = df['EthnicCode'].astype('Int64') # Int deals with NaNs, int does not\n",
        "    df['EthnicCodeShort'] = df['EthnicCodeShort'].astype('Int64')\n",
        "    df['TherapistID'] = df['TherapistID'].astype('Int64')\n",
        "    df['ExperienceLevel'] = df['ExperienceLevel'].astype('Int64')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "uAOanAnaN9hJ"
      },
      "outputs": [],
      "source": [
        "def map_features(df):\n",
        "\n",
        "    Gender_map = {\n",
        "        'CHANGE ME': np.nan,\n",
        "        'X': np.nan}\n",
        "    df['Gender'] = df['Gender'].replace(Gender_map).astype('Int64')\n",
        "\n",
        "    Treated_map = {\n",
        "        'Seen and treated': 1,\n",
        "        'Seen but not treated': 0}\n",
        "    df['Treated'] = df['Treated'].replace(Treated_map).astype('Int64')\n",
        "\n",
        "    ReliableChangeDesc_map = {\n",
        "        'Reliable improvement': 2, # what about(-1, 0, 1)?\n",
        "        'No reliable change': 1,\n",
        "        'Reliable deterioration': 0,\n",
        "        'Not applicable': np.nan}\n",
        "    df['ReliableChangeDesc'] = df['ReliableChangeDesc'].replace(ReliableChangeDesc_map).astype('Int64')\n",
        "\n",
        "    Recovery_map = {\n",
        "        'At recovery': 1,\n",
        "        'Not at recovery': 0,\n",
        "        'Not applicable': np.nan}\n",
        "    df['Recovery'] = df['Recovery'].replace(Recovery_map).astype('Int64')\n",
        "\n",
        "    ReliableRecovery_map = {\n",
        "        'Reliable recovery': 1,\n",
        "        'No reliable recovery': 0,\n",
        "        'Not applicable': np.nan}\n",
        "    df['ReliableRecovery'] = df['ReliableRecovery'].replace(ReliableRecovery_map).astype('Int64')\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "quT-_D2KcbCD"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode_features(df):\n",
        "\n",
        "    EndDesc_cols = pd.get_dummies(df['EndDesc'], prefix = 'EndDesc')\n",
        "    EndDesc_index = df.columns.get_loc('EndDesc')\n",
        "    df = pd.concat([df.iloc[:, :EndDesc_index + 1], EndDesc_cols, df.iloc[:, EndDesc_index + 1:]], axis = 1)\n",
        "    df = df.drop(columns = ['EndDesc'])\n",
        "\n",
        "    EndDescShort_cols = pd.get_dummies(df['EndDescShort'], prefix = 'EndDescShort')\n",
        "    EndDescShort_index = df.columns.get_loc('EndDescShort')\n",
        "    df = pd.concat([df.iloc[:, :EndDescShort_index + 1], EndDescShort_cols, df.iloc[:, EndDescShort_index + 1:]], axis = 1)\n",
        "    df = df.drop(columns = ['EndDescShort'])\n",
        "\n",
        "    return df\n",
        "\n",
        "def convert_to_int_features(df):\n",
        "    int_cols = ['SexualOrientation', 'DaystoAssessment', 'DaystoTreatment', 'CareContacts']\n",
        "    for col in int_cols:\n",
        "        df[col] = df[col].astype('Int64')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "Hu8simKq4woU"
      },
      "outputs": [],
      "source": [
        "def plot_features(df):\n",
        "\n",
        "    plot_cols = 4\n",
        "    plot_rows = len(df.columns)//4 + 1\n",
        "\n",
        "    plt.figure(figsize = (plot_cols*3, plot_rows*3))\n",
        "\n",
        "    for i, col in enumerate(df.columns, start = 1):\n",
        "        if df[col].dtype in ['int64', 'float64', 'Int64', 'datetime64[ns]', 'bool']:\n",
        "            data = df[col].dropna()\n",
        "            if df[col].dtype == 'bool':  # convert boolean to integers\n",
        "                data = data.astype(int)\n",
        "            plt.subplot(plot_rows, 4, i)\n",
        "            plt.hist(data, bins = 20, color='darkgrey', edgecolor='white')\n",
        "            plt.title(col)\n",
        "            plt.xlabel(col)\n",
        "            plt.ylabel('Frequency')\n",
        "\n",
        "    plt.suptitle('Data Plots', y = 1, fontsize = 24)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "2nfsedT9_wYy"
      },
      "outputs": [],
      "source": [
        "def preprocess_nonques_features(df):\n",
        "\n",
        "    df = rename_features(df)\n",
        "    df = create_referral_count(df)\n",
        "    df = clean_client_id(df)\n",
        "    df = convert_features_to_datetime(df)\n",
        "    df = convert_float_features_to_int(df)\n",
        "    df = map_features(df)\n",
        "    df = one_hot_encode_features(df)\n",
        "    df = convert_to_int_features(df)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW2b1hV0CUIz"
      },
      "source": [
        "2.3 Questionnaire Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "GsoRt1DtCcWD"
      },
      "outputs": [],
      "source": [
        "def preprocess_ques_features(df):\n",
        "\n",
        "    # convert all variables to float\n",
        "    for col in df.columns[27:]:\n",
        "        df[col] = pd.to_numeric(df[col], errors = 'coerce') # (I think this also removes '.')\n",
        "\n",
        "    item_cols = df.columns.str.contains('Item').tolist()\n",
        "    item_cols = df.columns[item_cols]\n",
        "    for col in item_cols:\n",
        "        df[col] = df[col].apply(lambda x: x if pd.isna(x) or x.is_integer() else np.nan) # NaN non-integer values\n",
        "        df[col] = df[col].astype('Int64') # convert to int\n",
        "\n",
        "    thresh_cols = df.columns.str.contains('Threshold').tolist()\n",
        "    thresh_cols = df.columns[thresh_cols]\n",
        "    for col in thresh_cols:\n",
        "        df[col] = df[col].apply(lambda x: x if pd.isna(x) or x.is_integer() else np.nan)\n",
        "        df[col] = df[col].astype('Int64')\n",
        "\n",
        "    total_cols = df.columns.str.contains('Total').tolist()\n",
        "    total_cols = df.columns[total_cols]\n",
        "    for col in total_cols:\n",
        "        df[col] = df[col].apply(lambda x: x if pd.isna(x) or x.is_integer() else np.nan)\n",
        "        df[col] = df[col].astype('Int64')\n",
        "\n",
        "    # bool cols (all)\n",
        "    bool_cols = df.select_dtypes(include = bool)\n",
        "    for col in bool_cols:\n",
        "        df[col] = df[col].astype('Int64')\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gQIQg-Y-TmD"
      },
      "source": [
        "2 Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "S3XWbNeNlGLl"
      },
      "outputs": [],
      "source": [
        "def Clean_Data(df):\n",
        "\n",
        "    # clean non-questionnaire data\n",
        "    df = preprocess_nonques_features(df)\n",
        "\n",
        "    # clean questionnaire data\n",
        "    df = preprocess_ques_features(df)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THGbfknuDUj7"
      },
      "source": [
        "## 1.3 EDA Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjkFhRmV-aVr"
      },
      "source": [
        "3 EDA Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "yYGl8zk5Dx48"
      },
      "outputs": [],
      "source": [
        "def single_summary_plot(df, tic_freq):\n",
        "\n",
        "    statistics = df.describe()\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    for index, row in statistics.iterrows():\n",
        "        plt.plot(statistics.columns, row, label=index)\n",
        "\n",
        "    plt.xlabel('Column')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Summary Statistics')\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.xticks(statistics.columns[::tic_freq])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "6fkAzO2bD3mI"
      },
      "outputs": [],
      "source": [
        "def summary_plot(dfs, dfs_names):\n",
        "\n",
        "    # subplot dimensions\n",
        "    num_dfs = len(dfs)\n",
        "    num_rows = num_dfs // 2 + num_dfs % 2\n",
        "\n",
        "    # plot\n",
        "    plt.figure(figsize=(15, 5 * num_rows))\n",
        "    for i, df in enumerate(dfs):\n",
        "\n",
        "        statistics = df.describe()\n",
        "\n",
        "        plt.subplot(num_rows, 2, i+1)\n",
        "        for index, row in statistics.iterrows():\n",
        "            plt.plot(statistics.columns, row, label=index)\n",
        "\n",
        "        plt.xlabel('Column')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title(f'{dfs_names[i]} Summary')\n",
        "        plt.xticks(statistics.columns[::2])\n",
        "        plt.legend(loc = 'center right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGdQ8RZ5f26v"
      },
      "source": [
        "## 1.4 Processing Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74NDOIP3MXhd"
      },
      "source": [
        "### 4.1 Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "kK5D0BZEMVkn"
      },
      "outputs": [],
      "source": [
        "def find_duplicates_features(df):\n",
        "    dupe_cols = np.transpose(df).duplicated()\n",
        "    dupe_cols = df.columns[dupe_cols].tolist()\n",
        "    dupes_of = {}\n",
        "    for col_name in dupe_cols:\n",
        "        col_values = df[col_name]\n",
        "        dupes = [other_col for other_col in df.columns if (other_col != col_name) and df[other_col].equals(col_values)]\n",
        "        dupes_of[col_name] = dupes\n",
        "        print(f'{col_name} is a duplicate of: {dupes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9_mblKrB0dM"
      },
      "source": [
        "Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "gjiI4WFX_i2r"
      },
      "outputs": [],
      "source": [
        "def drop_duplicate_features(df):\n",
        "    dupe_cols = np.transpose(df).duplicated()\n",
        "    dupe_cols = df.columns[dupe_cols].tolist()\n",
        "    df = df.drop(columns = dupe_cols)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMCGtYfVQbR3"
      },
      "source": [
        "### 4.2 Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "S7h4646W_D_y"
      },
      "outputs": [],
      "source": [
        "def column_contents(df):\n",
        "    pot_cols = []\n",
        "    for col in df.columns:\n",
        "        if not df[col].isin([0, 1, pd.NA]).all():\n",
        "            pot_cols.append(col)\n",
        "    if len(pot_cols) == 0:\n",
        "        print(f'Columns only contain 0, 1 and <NA>')\n",
        "    else:\n",
        "        print(f'Columns contining more than 0, 1 and <NA>: {pot_cols}')\n",
        "    return pot_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "RX_6goE0QeVF"
      },
      "outputs": [],
      "source": [
        "def mad(df):\n",
        "    median = df.median()\n",
        "    deviations = np.abs(df - median)\n",
        "    mad_val = deviations.median() # MAD\n",
        "    return mad_val\n",
        "\n",
        "def modified_zscore(df, threshold = 3.5):\n",
        "    median = df.median()\n",
        "    mad_val = mad(df) # MAD\n",
        "    modified_zscores = 0.6745 * (df - median) / mad_val # modified Z-score\n",
        "    return np.abs(modified_zscores) > threshold\n",
        "\n",
        "def find_outlier_cols(df, threshold = 3.5): # counts also\n",
        "\n",
        "    outlier_df = modified_zscore(df)\n",
        "    outlier_cols = set()\n",
        "    for col in outlier_df.columns:\n",
        "        outlier_list = []\n",
        "        for i, val in outlier_df[col].items():\n",
        "            if pd.notna(val) and val:\n",
        "                outlier_list.append(df.iloc[i][col])\n",
        "        if outlier_list:\n",
        "            outlier_cols.add(col)\n",
        "            print(f'{col} outlier count: {len(outlier_list)}')\n",
        "\n",
        "    # return outlier_df\n",
        "    outlier_cols = list(outlier_cols)\n",
        "    outlier_df = df[outlier_cols]\n",
        "    return outlier_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "NytZCSxER-sR"
      },
      "outputs": [],
      "source": [
        "def plot_outlier_cols(outlier_df, title, legend=True):\n",
        "    outlier_df.plot(figsize = (5, 3))\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Total Score')\n",
        "    if legend:\n",
        "        plt.legend(loc='upper right')\n",
        "    else:\n",
        "        plt.gca().legend().set_visible(False)\n",
        "    plt.show()\n",
        "\n",
        "def plot_datetime_features(df):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i, col in enumerate(datetime_cols):\n",
        "        plt.subplot(1, 2, i + 1)\n",
        "        df[col].hist(bins = 100, color = 'grey', edgecolor = 'white')\n",
        "        plt.title(col)\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.grid(False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_outlier_cols2(outlier_df, title, color):\n",
        "\n",
        "    plot_cols = 4\n",
        "    plot_rows = len(outlier_df.columns)//4 + 1\n",
        "    plt.figure(figsize = (plot_cols*3, plot_rows*3))\n",
        "\n",
        "    for i, col in enumerate(outlier_df.columns, start = 1):\n",
        "        plt.subplot(plot_rows, 4, i)\n",
        "        outlier_df[col].plot(color=color)\n",
        "        plt.title(col)\n",
        "        plt.xlabel('Index')\n",
        "        plt.ylabel('Total Score')\n",
        "\n",
        "    plt.suptitle(title, y = 1, fontsize = 24)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BmANarfB5P7"
      },
      "source": [
        "Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "HcfP6XRJ3Jfn"
      },
      "outputs": [],
      "source": [
        "def replace_ques_outliers(df): # shortens search using quartiles\n",
        "\n",
        "    item_cols = [col for col in df.columns if 'Item' in col]\n",
        "    item_df = df[item_cols]\n",
        "\n",
        "    total_cols = [col for col in df.columns if 'Total' in col]\n",
        "    total_df = df[total_cols]\n",
        "\n",
        "    # replace negative totals with nan\n",
        "    for col in total_df:\n",
        "        df.loc[df[col] < 0, col] = np.nan # replace in df\n",
        "        total_df.loc[total_df[col] < 0, col] = np.nan # replace in total_df\n",
        "\n",
        "    # find col contents\n",
        "    def create_pot_cols(df):\n",
        "        pot_cols = []\n",
        "        for col in df.columns:\n",
        "            if not df[col].isin([0, 1, pd.NA]).all():\n",
        "                pot_cols.append(col)\n",
        "        return pot_cols\n",
        "\n",
        "    # detect whether cols are just 0,1,na\n",
        "    item_pot_cols = create_pot_cols(item_df)\n",
        "    total_pot_cols = create_pot_cols(total_df)\n",
        "\n",
        "    # potential columns containing outliers (shortening the search, only these can contain outliers)\n",
        "    total_pot_df = total_df[total_pot_cols]\n",
        "    item_pot_df = item_df[item_pot_cols]\n",
        "\n",
        "    # outlier counter and df function\n",
        "    def create_outlier_df(df, threshold = 3.5):\n",
        "\n",
        "        outlier_df = modified_zscore(df)\n",
        "        outlier_cols = set()\n",
        "        for col in outlier_df.columns:\n",
        "            outlier_list = []\n",
        "            for i, val in outlier_df[col].items():\n",
        "                if pd.notna(val) and val:\n",
        "                    outlier_list.append(df.iloc[i][col])\n",
        "            if outlier_list:\n",
        "                outlier_cols.add(col)\n",
        "\n",
        "        # return outlier_df\n",
        "        outlier_cols = list(outlier_cols)\n",
        "        outlier_df = df[outlier_cols]\n",
        "        return outlier_df\n",
        "\n",
        "    # find outliers in variables\n",
        "    total_outlier_df = create_outlier_df(total_pot_df)\n",
        "    item_outlier_df = create_outlier_df(item_pot_df)\n",
        "\n",
        "    # replace outliers with nan\n",
        "    for col in total_outlier_df:\n",
        "        df.loc[df[col] > 200, col] = np.nan\n",
        "        total_outlier_df.loc[total_outlier_df[col] > 200, col] = np.nan\n",
        "\n",
        "    for col in item_outlier_df:\n",
        "        df.loc[df[col] > 50, col] = np.nan\n",
        "        item_outlier_df.loc[item_outlier_df[col] > 50, col] = np.nan\n",
        "\n",
        "    df.loc[df['Total14'] > 37, 'Total14'] = np.nan\n",
        "    df.loc[df['Item70'] > 3, 'Item70'] = np.nan\n",
        "    df.loc[df['Item132'] > 5, 'Item132'] = np.nan\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "FOvHn6v9CMeP"
      },
      "outputs": [],
      "source": [
        "def replace_outliers(df):\n",
        "    df.loc[df['AgeAtReferralRequest'] == 0, 'AgeAtReferralRequest'] = np.nan\n",
        "    df = replace_ques_outliers(df)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piAcrqMCUf_b"
      },
      "source": [
        "### 4.3 Constant and Quasi Constant Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "odjJkEB63184"
      },
      "outputs": [],
      "source": [
        "def drop_const_features(df):\n",
        "    const_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
        "    df = df.drop(columns = const_cols)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "bvejzgiDUj3L"
      },
      "outputs": [],
      "source": [
        "def plot_datetime_features(df):\n",
        "    plt.figure(figsize = (10,4))\n",
        "    for i, col in enumerate(datetime_cols, 1):\n",
        "        plt.subplot(1, len(datetime_cols), i)\n",
        "        plt.hist(df[col], bins = 20, color = 'grey', edgecolor = 'white')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title(col)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JibfK4xBB-pj"
      },
      "source": [
        "Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "RijZyYsG_a4k"
      },
      "outputs": [],
      "source": [
        "def quasi_percentage(df):\n",
        "\n",
        "    # ordinal variables\n",
        "    ordinal_df = df.select_dtypes(include = ['int64', 'Int64'])\n",
        "\n",
        "    # select col modes\n",
        "    modes = ordinal_df.mode() # mode of each col\n",
        "    modes = modes.iloc[0] # select modes only\n",
        "\n",
        "    # calc quasi percentage\n",
        "    quasi_percentages = (ordinal_df == modes).sum() / len(ordinal_df) # how quasi a col is\n",
        "\n",
        "    return quasi_percentages\n",
        "\n",
        "# drop quasi features\n",
        "def drop_quasi_features(df, threshold):\n",
        "\n",
        "    # calculate quasi percentage\n",
        "    quasi_percentages = quasi_percentage(df)\n",
        "\n",
        "    # 0.995 is about 3 observations\n",
        "    exceeding_threshold_columns = quasi_percentages[quasi_percentages > threshold].index\n",
        "    df = df.drop(columns = exceeding_threshold_columns)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTOHK-xOBdpr"
      },
      "source": [
        "### 4.4 Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "-uxid4K6dlg8"
      },
      "outputs": [],
      "source": [
        "def select_future_features(df):\n",
        "    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "    #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\n",
        "    future_vars = df.drop(columns = ['ReliableChangeDesc', 'Recovery', 'ReliableRecovery'] + EndDesc_cols)\n",
        "    return future_vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "C_o2RbFkMvaX"
      },
      "outputs": [],
      "source": [
        "def generate_corr_matricies(df, load_matrices):\n",
        "\n",
        "    file_path = '/content/drive/MyDrive/Data/Dissertation_Data/'\n",
        "\n",
        "    if not load_matrices:\n",
        "        # select explanatory variables, removing info on future\n",
        "        future_df = select_future_features(df)\n",
        "\n",
        "        # create correlation matrices\n",
        "        kendall_corr = future_df.corr(method='kendall').abs()\n",
        "        spearman_corr = future_df.corr(method='spearman').abs()\n",
        "        kendall_corr.to_csv(file_path + 'kendall_corr.csv') # save\n",
        "        spearman_corr.to_csv(file_path + 'spearman_corr.csv') # save\n",
        "\n",
        "    else:\n",
        "        kendall_corr = pd.read_csv(file_path + 'kendall_corr.csv', index_col=0)\n",
        "        spearman_corr = pd.read_csv(file_path + 'spearman_corr.csv', index_col=0)\n",
        "\n",
        "    return kendall_corr, spearman_corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "KHv11Vma1dWy"
      },
      "outputs": [],
      "source": [
        "def plot_correlation_matrices(corr_matricies):\n",
        "\n",
        "    plt.figure(figsize=(8, 3))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    sns.heatmap(corr_matricies[0], annot = False, cmap = 'flare', fmt = \".2f\", xticklabels = False, yticklabels = False)\n",
        "    plt.title('Kendall Correlation Matrix')\n",
        "\n",
        "    plt.subplot(122)\n",
        "    sns.heatmap(corr_matricies[1], annot = False, cmap = 'flare', fmt = \".2f\", xticklabels = False, yticklabels = False)\n",
        "    plt.title('Spearman Correlation Matrix')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "vaIUUFSLdsBA"
      },
      "outputs": [],
      "source": [
        "# count correlated variable pairs above each threshold\n",
        "def count_correlated_pairs(correlation_matrix, thresholds):\n",
        "\n",
        "    counts = {}\n",
        "    for threshold in thresholds:\n",
        "        correlated_vars = (correlation_matrix.abs() > threshold)\n",
        "        # get upper triangle\n",
        "        np.fill_diagonal(correlated_vars.values, False)\n",
        "        upper_triangle = correlated_vars.values[np.triu_indices_from(correlated_vars, k=1)]\n",
        "        count = np.sum(upper_triangle)\n",
        "        counts[threshold] = count\n",
        "    return counts\n",
        "\n",
        "def correlated_features_above_treshold(matricies, thresholds):\n",
        "\n",
        "    print('Kendall Correlation:')\n",
        "    kendall_counts = count_correlated_pairs(matricies[0], thresholds)\n",
        "    for threshold, count in kendall_counts.items():\n",
        "        print(f'{threshold * 100}%: {count}')\n",
        "\n",
        "    print('Spearman Correlation:')\n",
        "    spearman_counts = count_correlated_pairs(matricies[1], thresholds)\n",
        "    for threshold, count in spearman_counts.items():\n",
        "        print(f'{threshold * 100}%: {count}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "Vs7au_sOADwf"
      },
      "outputs": [],
      "source": [
        "def drop_corr_cols(df, corr_matrix, threshold, ignore=True):\n",
        "\n",
        "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "    corr_cols = [col for col in upper_tri.columns if any(upper_tri[col] > threshold)]\n",
        "\n",
        "    try:\n",
        "        df = df.drop(corr_cols, axis=1)\n",
        "    except KeyError as e:\n",
        "        if ignore:\n",
        "            #print(f\"Ignoring KeyError: {e}\")\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fj1PpVXCl-K"
      },
      "source": [
        "Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "5VBGPv-Udz12"
      },
      "outputs": [],
      "source": [
        "def remove_corr_features(df, load_matrices, threshold):\n",
        "\n",
        "    kendall_corr, spearman_corr = generate_corr_matricies(df, load_matrices)\n",
        "\n",
        "    # remove highly correlated variables\n",
        "    df = drop_corr_cols(df, kendall_corr, threshold)\n",
        "    df = drop_corr_cols(df, spearman_corr, threshold)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pSpe5vf52AJ"
      },
      "source": [
        "### 4.5 Missing Values within Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "6ooEjn1a56BK"
      },
      "outputs": [],
      "source": [
        "def drop_missing_values_col(df, threshold):\n",
        "\n",
        "    missing_value_percentage = df.isna().mean(axis=0) # cols\n",
        "\n",
        "    drop_cols = missing_value_percentage[missing_value_percentage > threshold].index\n",
        "    df = df.drop(drop_cols, axis=1)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFVrpck2Ls5P"
      },
      "source": [
        "### 4.6 Missing Values within Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "HRvOzmlg5SMq"
      },
      "outputs": [],
      "source": [
        "def drop_missing_values_row(df, threshold):\n",
        "\n",
        "    missing_value_percentage = df.isna().mean(axis=1) # rows\n",
        "\n",
        "    drop_cols = missing_value_percentage[missing_value_percentage > threshold].index\n",
        "    df = df.drop(drop_cols, axis=0)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Gp1AdsW3ya"
      },
      "source": [
        "### 4.7 Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "drv67OT8W3QC"
      },
      "outputs": [],
      "source": [
        "def standardise(df):\n",
        "    scaler = StandardScaler()\n",
        "    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv0ov3LGKkTv"
      },
      "source": [
        "### 4.8 Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "_7Ji0tiXTs0X"
      },
      "outputs": [],
      "source": [
        "def impute_data(df, method, n_neighbours=None, max_iter=None):\n",
        "\n",
        "    original_dtypes = df.dtypes.to_dict()\n",
        "\n",
        "    #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\n",
        "    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "    nontarget_df = df.drop(columns = ['ReliableChangeDesc', 'Recovery', 'ReliableRecovery'] + EndDesc_cols)\n",
        "\n",
        "    datetime_cols = nontarget_df.select_dtypes(include=['datetime64[ns]']).columns\n",
        "    for col in datetime_cols:\n",
        "        nontarget_df[col] = pd.to_numeric(nontarget_df[col])\n",
        "\n",
        "    if method == 'none':\n",
        "        nontarget_mat = nontarget_df.values\n",
        "\n",
        "    elif method == 'mean':\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        nontarget_mat = imputer.fit_transform(nontarget_df) # *numpy array*\n",
        "\n",
        "    elif method == 'median':\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        nontarget_mat = imputer.fit_transform(nontarget_df)\n",
        "\n",
        "    elif method == 'knn':\n",
        "        imputer = KNNImputer(n_neighbors=n_neighbours)\n",
        "        nontarget_mat = imputer.fit_transform(nontarget_df)\n",
        "\n",
        "    elif method == 'iterative':\n",
        "        imputer = IterativeImputer(max_iter=max_iter, random_state=2001)\n",
        "        nontarget_mat = imputer.fit_transform(nontarget_df)\n",
        "\n",
        "    nontarget_df.iloc[:, :] = nontarget_mat.tolist()\n",
        "\n",
        "    # add imputed data to df\n",
        "    for col in nontarget_df.columns:\n",
        "        df[col] = nontarget_df[col]\n",
        "\n",
        "    if method == 'none':\n",
        "        pass\n",
        "\n",
        "    else:\n",
        "        for col, dtype in original_dtypes.items():\n",
        "            if dtype == 'datetime64[ns]':\n",
        "                pass # keep as float\n",
        "            else:\n",
        "                try:\n",
        "                    df[col] = df[col].astype(dtype)\n",
        "                except (ValueError, TypeError):\n",
        "                    # *convert to original dtype*\n",
        "                    df[col] = df[col].round().astype(int)\n",
        "                    df[col] = df[col].astype(dtype)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "RW5BGatARrMi"
      },
      "outputs": [],
      "source": [
        "# def multi_impute_data(df, method='knn', n_neighbours=3, iter=1):\n",
        "\n",
        "#     EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "#     #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\n",
        "#     nontarget_df = df.drop(columns = ['ReliableChangeDesc', 'Recovery', 'ReliableRecovery'] + EndDesc_cols)\n",
        "\n",
        "#     datetime_cols = nontarget_df.select_dtypes(include=['datetime64[ns]']).columns\n",
        "#     for col in datetime_cols:\n",
        "#         nontarget_df[col] = nontarget_df[col].astype(int) / 10**9\n",
        "\n",
        "#     ordinal_cols = list(nontarget_df.select_dtypes(include=['Int64', 'int64']).columns)\n",
        "#     imputed_dfs = []\n",
        "\n",
        "#     for i in range(iter):\n",
        "\n",
        "#         copy_df = nontarget_df.copy()\n",
        "\n",
        "#         if method == 'mean':\n",
        "#             mean_imputer = SimpleImputer(strategy='mean')\n",
        "#             copy_df[nontarget_df.columns] = mean_imputer.fit_transform(copy_df[nontarget_df.columns])\n",
        "\n",
        "#         elif method == 'median':\n",
        "#             median_imputer = SimpleImputer(strategy='median')\n",
        "#             copy_df[nontarget_df.columns] = median_imputer.fit_transform(copy_df[nontarget_df.columns])\n",
        "\n",
        "#         elif method == 'knn':\n",
        "#             KNN_imputer = KNNImputer(n_neighbors=n_neighbours)\n",
        "#             copy_df[nontarget_df.columns] = KNN_imputer.fit_transform(copy_df)\n",
        "\n",
        "#         imputed_dfs.append(copy_df)\n",
        "\n",
        "#     df = np.mean(imputed_dfs, axis=0)\n",
        "#     print(df.dtypes)\n",
        "#     for col in datetime_cols:\n",
        "#         df[col] = pd.to_datetime(df[col], unit='ns')\n",
        "\n",
        "#     # convert imputed values for ordinal variables to discrete values\n",
        "#     for col in ordinal_cols:\n",
        "#         df[col] = df[col].round().astype(int)\n",
        "\n",
        "#     return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILBLDx4T1ycH"
      },
      "source": [
        "### 4.9 Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "F_pDAKYE1xx_"
      },
      "outputs": [],
      "source": [
        "def create_date_features(df):\n",
        "\n",
        "    # df['ReferralYear'] = df['ReferralDate'].dt.year\n",
        "    # df['ReferralMonth'] = df['ReferralDate'].dt.month\n",
        "    # df['ReferralWeek'] = df['ReferralDate'].dt.isocalendar().week\n",
        "    # df['ReferralDay'] = df['ReferralDate'].dt.day\n",
        "    # df['ReferralHour'] = df['ReferralDate'].dt.hour\n",
        "    # df['ReferralWeekDay'] = df['ReferralDate'].dt.dayofweek\n",
        "    # df['ReferralYearDay'] = df['ReferralDate'].dt.dayofyear\n",
        "\n",
        "    df['YearofQuestionnaire'] = df['DateOfQuestionnaire'].dt.year\n",
        "    df['MonthofQuestionnaire'] = df['DateOfQuestionnaire'].dt.month\n",
        "    df['WeekofQuestionnaire'] = df['DateOfQuestionnaire'].dt.isocalendar().week\n",
        "    df['DayofQuestionnaire'] = df['DateOfQuestionnaire'].dt.day\n",
        "    df['HourofQuestionnaire'] = df['DateOfQuestionnaire'].dt.hour\n",
        "    df['WeekDayofQuestionnaire'] = df['DateOfQuestionnaire'].dt.dayofweek\n",
        "    df['YearDayofQuestionnaire'] = df['DateOfQuestionnaire'].dt.dayofyear\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "LgJIuZFR25pq"
      },
      "outputs": [],
      "source": [
        "def feature_engineering(df):\n",
        "\n",
        "    df = create_date_features(df)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8mML9d7ahEe"
      },
      "source": [
        "### 4.10 Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "U6uBX7QQanjd"
      },
      "outputs": [],
      "source": [
        "def find_important_features(df, k_features, target='Recovery'):\n",
        "\n",
        "    \"\"\"\n",
        "    k is the number of features each feature selection method should select.\n",
        "    NOT the number of features returned\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if k_features == None:\n",
        "        important_features = df.columns\n",
        "\n",
        "    else:\n",
        "        features = df.dropna()\n",
        "        EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "        explanatory = features.drop(['Recovery', 'ReliableRecovery', 'ReliableChangeDesc'] + EndDesc_cols, axis = 1)\n",
        "        target = features[target]\n",
        "\n",
        "        # fishers score\n",
        "        kbest_fisher = SelectKBest(score_func=f_classif, k=k_features)\n",
        "        selected_fisher = kbest_fisher.fit_transform(explanatory, target)\n",
        "        index_fisher = kbest_fisher.get_support(indices=True)\n",
        "        names_fisher = explanatory.columns[index_fisher]\n",
        "\n",
        "        # mutual information gain\n",
        "        def mutual_info_classif_wseed(X, y):\n",
        "            return mutual_info_classif(X, y, random_state=11)\n",
        "        kbest_gain = SelectKBest(score_func=mutual_info_classif_wseed, k=k_features)\n",
        "        selected_gain = kbest_gain.fit_transform(explanatory, target)\n",
        "        index_gain = kbest_gain.get_support(indices=True)\n",
        "        names_gain = explanatory.columns[index_gain]\n",
        "\n",
        "        important_combined = set(names_fisher).union(set(names_gain))\n",
        "        important_features = list(important_combined)\n",
        "\n",
        "    return important_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "NTQjIpLqg-1a"
      },
      "outputs": [],
      "source": [
        "def select_important_features(df, k_features, target='Recovery'):\n",
        "\n",
        "    important_features = find_important_features(df, k_features, target='Recovery')\n",
        "\n",
        "    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col]\n",
        "    df = df[['Recovery', 'ReliableRecovery', 'ReliableChangeDesc'] + EndDesc_cols + important_features]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3-sW_8Qa2Pt"
      },
      "source": [
        "### 4 Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "ggq9DTNw4ycz"
      },
      "outputs": [],
      "source": [
        "def Prepare_Data(df,\n",
        "                 quasi_thresh=1.0,\n",
        "                 corr_thresh=1.0,\n",
        "                 load_matrices=True,\n",
        "                 col_thresh=1.0,\n",
        "                 row_thresh=1.0,\n",
        "                 imputation_method='knn',\n",
        "                 n_neighbours=10,\n",
        "                 max_iter=10,\n",
        "                 k_features=200):\n",
        "\n",
        "    df = drop_duplicate_features(df)\n",
        "    df = replace_outliers(df)\n",
        "    df = drop_const_features(df)\n",
        "    df = drop_quasi_features(df, quasi_thresh)\n",
        "    df = remove_corr_features(df, corr_thresh, load_matrices)\n",
        "    df = drop_missing_values_col(df, col_thresh)\n",
        "    df = drop_missing_values_col(df, row_thresh)\n",
        "    #df = standardise(df)\n",
        "    df = impute_data(df, imputation_method, n_neighbours, max_iter)\n",
        "    #df = feature_engineering(df)\n",
        "    df = select_important_features(df, k_features)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkNvB98vLreZ"
      },
      "source": [
        "## 1.5 Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV4vMwKNS5vp"
      },
      "source": [
        "### Summary Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "FzadOsBMS8NH"
      },
      "outputs": [],
      "source": [
        "def ModelSelection_Summary(model):\n",
        "\n",
        "# pr curve\n",
        "\n",
        "    scores = model[0]\n",
        "    preds = model[1]\n",
        "    actuals = model[2]\n",
        "\n",
        "    print('Average accuracy score: {0}'.format(np.average(scores)))\n",
        "\n",
        "    prec, recall, _ = metrics.precision_recall_curve(actuals, preds)\n",
        "    print('AUPRC score: {0}\\n'.format(metrics.auc(recall, prec)))\n",
        "\n",
        "    # plot pr curve\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(recall, prec, marker='.')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.show()\n",
        "\n",
        "# confusion matrix\n",
        "\n",
        "    # probabilities to binary\n",
        "    threshold = 0.5\n",
        "    binary_preds = [1 if pred >= threshold else 0 for pred in preds]\n",
        "\n",
        "    # plot confusion matrix\n",
        "    conf_matrix = confusion_matrix(actuals, binary_preds)\n",
        "\n",
        "    plt.subplot(121)\n",
        "    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = 'Purples')\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVmXoPNQL1IT"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "8zN_w7g0LwvE"
      },
      "outputs": [],
      "source": [
        "def XGBoost_ModelSelection(df, target, selector, param_grid, k=5):\n",
        "\n",
        "    # dataset\n",
        "    sample = df.dropna(subset = [target])\n",
        "    EndDesc_cols = [col for col in df.columns if 'EndDesc' in col] #EndDescShort_cols = [col for col in df.columns if 'EndDescShort' in col] # none\n",
        "    X = sample.drop(['ReliableChangeDesc', 'ReliableRecovery', 'Recovery'] + EndDesc_cols, axis = 1)\n",
        "    y = sample[target]\n",
        "    cols = X.columns\n",
        "\n",
        "    # machine learning algorithm\n",
        "    classifier = XGBClassifier()\n",
        "\n",
        "    # feature selection method\n",
        "    if selector == 'SelectFromModel':\n",
        "        selector = SelectFromModel(classifier)\n",
        "    elif selector == 'RFE':\n",
        "        selector = RFE(classifier)\n",
        "    else:\n",
        "        # fill this #\n",
        "        raise ValueError('Unsupported selector type')\n",
        "\n",
        "    # pipeline\n",
        "    pipeline = Pipeline([(\"FS\", selector), (\"classifier\", classifier)])\n",
        "\n",
        "    # initialise lists\n",
        "    scores, preds, actuals = [], [], []\n",
        "\n",
        "    # cross validation and hyperparameter tuning\n",
        "    outer_cv = StratifiedKFold(n_splits = k, shuffle = True)\n",
        "    inner_cv = StratifiedKFold(n_splits = k, shuffle = True) # both set to k for now\n",
        "    for train_index, test_index in outer_cv.split(X, y):\n",
        "\n",
        "        # outer CV train and test sets\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        # hyper-parameter tuning\n",
        "        grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=inner_cv, scoring='accuracy', verbose=0, n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        print(\"Inner CV accuracy: {}\".format(grid_search.best_score_)) # validation sets\n",
        "\n",
        "        # optimal model\n",
        "        estimator = grid_search.best_estimator_\n",
        "\n",
        "        # count features selected\n",
        "        support = estimator.named_steps['FS'].get_support()\n",
        "        num_feat = np.sum(support)\n",
        "        print(\"Number of selected features {0}\".format(num_feat))\n",
        "\n",
        "        # features selected\n",
        "        col_index = np.where(support)[0]\n",
        "        col_names = [cols[col] for col in col_index]\n",
        "        print(\"Selected features {0}\".format(col_names))\n",
        "\n",
        "        # hyperparameters selected\n",
        "        print(\"Max depth {0}\".format(estimator.named_steps[\"classifier\"].max_depth))\n",
        "        print(\"Number of trees {0}\".format(estimator.named_steps[\"classifier\"].n_estimators))\n",
        "        print(\"Learning rate {0}\".format(estimator.named_steps[\"classifier\"].learning_rate))\n",
        "        print(\"Minimum child weight {0}\".format(estimator.named_steps[\"classifier\"].min_child_weight))\n",
        "        print(\"Subsample {0}\".format(estimator.named_steps[\"classifier\"].subsample))\n",
        "        print(\"Colsample bytree {0}\".format(estimator.named_steps[\"classifier\"].colsample_bytree))\n",
        "        print(\"Gamma {0}\".format(estimator.named_steps[\"classifier\"].gamma))\n",
        "        print(\"Lambda {0}\".format(estimator.named_steps[\"classifier\"].reg_lambda))\n",
        "        print(\"Alpha {0}\".format(estimator.named_steps[\"classifier\"].reg_alpha))\n",
        "\n",
        "        # evaluating optimised model on test\n",
        "        predictions = estimator.predict(X_test)\n",
        "        score = metrics.accuracy_score(y_test, predictions)\n",
        "        scores.append(score)\n",
        "        print('Outer CV accuracy: {}'.format(score)) # test sets\n",
        "\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "        probs = estimator.predict_proba(X_test)[:, 1]\n",
        "        preds.extend(probs)\n",
        "        actuals.extend(y_test)\n",
        "\n",
        "    return scores, preds, actuals\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLNplPcaaHL3"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "Y31WhPuUaIp6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import torch\n",
        "\n",
        "class BERTClassifier(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, model_name='bert-base-uncased', max_length=128):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model.train()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        inputs = self.tokenizer(X.tolist(), return_tensors='pt', truncation=True, padding=True, max_length=self.max_length)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, axis=1)\n",
        "        return preds.numpy()\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        inputs = self.tokenizer(X.tolist(), return_tensors='pt', truncation=True, padding=True, max_length=self.max_length)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "        return probs.numpy()\n",
        "\n",
        "def BERT_ModelSelection(df, text_column, target, param_grid, k=5):\n",
        "    # dataset to model\n",
        "    sample = df.dropna(subset=[target])  # remove missing target values\n",
        "    X = sample[text_column]\n",
        "    y = sample[target]\n",
        "\n",
        "    # machine learning model\n",
        "    classifier = BERTClassifier()\n",
        "\n",
        "    # pipeline\n",
        "    pipeline = Pipeline([('classifier', classifier)])\n",
        "\n",
        "    # initialize lists\n",
        "    scores = []\n",
        "    preds = []\n",
        "    actuals = []\n",
        "\n",
        "    # k-fold CV\n",
        "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
        "    for train_index, test_index in kf.split(X, y):\n",
        "        # train and test data for CV\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        # finding optimal models, hyperparameter tuning\n",
        "        grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=kf, scoring='accuracy', verbose=0, n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        print(\"Internal CV Accuracy of estimator: {}\".format(grid_search.best_score_))\n",
        "\n",
        "        # best estimator\n",
        "        estimator = grid_search.best_estimator_\n",
        "\n",
        "        # print hyperparameters selected\n",
        "        # For BERT, hyperparameters can include learning rate, batch size, etc.\n",
        "        # Example:\n",
        "        print(\"BERT Model used: {}\".format(estimator.named_steps[\"classifier\"].model_name))\n",
        "        print(\"Max length used: {}\".format(estimator.named_steps[\"classifier\"].max_length))\n",
        "\n",
        "        # predicting the test data with the optimized models\n",
        "        predictions = estimator.predict(X_test)\n",
        "        score = accuracy_score(y_test, predictions)\n",
        "        scores.append(score)\n",
        "        print('Accuracy performance on this test set: {}'.format(score))\n",
        "\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "        probs = estimator.predict_proba(X_test)[:, 1]\n",
        "        preds.extend(probs)\n",
        "        actuals.extend(y_test)\n",
        "\n",
        "    return scores, preds, actuals"
      ]
    }
  ]
}